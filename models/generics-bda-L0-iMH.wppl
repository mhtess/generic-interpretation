// time ~/webppl-fork/webppl generics-bda-L0-iMH.wppl --require utils uncertain 1

var chain = last(process.argv) // load index as last command line index

// penultimate argument is the semantics
// uncertain = uncertain threshold
// fixed = fixed threshold at lowest threshold value
var semantics = process.argv[process.argv.length - 2]
console.log(semantics)
var dataPath = "../data/",
    priorFilePrefix = "prior-2",
    interpretationFilePrefix = "interpretation-4"

var priorFile = dataPath + "prior/"+priorFilePrefix+"/"+priorFilePrefix+"-trials.csv",
    priorCatchFile = dataPath + "prior/"+priorFilePrefix+"/"+priorFilePrefix+"-catch_trials.csv",
    interpretationFile = dataPath + "interpretation/"+interpretationFilePrefix+"/"+interpretationFilePrefix+"-trials.csv",
    interpretationCatchFile = dataPath + "interpretation/"+interpretationFilePrefix+"/"+interpretationFilePrefix+"-catch_trials.csv";

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["workerid","response"]),
    d_prior_catch = dataFrame(utils.readCSV(priorCatchFile).data, ["workerid",  "correct"]),
		d_interpretation = dataFrame(utils.readCSV(interpretationFile).data, ["workerid", "response"]),
    d_interpretation_catch=dataFrame(utils.readCSV(interpretationCatchFile).data, ["workerid", "correct"]);

var priorWorkerids = levels(d_prior_catch, "workerid")
var interpretationWorkerids = levels(d_interpretation_catch, "workerid")

var priorWorkeridsPassed = filter(function(workerid){
  sum(_.map(_.filter(d_prior_catch, {workerid: workerid}), "correct")) >= 8
}, priorWorkerids)

var interpretationWorkeridsPassed = filter(function(workerid){
  sum(_.map(_.filter(d_prior_catch, {workerid: workerid}), "correct")) >= 8
}, interpretationWorkerids)

var data = {
	prior: filter(function(di){
    return (priorWorkeridsPassed.indexOf(di.workerid) > -1) && (utils.isNumber(di.response))
  }, map(function(d){ return extend(d, {
		avoided_endval: avoidEnds(d.response)
  })}, d_prior)),
	interpretation: filter(function(di){
    return (interpretationWorkeridsPassed.indexOf(di.workerid) > -1) && (utils.isNumber(di.response))
  }, map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins.slice(1), d.response)
	})}, d_interpretation))
};


// // test that removing Ss who failed catch trial works properly
assert.ok(levels(data.interpretation, "workerid").length == interpretationWorkeridsPassed.length)

assert.ok(levels(data.prior, "workerid").length == priorWorkeridsPassed.length)


var utterancePrior = Infer({model: function(){
	return uniformDraw(["generic", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=='silence'? true :
         utt=='some'? state > 0 :
         true
}
//
var properties = levels(data.interpretation, "property");
var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);
// properties

var model = function(){

  var numberOfComponents = 3;//poisson(1) + 1;

  foreach(properties, function(item){
    // display(item)
    var propertyData = {
        prior: _.filter(data.prior, {property: item}),
        interpretation: _.filter(data.interpretation, {property: item})
    };

    var weights = normalize(repeat(numberOfComponents,
      function(){ return uniformDrift({a: 0, b: 1, width:0.2}) }
    ))

    var componentParameters = repeat(numberOfComponents,
     function(){
      return betaShape({
          g: uniformDrift({a: 0, b: 1, width: 0.2}),
          d: uniformDrift({a: 0, b: 100, width: 5})
        })
    })

  	mapData({data: propertyData.prior}, function(d){
      var componentLogLikelihood = map2(function(w, params){
        Math.log(w) + Beta(params).score(d.avoided_endval)
      }, weights, componentParameters)
      var scr = util.logsumexp(componentLogLikelihood)
      scr == -Infinity ? displayObj(d) : null
  		factor(scr)
  	})

    foreach(_.range(numberOfComponents), function(i){

      query.add(["componentParameters", item, i, "weight"],
        weights[i]
      )
      query.add(["componentParameters", item, i, "alpha"],
        componentParameters[i]["a"]
      )

      query.add(["componentParameters", item, i, "beta"],
        componentParameters[i]["b"]
      )

    })

  	var statePrior = Infer({model: function(){
      sample(DiscretizedBeta(componentParameters[discrete(weights)]))
  	}});

  	/// RSA model
  	var listener0 = cache(function(utterance) {
  	  Infer({model: function(){
  	    var state = sample(statePrior);
        var theta = (semantics == "uncertain") ? sample(thetaPrior) : _.min(thetaBins);
  	    var m = meaning(utterance, state, theta)
  	    condition(m)
  	    return state
  	 }})}, 10000)

    var interpretationPrediction = listener0("generic")

   mapData({data:propertyData.interpretation}, function(d){
     // display(d)
     var scr = interpretationPrediction.score(d.binnedResponse)
     scr == -Infinity ? displayObj(d) : null
     // display(scr)
     observe(interpretationPrediction, d.binnedResponse)
   })

   query.add(["prediction", item, -99, "prior"], expectation(statePrior))
   query.add(["prediction", item, -99, "posterior"], expectation(interpretationPrediction))
 })

   // RECORD PARAMETERS AND PREDICTIVES
   // query.add("numberOfComponents", [-99, -99, -99, numberOfComponents])
   // query.add(["numberOfComponents", item, -99, -99], numberOfComponents)

	return query
}

var totalIterations = 250000, lag = 75;
var samples = totalIterations/lag, burn = totalIterations / 2;

var outfile = 'pilot-results-genint-L0-3Components_'+semantics+'-semantics_'+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'

var posterior = Infer({
  model: model,
	method: "incrementalMH",
  samples: samples, burn: burn, //lag: lag,
  verbose: T,
  verboseLag: totalIterations / 20,
	stream: {
		path: "results/" + outfile,
		header: ["type", "param", "property", "category", "val"]
	}
})

"written to " + outfile;