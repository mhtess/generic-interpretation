// time ~/webppl-fork/webppl generics-bda.wppl --require utils

var chain = last(process.argv) // load index as last command line index

// // penultimate argument is the semantics
// // causal = uncertain threshold
// // some = fixed threshold at lowest threshold value
// var targetUtterance = process.argv[process.argv.length - 2]

var dataPath = "../data/",
    priorFilePrefix = "prior-1",
    interpretationFilePrefix = "interpretation-3"

var priorFile = dataPath + "prior/"+priorFilePrefix+"/"+priorFilePrefix+"-trials.csv",
    priorCatchFile = dataPath + "prior/"+priorFilePrefix+"/"+priorFilePrefix+"-catch_trials.csv",
    interpretationFile = dataPath + "interpretation/"+interpretationFilePrefix+"/"+interpretationFilePrefix+"-trials.csv",
    interpretationCatchFile = dataPath + "interpretation/"+interpretationFilePrefix+"/"+interpretationFilePrefix+"-catch_trials.csv";

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["workerid","response"]),
    d_prior_catch = dataFrame(utils.readCSV(priorCatchFile).data, ["workerid",  "correct"]),
		d_interpretation = dataFrame(utils.readCSV(interpretationFile).data, ["workerid", "response"]),
    d_interpretation_catch=dataFrame(utils.readCSV(interpretationCatchFile).data, ["workerid", "correct"]);

var priorWorkerids = levels(d_prior_catch, "workerid")
var interpretationWorkerids = levels(d_interpretation_catch, "workerid")

var priorWorkeridsPassed = filter(function(workerid){
  sum(_.map(_.filter(d_prior_catch, {workerid: workerid}), "correct")) >= 8
}, priorWorkerids)

var interpretationWorkeridsPassed = filter(function(workerid){
  sum(_.map(_.filter(d_prior_catch, {workerid: workerid}), "correct")) >= 8
}, interpretationWorkerids)

var data = {
	prior: filter(function(di){
    return priorWorkeridsPassed.indexOf(di.workerid) > -1
  }, map(function(d){ return extend(d, {
		avoided_endval: avoidEnds(d.response)
  })}, d_prior)),
	interpretation: filter(function(di){
    return interpretationWorkeridsPassed.indexOf(di.workerid) > -1
  }, map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins, d.response)
	})}, d_interpretation))
};

// // test that removing Ss who failed catch trial works properly
assert.ok(levels(data.interpretation, "workerid").length == interpretationWorkeridsPassed.length)

assert.ok(levels(data.prior, "workerid").length == priorWorkeridsPassed.length)


var utterancePrior = Infer({model: function(){
	return uniformDraw(["generic", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=='silence'? true :
         utt=='some'? state > 0 :
         true
}
//
var properties = levels(data.interpretation, "property");
var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);
// properties

var model = function(priorData, interpretationData){

  var numberOfComponents = poisson(1) + 1;

  var weights = normalize(repeat(numberOfComponents,
    function(){ return uniformDrift({a: 0, b: 1, width:0.2}) }
  ))

  var componentParameters = repeat(numberOfComponents,
   function(){
    return betaShape({
        g: uniformDrift({a: 0, b: 1, width: 0.2}),
        d: uniformDrift({a: 0, b: 100, width: 5})
      })
  })

	mapData({data: priorData}, function(d){
    var componentLogLikelihood = map2(function(w, params){
      Math.log(w) + Beta(params).score(d.avoided_endval)
    }, weights, componentParameters)
    var scr = util.logsumexp(componentLogLikelihood)
    // console.log(scr)
		factor(scr)
	})

	var statePrior = Infer({model: function(){
    sample(DiscretizedBeta(componentParameters[discrete(weights)]))
	}});

	/// RSA model
	var listener0 = cache(function(utterance) {
	  Infer({model: function(){
	    var state = sample(statePrior), theta = sample(thetaPrior);
	    var m = meaning(utterance, state, theta)
	    condition(m)
	    return state
	 }})}, 10000)

   var interpretationPrediction = listener0("generic")

   mapData({data:interpretationData}, function(d){
     // display(d)
     // var scr = interpretationPrediction.score(d.binnedResponse)
     // scr == -Infinity ? displayObj(d) : null
     // display(scr)
     observe(interpretationPrediction, d.binnedResponse)
   })

   // RECORD PARAMETERS AND PREDICTIVES
   // query.add("numberOfComponents", [-99, -99, -99, numberOfComponents])
   query.add(["numberOfComponents", -99, -99, -99], numberOfComponents)

   foreach(_.range(numberOfComponents), function(i){

     query.add(["componentParameters", i, weights[i], componentParameters[i]["a"]],
       componentParameters[i]["b"]
     )

   })

  // query.add("prediction", [-99, -99, expectation(statePrior), expectation(interpretationPrediction)])
  query.add(["prediction", -99, -99, expectation(statePrior)], expectation(interpretationPrediction))

	return query
}

// model()
var propertyData = {
    prior: _.filter(data.prior, {property: properties[0]}),
    interpretation: _.filter(data.interpretation, {property: properties[0]})
};

var samples = 100;
var burn = samples / 2;

Infer({
  model: function(){
    model(propertyData.prior, propertyData.interpretation)
  },
  method: "MCMC", samples, burn, verbose: true
})

// var totalIterations = 5000, lag = 5;
// var mhiter = totalIterations/lag, burn = totalIterations / 2;
// var outfile = 'pilot-results-causals-jointModel-S1-smntcs_'+
// targetUtterance + '-'+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'

// var posterior = Infer({
//   model: model,
//   method: "incrementalMH",
//   samples: mhiter, burn: burn, lag: lag,
//   verbose: T, verboseLag: totalIterations / 20,
// 	stream: {
// 		path: "results/" + outfile,
// 		header: [
// 			"type", "item", "dist", "roundedFreq", "frequency", "val"
// 		]
// 	}
// })

// "written to " + outfile;
