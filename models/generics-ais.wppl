// time webppl generics-ais.wppl --require utils --require webppl-json lit_thetaPrior uncertain 1

// modelName: pragmatic, pragmatic_unlifted, literal, lit_thetaPrior

// penultimate argument is the semantics
// uncertain = uncertain threshold
// fixed = fixed threshold at lowest threshold value

var arguments = process.argv
var modelName = arguments[arguments.length - 3]
var semantics = arguments[arguments.length - 2]
var iter = arguments[arguments.length - 1]

var n_obs = 3 // only does something if semantics == n_obs

var priorFilePrefix = "prior-3",
    // priorFilePrefix = "prior-manipulation-3",
    interpretationFilePrefix = "interpretation-6",
    endorsementFilePrefix = "endorsement-1";

// expt 2: measured priors
var d_prior = readDataFile("expt1a-prior"),
    d_prior_catch = readCatchFile("expt1a-prior"),
    d_prior_subj = readSubjFile("expt1a-prior"),
    d_interpretation = readDataFile("expt1b-interpretation"),
    d_interpretation_catch = readCatchFile("expt1b-interpretation"),
    d_interpretation_subj = readSubjFile("expt1b-interpretation");
    // d_endorsement = readDataFile("endorsement", endorsementFilePrefix),
    // d_endorsement_catch = readCatchFile("endorsement", endorsementFilePrefix),
    // d_endorsement_subj = readSubjFile("endorsement", endorsementFilePrefix);

// expt 3: Prior manipulation
// var d_prior = readDataFile("prior-manipulation", priorFilePrefix),
//     d_prior_catch = readCatchFile("prior-manipulation", priorFilePrefix),
//     d_prior_subj = readSubjFile("prior-manipulation", priorFilePrefix),
//     d_interpretation = readDataFile("prior-manipulation", interpretationFilePrefix),
//     d_interpretation_catch = readCatchFile("prior-manipulation", interpretationFilePrefix),
//     d_interpretation_subj = readSubjFile("prior-manipulation", interpretationFilePrefix);

var priorWorkeridsPassed = passCatchTrials(d_prior_catch),
    interpretationWorkeridsPassed = passCatchTrials(d_interpretation_catch),
    // endorsementWorkeridsPassed = passCatchTrials(d_endorsement_catch),
    priorWorkeridsEnglish = nativeEnglish(d_prior_subj),
    interpretationWorkeridsEnglish = nativeEnglish(d_interpretation_subj);
    // endorsementWorkeridsEnglish = nativeEnglish(d_endorsement_subj);

var data = {
	prior: filter(function(di){
    return ((priorWorkeridsPassed.indexOf(di.workerid) > -1) &&
      (priorWorkeridsEnglish.indexOf(di.workerid) > -1) &&
      utils.isNumber(di.response) &&
      di.trial_type == "prevalence_elicitation")
  }, map(function(d){ return extend(d, {
		avoided_endval: avoidEnds(d.response),
    binnedResponse:  utils.closest( midBins, d.response)
  })}, d_prior)),
	interpretation: filter(function(di){
    return ((interpretationWorkeridsPassed.indexOf(di.workerid) > -1) &&
      (interpretationWorkeridsEnglish.indexOf(di.workerid) > -1) &&
      utils.isNumber(di.response) &&
      di.trial_type == "implied_prevalence")
  }, map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins.slice(1), d.response)
	})}, d_interpretation))//,
  // endorsement: filter(function(di){
  //   return ((endorsementWorkeridsPassed.indexOf(di.workerid) > -1) &&
  //     (endorsementWorkeridsEnglish.indexOf(di.workerid) > -1) &&
  //     utils.isNumber(di.response) &&
  //     di.trial_type == "truth_conditions")
  // }, map(function(d){ return extend(d, {
  //   binned_prevalence:  utils.closest( midBins, d.prevalence_level / 100)
  // })}, d_endorsement))
};


// var prevalence_levels = map(function(x){ utils.closest(midBins, x) }, [0.1,0.3,0.5,0.7,0.9])

// // // test that removing Ss who failed catch trial works properly (computed in R)
assert.ok(levels(data.interpretation, "workerid").length == 132)
// assert.ok(levels(data.interpretation, "workerid").length == 107)
// assert.ok(levels(data.prior, "workerid").length == 131)
// assert.ok(levels(data.prior, "workerid").length == 126) // prior 3
assert.ok(levels(data.prior, "workerid").length == 175) // prior 3 n200
// assert.ok(levels(data.endorsement, "workerid").length == 150) // endorsement 1

var utterancePrior = Infer({model: function(){
	return uniformDraw(["generic", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=='silence'? true :
         utt=='some'? state > 0 :
         true
}
//
var properties = levels(data.interpretation, "property");
var nullParams = {a:1, b:100}, nullDistribution = Beta(nullParams);
// properties

var addNoise = function(dist, noise){
  return Infer({model: function(){
    return flip(noise) ? uniformDraw(midBins) : sample(dist)
  }})
}

var fixedThreshold = (semantics == "some") ?_.min(thetaBins) :
  (semantics == "most") ? 0.5 :
  false

var priors = _.fromPairs(map(function(p){
  var property_data = _.filter(data.prior, {property: p})
  return [p, map(function(b){ _.filter(property_data, {binnedResponse: b}).length + 1}, midBins)]
}, properties))

display(priors)
var marginalLikelihood = function(){

  var noise = (semantics == "most") ? uniform(0, 1) : 0
  var speakerOptimality = (modelName == "literal") ? 1 : uniform(0, 30)
  var cost = (modelName == "literal") ? 1 :
			(modelName == "lit_thetaPrior") ? 1 : uniform(0, 10)

	var theta_mean = (modelName == "lit_thetaPrior") ? uniformDrift({a: 0, b: 1, width:0.2}) : 0.5
	var theta_concentration = (modelName == "lit_thetaPrior") ? uniformDrift({a: 0, b: 50, width:2}) : 2

	var thetaParams = {
		a: theta_mean * theta_concentration,
		b: (1 - theta_mean) * theta_concentration
	}

	var nonuniform_ThetaPrior = Categorical({
      vs: thetaBins,
      ps: map2(function(b1, b2){
        return utils.betaCDF(b2, thetaParams) -
        utils.betaCDF(b1, thetaParams)
      }, midBins.slice(0, midBins.length-1),
        midBins.slice(1))
	 })


  var cost_uttProb = Math.exp(-cost)

  var numberOfComponents = 3;//poisson(1) + 1;

  foreach(properties, function(item){
    // display(item)
    var propertyData = {
        prior: _.filter(data.prior, {property: item}),
        interpretation: _.filter(data.interpretation, {property: item})
    };

    // var weights = normalize(repeat(numberOfComponents,
    //   function(){ return uniformDrift({a: 0, b: 1, width:0.2}) }
    // ))

    // var componentParameters = repeat(numberOfComponents,
    //  function(){
    //   return betaShape({
    //       g: uniformDrift({a: 0, b: 1, width: 0.2}),
    //       d: uniformDrift({a: 0, b: 100, width: 5})
    //     })
    // })
    //
  	// mapData({data: propertyData.prior}, function(d){
    //   var componentLogLikelihood = map2(function(w, params){
    //     Math.log(w) + Beta(params).score(d.avoided_endval)
    //   }, weights, componentParameters)
    //   var scr = util.logsumexp(componentLogLikelihood)
    //   scr == -Infinity ? displayObj(d) : null
  	// 	factor(scr)
  	// })

    // foreach(_.range(numberOfComponents), function(i){

    //   query.add(["componentParameters", item, i, "weight"],
    //     weights[i]
    //   )
    //   query.add(["componentParameters", item, i, "alpha"],
    //     componentParameters[i]["a"]
    //   )

    //   query.add(["componentParameters", item, i, "beta"],
    //     componentParameters[i]["b"]
    //   )

    // })

    var statePrior = Categorical({vs: midBins, ps: priors[item]})
  	// var statePrior = Infer({model: function(){
    //   sample(DiscretizedBeta(componentParameters[discrete(weights)]))
  	// }});

  	/// RSA model

		 var listener0 = cache(function(utterance){
			 return Infer({model: function(){
				 var theta = (semantics == "uncertain") ? sample(nonuniform_ThetaPrior) : fixedThreshold;
				 var state = sample(statePrior);
				 var m = meaning(utterance, state, theta)
   	    condition(m)
					return state
			}})})

     /// LIFTED PRAGMATIC
   	var listener0_forL1 = cache(function(utterance, theta) {
   	  Infer({model: function(){
   	    var state = sample(statePrior);
   	    var m = meaning(utterance, state, theta)
   	    condition(m)
   	    return state
   	 }})}, 10000)

     var speaker1 = cache(function(state, theta) {
      Infer({model: function(){
        var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
        var L0 = listener0_forL1(utterance, theta);
        factor(speakerOptimality * L0.score(state))
        return utterance
      }})}, 10000)

    var listener1 = cache(function(utterance) {
      Infer({model: function(){
        var state = sample(statePrior);
        var theta = (semantics == "uncertain") ? sample(thetaPrior) : _.min(thetaBins);
        var S1 = speaker1(state, theta)
        observe(S1, utterance)
        return state
      }})}, 10000)

      /// UNLIFTED PRAGMATIC

       var speaker1_unlifted = cache(function(state) {
        Infer({model: function(){
          var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
          var L0 = listener0(utterance);
          factor(speakerOptimality * L0.score(state))
          return utterance
        }})}, 10000)

      var listener1_unlifted = cache(function(utterance) {
        Infer({model: function(){
          var state = sample(statePrior);
          var S1 = speaker1_unlifted(state)
          observe(S1, utterance)
          return state
        }})}, 10000)


        var n_obs_update = Infer({model: function(){
      	    var state = sample(statePrior);
            // display(state)
            factor(Binomial({p: state, n: n_obs}).score(n_obs))
      	    return state
      	 }})

    var interpretationPrediction = modelName == "literal" ? listener0("generic") :
      modelName == "pragmatic_unlifted" ? listener1_unlifted("generic") :
      modelName == "prior" ? statePrior :
      listener1("generic")

      // var interpretationPrediction = n_obs_update

    // var L0 = listener0

  // var L0 = listener0("generic")
  var noisyPrediction = addNoise(interpretationPrediction, noise)

   mapData({data:propertyData.interpretation}, function(d){
     // display(d)
     var scr = noisyPrediction.score(d.binnedResponse)
     scr == -Infinity ? displayObj(d) : null
     // display(scr)
     observe(noisyPrediction, d.binnedResponse)
   })

   // var supp = interpretationPrediction.support()
   //
   // foreach(supp, function(s){
   //   query.add(["prediction", item, "postDist", s], Math.exp(interpretationPrediction.score(s)))
   // })

   // var speaker1 = function(state, speakerOptimality) {
   //  Infer({model: function(){
   //    var utterance = sample(utterancePrior);
   //    var L0 = listener0(utterance);
   //    factor(speakerOptimality * L0.score(state))
   //    return utterance
   //  }})}

    // foreach([1, 5, 10], function(speakerOptimality){

    //  foreach(prevalence_levels, function(p){

    //    var endorsement = speaker1(p, speakerOptimality)

    //    query.add(["endorsement", item, speakerOptimality, p], Math.exp(endorsement.score("generic")))

    //  })

    // })

   // query.add(["prediction", item, -99, "prior"], expectation(statePrior))
   // query.add(["prediction", item, -99, "posterior"], expectation(interpretationPrediction))
 })

   // RECORD PARAMETERS AND PREDICTIVES
   // query.add("numberOfComponents", [-99, -99, -99, numberOfComponents])
   // semantics == "most" ? query.add(["noise", -99, -99, -99], noise) : null

	// return query
}


// data.endorsement
var steps = 100
var samples = 1
var mll = AIS(marginalLikelihood, {steps:steps, samples: samples})

var semantics_print = semantics == "n_obs" ? semantics + n_obs : semantics

var resultsOutFile = 'results/genint/marginal_likelihood/ais-genint-wCost-'+modelName+
  "-" + semantics_print +'-fixedPriors-steps'+steps +'-iter'+iter+'.json'

// // postOut
json.write(resultsOutFile, mll)
'results written to ' + resultsOutFile


// priors
// midBins
// property_data
