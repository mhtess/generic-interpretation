// time ~/tools/webppl-fork/webppl cimpian-bda-L0-iMH.wppl --require utils pragmatic_unlifted uncertain 1
// time ~/tools/webppl-fork/webppl cimpian-bda-L0-iMH.wppl --require utils pragmatic uncertain 1
// time ~/tools/webppl-fork/webppl cimpian-bda-L0-iMH.wppl --require utils literal uncertain 1


// time webppl models/bda_models/cimpian-bda-iMH.wppl  --require models/node_modules/utils --require ~/tools/webppl-sample-writer-fork literal highBeta 1
// time webppl models/bda_models/cimpian-bda-iMH.wppl  --require models/node_modules/utils --require ~/tools/webppl-sample-writer-fork literal most 1
// time webppl models/bda_models/cimpian-bda-iMH.wppl  --require models/node_modules/utils --require ~/tools/webppl-sample-writer-fork literal best 1
// time webppl models/bda_models/cimpian-bda-iMH.wppl  --require models/node_modules/utils --require ~/tools/webppl-sample-writer-fork 1

var all_semantics = ["most", "best", "ten", "thirty", "some", "highBeta", "uncertain"]

var chain = last(process.argv) // load index as last command line index

// penultimate argument is the semantics
// uncertain = uncertain threshold
// fixed = fixed threshold at lowest threshold value

// var modelName = process.argv[process.argv.length - 3]
var modelName = "literal"
// var semantics = process.argv[process.argv.length - 2]

var modelIndex = (chain - 1) % all_semantics.length
var semantics = all_semantics[modelIndex]

var AllNullDistributions = [
  {name: "delta", dist: Delta({v: _.min(midBins)})},
  {name: "beta1_100", dist: DiscretizedBeta({a:1, b:100})},
  {name: "beta1_1000", dist: DiscretizedBeta({a:1, b:1000})}
]

var NullDistributionObj = AllNullDistributions[(chain-1) % AllNullDistributions.length]
var NullDistribution = NullDistributionObj.dist;
var NullDistributionName = NullDistributionObj.name;

console.log("model = " + modelName+ " ___ semantics = " + semantics + " ___ null dist = " + NullDistributionName)

var dataPath = "data/pilot_data/preliminary/",
    priorFilePrefix = "cimpian-prior-4",
    interpretationFilePrefix = "cimpian-implied-prevalence-2"

var priorFile = dataPath + priorFilePrefix+"-trials.csv",
    interpretationFile = dataPath + interpretationFilePrefix+"-trials.csv";

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["workerid","response0", "response1"]),
		d_interpretation = dataFrame(utils.readCSV(interpretationFile).data, ["workerid", "response"]);

var priorWorkerids = levels(d_prior, "workerid")
var interpretationWorkerids = levels(d_interpretation, "workerid")

var data = {
	prior: map(function(d){ return extend(d, {
		response0_avoidedEndval: avoidEnds(d.response0),
    response1_avoidedEndval: avoidEnds(d.response1)
  })}, d_prior),
	interpretation: map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins.slice(1), d.response)
	})}, d_interpretation)
};


var utterancePrior = Infer({model: function(){
	return uniformDraw(["generic", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="generic" ? state > theta :
         utt=="generic is false" ? state<=theta :
         utt=='silence'? true :
         utt=='some'? state > _.min(thetaBins) :
         utt=='most'? state > 0.5 :
         true
}
//
var properties = levels(data.interpretation, "stim_property");


// properties

var addNoise = function(dist, noise){
  return Infer({model: function(){
    return flip(noise) ? uniformDraw(midBins) : sample(dist)
  }})
}

var nBins = thetaBins.length;
var thetaWeights = map(function(s){ return exp(thetaPrior.score(s)) }, thetaBins)
var kernelWidth = 3//nBins / 4;

var kernelFn = function(prevVal){
  var i = thetaBins.indexOf(prevVal);
  var upper = (i + kernelWidth) > nBins ? nBins : i + kernelWidth;
  var lower = (i - kernelWidth) < 0 ? 0 : i - kernelWidth;
  return Categorical({vs: thetaBins.slice(lower,upper),
                      ps: thetaWeights.slice(lower,upper)})
}

var fixed_threshold_options = ["most", "best", "ten", "fifteen", "twenty", "twentyfive",
"thirty", "forty", "sixtyfive"]

var model = function(){

    // var fixedThreshold = semantics == "fixed" ? uniformDraw(thetaBins) : -99
    var noise = !(_.includes(fixed_threshold_options, semantics)) ? 0 :
			uniformDrift({a: 0, b: 1, width:0.2})

			var fixedThreshold = (semantics == "some") ?_.min(thetaBins) :
				(semantics == "ten") ? 0.08 :
				(semantics == "fifteen") ? 0.13 :
				(semantics == "twenty") ? 0.18 :
				(semantics == "twentyfive") ? 0.23 :
				(semantics == "thirty") ? 0.28 :
				(semantics == "forty") ? 0.38 :
			  (semantics == "most") ? 0.5 :
				(semantics == "sixtyfive") ? 0.63 :
				(semantics == "best") ? sample(thetaPrior, {driftKernel: kernelFn}) :
			  false

    // var speakerOptimality = (modelName == "literal") ? 1 : uniform(0, 30)
    // var cost_uttProb = 1//uniformDrift({a: 0, b: 1, width: 0.2})

  foreach(properties, function(item){
    // display(item)
    var propertyData = {
        prior: _.filter(data.prior, {stim_property: item}),
        interpretation: _.filter(data.interpretation, {stim_property: item})
    };

    var mixture_params = betaShape({
          g: uniformDrift({a: 0, b: 1, width: 0.2}),
					d: exponential({a: 1})
    })

    mapData({data: propertyData.prior}, function(d){
      observe( Beta(mixture_params), d.response0_avoidedEndval )
    })

    var prevalence_when_present = betaShape({
          g: uniformDrift({a: 0, b: 1, width: 0.2}),
					d: exponential({a: 1})
    })

    mapData({data: propertyData.prior}, function(d){
      observe( Beta(prevalence_when_present), d.response1_avoidedEndval )
    })

    query.add(["prevPriorParams", item, "mixtureParam", "alpha"], mixture_params.a)
    query.add(["prevPriorParams", item, "mixtureParam", "beta"], mixture_params.b)

    query.add(["prevPriorParams", item, "prevWhenPresent", "alpha"], prevalence_when_present.a)
    query.add(["prevPriorParams", item, "prevWhenPresent", "beta"], prevalence_when_present.b)

  	var statePrior = Infer({model: function(){
      sample(
        flip(sample(DiscretizedBeta(mixture_params))) ?
        DiscretizedBeta(prevalence_when_present) :
        NullDistribution
      )
  	}});

		var higherBetaComponent = DiscretizedBeta(prevalence_when_present)

     /// RSA model
   	var listener0 = cache(function(utterance) {
   	  Infer({model: function(){
   	    var state = sample(statePrior);
         var theta = (semantics == "uncertain") ? sample(thetaPrior) : fixedThreshold;
   	    var m = meaning(utterance, state, theta)
   	    condition(m)
   	    return state
   	 }})}, 10000)

      /// LIFTED PRAGMATIC
    	// var listener0_forL1 = cache(function(utterance, theta) {
    	//   Infer({model: function(){
    	//     var state = sample(statePrior);
    	//     var m = meaning(utterance, state, theta)
    	//     condition(m)
    	//     return state
    	//  }})}, 10000)
		 //
     //  var speaker1 = cache(function(state, theta) {
     //   Infer({model: function(){
     //     var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
     //     var L0 = listener0_forL1(utterance, theta);
     //     factor(speakerOptimality * L0.score(state))
     //     return utterance
     //   }})}, 10000)
		 //
     // var listener1 = cache(function(utterance) {
     //   Infer({model: function(){
     //     var state = sample(statePrior);
     //     var theta = (semantics == "uncertain") ? sample(thetaPrior) : fixedThreshold;
     //     var S1 = speaker1(state, theta)
     //     observe(S1, utterance)
     //     return state
     //   }})}, 10000)
		 //
     //   /// UNLIFTED PRAGMATIC
		 //
     //    var speaker1_unlifted = cache(function(state) {
     //     Infer({model: function(){
     //       var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
     //       var L0 = listener0(utterance);
     //       factor(speakerOptimality * L0.score(state))
     //       return utterance
     //     }})}, 10000)
		 //
     //   var listener1_unlifted = cache(function(utterance) {
     //     Infer({model: function(){
     //       var state = sample(statePrior);
     //       var S1 = speaker1_unlifted(state)
     //       observe(S1, utterance)
     //       return state
     //     }})}, 10000)

    // var L = modelName == "literal" ? listener0("generic") :
    // modelName == "pragmatic_unlifted" ? listener1_unlifted("generic") :
    // listener1("generic")

    // var interpretationPrediction = addNoise(L, noise)

		var L0 = semantics == "highBeta" ? higherBetaComponent : listener0("generic")
		var interpretationPrediction = addNoise(L0, noise)

   mapData({data:propertyData.interpretation}, function(d){
     // display(d)
     var scr = interpretationPrediction.score(d.binnedResponse)
     scr == -Infinity ? displayObj(d) : null
     // display(scr)
     observe(interpretationPrediction, d.binnedResponse)
   })

   // query.add(["prediction", item, -99, "prior"], expectation(statePrior))
   // query.add(["prediction", item, -99, "posterior"], expectation(interpretationPrediction))

	 var supp = statePrior.support()

   foreach(supp, function(s){
     query.add(["prediction", item, "priorDist", s], Math.exp(statePrior.score(s)))
     query.add(["prediction", item, "postDist", s], Math.exp(interpretationPrediction.score(s)))
   })




 })

  // query.add(["prevPriorParams", -99, "nullParams", "alpha"], nullParams.a)
  // query.add(["prevPriorParams", -99, "nullParams", "beta"], nullParams.b)

   // RECORD PARAMETERS AND PREDICTIVES
   // query.add("numberOfComponents", [-99, -99, -99, numberOfComponents])
	 _.includes(fixed_threshold_options, semantics) ? query.add(["parameter", "noise", -99, -99], noise) : null
	 _.includes(fixed_threshold_options, semantics) ? query.add(["parameter", "fixed_threshold", -99, -99], fixedThreshold) : null

	return query
}

// var totalIterations = 500000, lag =  150;
// var totalIterations = 100000, lag =  100;
var totalIterations = 1000000, lag =  1000;
var samples = totalIterations/lag, burn = totalIterations / 2;

var outfile = 'test-results-cimpian-'+modelName+'-structuredPrior_'+semantics+'-semantics-nullDist_' + NullDistributionName + "_" + totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv';
var header = "iter,type,param,property,category,val,score"
var callback = webpplSampleWriter.streamQueryCSV("results/" + outfile, header);


var posterior = Infer({
  model: model,
	method: "incrementalMH",
	onlyMAP: true,
  samples: samples, burn: burn, lag: lag,
  verbose: T,
  verboseLag: totalIterations / 20,
	callbacks: [callback]
})

"written to " + outfile;
