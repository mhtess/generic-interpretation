// time webppl cimpian-bda-ais.wppl --require utils --require webppl-json pragmatic_unlifted uncertain 1
// time webppl cimpian-bda-ais.wppl --require utils --require webppl-json pragmatic uncertain 1
// time webppl cimpian-bda-ais.wppl --require utils --require webppl-json literal uncertain 1

// time webppl models/model_comparison/cimpian-bda-ais.wppl  --require models/node_modules/utils --require webppl-csv 1

var chain = last(process.argv) // load index as last command line index
var all_semantics = ["most", "best", "ten", "thirty", "some", "highBeta", "uncertain"]

// penultimate argument is the semantics
// uncertain = uncertain threshold
// fixed = fixed threshold at lowest threshold value
// var modelName = process.argv[process.argv.length - 3]
var modelName = "literal"

// var semantics = process.argv[process.argv.length - 2]
var modelIndex = (chain - 1) % all_semantics.length
var semantics = all_semantics[modelIndex]

var AllNullDistributions = [
  {name: "delta", dist: Delta({v: _.min(midBins)})},
  {name: "beta1_100", dist: DiscretizedBeta({a:1, b:100})},
  {name: "beta1_1000", dist: DiscretizedBeta({a:1, b:1000})}
]

var NullDistributionObj = AllNullDistributions[(chain-1) % AllNullDistributions.length]
var NullDistribution = NullDistributionObj.dist;
var NullDistributionName = NullDistributionObj.name;

console.log("model = " + modelName+ " ___ semantics = " + semantics + " ___ null dist = " + NullDistributionName)

var dataPath = "data/pilot_data/preliminary/",
    priorFilePrefix = "cimpian-prior-4",
    interpretationFilePrefix = "cimpian-implied-prevalence-2"

var priorFile = dataPath + priorFilePrefix+"-trials.csv",
    interpretationFile = dataPath + interpretationFilePrefix+"-trials.csv";

var d_prior = dataFrame(utils.readCSV(priorFile).data, ["workerid","response0", "response1"]),
		d_interpretation = dataFrame(utils.readCSV(interpretationFile).data, ["workerid", "response"]);

var priorWorkerids = levels(d_prior, "workerid")
var interpretationWorkerids = levels(d_interpretation, "workerid")

var data = {
	prior: map(function(d){ return extend(d, {
    binnedResponse0: utils.closest( midBins, d.response0),
    binnedResponse1: utils.closest( midBins, d.response1),
		response0_avoidedEndval: avoidEnds(d.response0),
    response1_avoidedEndval: avoidEnds(d.response1)
  })}, d_prior),
	interpretation: map(function(d){ return extend(d, {
		binnedResponse:  utils.closest( midBins.slice(1), d.response)
	})}, d_interpretation)
};


var utterancePrior = Infer({model: function(){
	return uniformDraw(["generic", "silence"])
}});

var meaning = function(utt,state, theta) {
  return utt=="generic" ? state > theta :
         utt=="generic is false" ? state<=theta :
         utt=='silence'? true :
         utt=='some'? state > _.min(thetaBins) :
         utt=='most'? state > 0.5 :
         true
}
//

var properties = levels(data.interpretation, "stim_property");

// var fixedThreshold = (semantics == "some") ?_.min(thetaBins) :
//   (semantics == "most") ? 0.5 :
//   false

var fixed_threshold_options = ["most", "best", "ten", "fifteen", "twenty", "twentyfive",
"thirty", "forty", "sixtyfive"]

// properties

var addNoise = function(dist, noise){
  return Infer({model: function(){
    return flip(noise) ? uniformDraw(midBins) : sample(dist)
  }})
}

// var priors = _.fromPairs(map(function(p){
//   var property_data = _.filter(data.prior, {stim_property: p})
//
//   var distProbs = _.fromPairs(map(function(r){
//     // var response_data = _.map(property_data, r)
//     // return [r, listMean(response_data)]
//
//     return r == "binnedResponse0" ?
//       [r, map(function(b){ _.filter(property_data, {binnedResponse0: b}).length + 1}, midBins)] :
//       [r, map(function(b){ _.filter(property_data, {binnedResponse1: b}).length + 1}, midBins)]
//
//   }, ["binnedResponse0", "binnedResponse1"]))
//
//   return [p, Infer({model: function(){
//     sample(
//       flip(
//         categorical({vs: midBins, ps: distProbs.binnedResponse0})
//       ) ?
//       Categorical({vs: midBins, ps: distProbs.binnedResponse1}) :
//       NullDistribution
//     )
//   }})]
//
// }, properties))

// priors["big claws"]
// data.prior

var marginalLikelihood = function(){

    // var fixedThreshold = semantics == "fixed" ? uniformDraw(thetaBins) : -99
		var noise = !(_.includes(fixed_threshold_options, semantics)) ? 0 :
			uniformDrift({a: 0, b: 1, width:0.2})

		var fixedThreshold = (semantics == "some") ?_.min(thetaBins) :
			(semantics == "ten") ? 0.08 :
			(semantics == "fifteen") ? 0.13 :
			(semantics == "twenty") ? 0.18 :
			(semantics == "twentyfive") ? 0.23 :
			(semantics == "thirty") ? 0.28 :
			(semantics == "forty") ? 0.38 :
		  (semantics == "most") ? 0.5 :
			(semantics == "sixtyfive") ? 0.63 :
			(semantics == "best") ? sample(thetaPrior) :
		  false

  foreach(properties, function(item){
    // display(item)
    var propertyData = {
        prior: _.filter(data.prior, {stim_property: item}),
        interpretation: _.filter(data.interpretation, {stim_property: item})
    };

    var mixture_params = betaShape({
          g: uniformDrift({a: 0, b: 1, width: 0.2}),
					d: exponential({a: 1})
    })

    mapData({data: propertyData.prior}, function(d){
      observe( Beta(mixture_params), d.response0_avoidedEndval )
    })

    var prevalence_when_present = betaShape({
          g: uniformDrift({a: 0, b: 1, width: 0.2}),
					d: exponential({a: 1})
    })

    mapData({data: propertyData.prior}, function(d){
      observe( Beta(prevalence_when_present), d.response1_avoidedEndval )
    })

  	var statePrior = Infer({model: function(){
      sample(
        flip(sample(DiscretizedBeta(mixture_params))) ?
        DiscretizedBeta(prevalence_when_present) :
        NullDistribution
      )
  	}});
		var higherBetaComponent = DiscretizedBeta(prevalence_when_present)

    // var statePrior = priors[item]

     /// RSA model
   	var listener0 = cache(function(utterance) {
   	  Infer({model: function(){
   	    var state = sample(statePrior);
         var theta = (semantics == "uncertain") ? sample(thetaPrior) : fixedThreshold;
   	    var m = meaning(utterance, state, theta)
   	    condition(m)
   	    return state
   	 }})}, 10000)

      /// LIFTED PRAGMATIC
    	// var listener0_forL1 = cache(function(utterance, theta) {
    	//   Infer({model: function(){
    	//     var state = sample(statePrior);
    	//     var m = meaning(utterance, state, theta)
    	//     condition(m)
    	//     return state
    	//  }})}, 10000)

     //  var speaker1 = cache(function(state, theta) {
     //   Infer({model: function(){
     //     var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
     //     var L0 = listener0_forL1(utterance, theta);
     //     factor(speakerOptimality * L0.score(state))
     //     return utterance
     //   }})}, 10000)
		 //
     // var listener1 = cache(function(utterance) {
     //   Infer({model: function(){
     //     var state = sample(statePrior);
     //     var theta = (semantics == "uncertain") ? sample(thetaPrior) : fixedThreshold;
     //     var S1 = speaker1(state, theta)
     //     observe(S1, utterance)
     //     return state
     //   }})}, 10000)
		 //
     //   /// UNLIFTED PRAGMATIC
		 //
     //    var speaker1_unlifted = cache(function(state) {
     //     Infer({model: function(){
     //       var utterance = categorical({vs: ["generic", "silence"], ps: [cost_uttProb, 1]})
     //       var L0 = listener0(utterance);
     //       factor(speakerOptimality * L0.score(state))
     //       return utterance
     //     }})}, 10000)
		 //
     //   var listener1_unlifted = cache(function(utterance) {
     //     Infer({model: function(){
     //       var state = sample(statePrior);
     //       var S1 = speaker1_unlifted(state)
     //       observe(S1, utterance)
     //       return state
     //     }})}, 10000)

		 var L0 = semantics == "highBeta" ? higherBetaComponent : listener0("generic")

    var interpretationPrediction = addNoise(L0, noise)

   mapData({data:propertyData.interpretation}, function(d){
     // display(d)
     var scr = interpretationPrediction.score(d.binnedResponse)
     scr == -Infinity ? displayObj(d) : null
     // display(scr)
     observe(interpretationPrediction, d.binnedResponse)
   })

   // query.add(["prediction", item, -99, "prior"], expectation(statePrior))
   // query.add(["prediction", item, -99, "posterior"], expectation(interpretationPrediction))
 })

  // query.add(["prevPriorParams", -99, "nullParams", "alpha"], nullParams.a)
  // query.add(["prevPriorParams", -99, "nullParams", "beta"], nullParams.b)

   // RECORD PARAMETERS AND PREDICTIVES
   // query.add("numberOfComponents", [-99, -99, -99, numberOfComponents])
   // semantics == "most" ? query.add(["noise", -99, -99, -99], noise) : null
   // modelName == "literal"? null: query.add(["optimality", -99, -99, -99], speakerOptimality)

	// return query
}

var steps = 10000
var samples = 50

var outfile = 'results/mll/ais-cimpian-'+
	modelName+'-bdaPrior_'+semantics+'-semantics-nullDist_' +
	NullDistributionName +"_"+ steps + "-steps_" + samples + '-samples_iter' +chain+'.csv';

var fileHandle = webpplCsv.openFile(outfile)
var header_to_write = ["modelName", "semantics", "nullDist", "steps", "iter", "weight"].join(",")
webpplCsv.writeLine(header_to_write, fileHandle)

var weights = map(function(i) {

   display(i)
	 var weight = AIS(marginalLikelihood, {samples: 1, steps: steps})
	 var line_to_write = [modelName, semantics, NullDistributionName, steps, i, weight].join(",")
 	 webpplCsv.writeLine(line_to_write, fileHandle)

}, _.range(samples))

webpplCsv.closeFile(fileHandle)

"written to " + outfile;
