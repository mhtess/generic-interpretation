---
title             : "Generic statements have weak (and strong) implications"
shorttitle        : "Context-sensitive generic interpretations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Introduction

Learning from others comes in many forms. 
An expert may tolerate onlookers, a demonstrator may slow down when completing a particularly challenging part of the task, and a teacher may actively provide pedagogical examples and describe them with language [@Kline2014; @Csibra2009]. 
Informative demonstrations may be particularly useful for procedural learning (e.g., hunting seals, riding a bicycle). 
Language is uniquely powerful in its ability to convey information that is abstract or difficult to observe, or information that otherwise does not have a way of being safely acquired [e.g., learning that certain plants are poisonous, staring at the sun causes blindness; @Gelman2009]. 

The premier example of language's ability to transmit abstract knowledge comes in the form of statements that convey generalizations, otherwise known as *generic language* [e.g., "Birds fly"; @Carlson1977]. 
One of the most important roles for generic language is to provide learners with information about new or poorly understood categories.
As such, generics are ubiquitous in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], and learning from generics is believed to be central to conceptual development [@Gelman2004; @Cimpian2009].
Understanding how beliefs are updated from generic language is thus a central question in cognitive science.

<!-- But unlike statements about concrete events or exemplars  (e.g., "This bird flys"), generics convey information that is not directly observable and, in fact, resilient to counterexamples (e.g., penguins do not fly). -->
<!-- Here, we investigate how generic language is interpreted  -->

@Abelson1966 argued that generics, "...once accepted psychologically, [...] appear to be commonly taken in a rather strong sense, as though the qualifier *always* had implicitly crept into their interpretation" (p. 172).
@Gelman2002 provided adults with non-obvious generics about animals (e.g., "Bears like to eat ants") and found that the *implied prevalence* of the feature among category members (e.g., the percentage of bears that like to eat ants) was high ($\sim 85\%$). 
Similarly strong interpretations were reported using novel categories and plausibly biological properties [e.g., "Lorches have purple feathers"; @Cimpian2010, Expt. 1].
The fact that generics seem to convey strong implications provides some explanations as to why social stereotypes get propagted and maintained [@Rhodes2012].

Do all generics carry strong implications?
"Mosquitos carry malaria" really seems to only have the quantificational force of an existential claim, meaning *some mosquitos carry malaria*.
@Cohen2004 argues that there are *existential generics*, where the sentence only implies the property is present in some instances.
He points to examples such as: "Birds lay eggs. Mammals also lay eggs.", where the second sentence can be interpreted as true because of the existence of platypuses and echidnas, both of which reproduce by laying eggs.
Empirically, @Cimpian2010 (Expt. 3) found somewhat weaker interpretations for features that could be construed as accidental [e.g., "Crullets have fungus-covered claws"; see also @Khemlani2009; @Khemlani2012 for evidence for weak inferences from generics].
Such context-sensitive interpretations have been explained as theory-based expectations [@Leslie2008; @Cimpian2010theory; @Prasada2013], but these notions have not been made sufficiently precise to provide a quantative, predictive account of how generics update beliefs.
In addition, in these previous studies, the actual implied prevalence from "weak" generics is still quite high [in @Cimpian2010 Expt. 3, mean implied prevalence $= 70\%$], leaving open the question of whether generics can actually have weak interpretations (e.g. $< 50\%$).

<!-- Interpreting a generic as *most* can also be too weak: "Triangles have three sides" is a statement about *all triangles*; if a shape does not have three sides, it is not a triangle.  -->
<!-- Indeed, the differences between quantified statements and generic statements are appreciated by children as young as three years [@Gelman2015genericsQuantifiers]. -->

<!-- Flexible interpretations of generic language may be in part attributed to beliefs about the property [@Nisbett1983]. -->
<!-- Properties that are biological in nature (e.g., "Wugs have wings") are interpreted as applying to most or all of the category [@Gelman2002; @Brandone2014], while features that could be construed as incidental (e.g., "Crullets have fungus-covered claws") are interpreted as applying to relatively fewer members of the category [@Cimpian2010]. -->

@TesslerLangGenUnderReview proposed a quantitative model of generic language, which used tools from formal semantics to update a listener's prior beliefs from a generic statement.
This model accounted for truth judgments (e.g., is "Mosquitos cary malaria" true or false?) about a wide range of linguistic stimuli including generics about animals, habitual statements about actions (e.g., "John runs"), and causal language (e.g., "This herb makes animals sleepy").
This endorsement model is cast a speaker deciding whether or not to assert a generic statement to a naive listener, who is able to update its beliefs from the generic statement.
The generic listener model, which provides a mapping from a generic statement to implied prevalence, is such a quantitative account of how generics update beliefs.
This paper tests the predictions of this model.

The paper is organized as follows.
First, we review @TesslerLangGenUnderReview's computational model of generic interpretation. 
In Expt. 1, we replicate and extend the findings of @Cimpian2010 that biological and accidental properties receive differential interpretations. 
In Expt. 2, we extend these findings using a larger and more diverse stimulus set better able to test the quantitative predictions of our model.
In Expt. 3, we experimentally manipulate the background knowledge (the prevalence prior) and test its effect on interpretations of novel generics.
In each experiment, we compare our model's predictions to alternative models, finding that our quantitative model is better able to acconut for context-sensitive generic interpretations. 
Together, these results provide strong evidence for an quantitative account of generic language.

<!-- First, we test whether or not weak interpretations of generics are possible by testing interpretations of generics about a wide range of properties (Expt. 1a).  -->
<!-- Then, we measure interlocutors' prior beliefs about the prevalence of the property (Expt. 1b) using the prior elicitation paradigm in @TesslerGenerics.  -->
<!-- We compare our interpretation model's predictions, which use the empirically measured priors, to the implied prevalence data and find that our model is able to predict, with high quantitative accuracy, the context-sensitive interpretations of generics.  -->
<!-- Finally, we show that, rather than being merely correlationa in nature, the prevalence prior is causally related to generic interpretation by manipulating the prevalence prior and measuring participants interpretaions of novel generics (Expt. 2).  -->


<!-- Despite these variable truth conditions, generic sentences are thought to have charasterically "strong implications" [@Gelman2002; @Cimpian2010; @Brandone2014]. -->
<!-- It is these strong interpretations that make generics lead to stereotypes [@Rhodes2012]. -->

<!-- We recently proposed that generics communicate in an underspecified way about the prevalence of the feature. -->
<!-- That is, our model predicted that "Mosquitos carry malaria" is a felicitious utterance because listeners expect *carrying malaria* to not be widespread within a category, even when it is present in some of the category.  -->

<!-- @Cimpian2010 too found that generics predicating biological properties of kinds carry strong interpretations.  -->
<!-- The properties used in those studies were all related to body parts of animals (e.g., "Lorches have purple feathers" $\rightarrow$ *almost all lorches have purple feathers*).  -->
<!-- Bare plural nouns predicated with *accidental* properties (e.g., "Lorches have broken legs") had significantly weaker interpretations in terms of how many of the kind were expected to have the property.  -->
<!-- However, these accidental properties do not lend themselves to generic interpretation and sound infelicitous.  -->
<!-- At the same time, the average implied prevalence for the *accidental property* generics was close to 70\%, leaving open the question as to whether or not generics can have truly weak (i.e., minority-based) interpretations. -->


# Computational model

Generic language conveys generalizations about categories [@Carlson1977; @Leslie2008].
Given that probability is a useful representation for human generalization from observational data [@Shepard1987; @Tenenbaum2011], it makes sense that probability would be at the core meaning of generalizations from language. 
@TesslerLangGenUnderReview formalize such a hypothesis, positing that the semantics of generics is simple but underspecified or vague. 

Using the truth-functional tools of formal semantics [@Montague1973], the literal meaning of a generic statement can be modeled as a threshold function operating on the prevalence $p$ of the feature in the category (e.g., the proportion of mosquitos that carry malaria): $\denote{generic} = \{p > \theta\}$.
The literal meaning of quantifiers can also be described by a threshold-function (e.g., $\denote{some} = \{p > 0\}$, $\denote{most} = \{p > 0.5\}$, $\denote{all} = \{p = 1\}$).
@TesslerLangGenUnderReview posit that the corresponding threshold for the generic $\theta$ is *a priori* uncertain---formally, is drawn from a context-invariant, uniform prior distribution thresholds $P(\theta)$---and is resolved by context. 

In order for a statement to update beliefs about prevalence, a listener must have prior beliefs about prevalence.
To interpret a generic sentence, a listener draws upon abstract knowledge about properties, formalized by a prior distribution over likely prevalence levels $P(p)$. ^[
  Though not the direct focus of this work, the prevalence prior distribution can be seen as a relatively shallow formalization of theory-based expectations. 
  That is, different theory-based relationships between kinds and properties will give rise different prior distributions over prevalence. 
  The prevalence prior, thus, acts as a statistical layer between conceptual knowledge and natural language semantics. 
].
Thus the quantitative model for generic interpretation is given by:

\begin{eqnarray}
L(p, \theta \mid u) &\propto& {\delta_{\denote{u}(p, \theta)} \cdot P(p) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

Formally, the truth-functional meaning is represented by the Kronecker delta function  $\delta_{\denote{u}(p, \theta)}$ that returns probabilities proportional to $1$ when the utterance is true (i.e., when $p > \theta$) and $0$ otherwise.

\begin{eqnarray}
\delta_{\denote{u_{gen}}(p, \theta)} &\propto  & \begin{cases}
1 & \text{if } p > \theta \\
0 & \text{otherwise}
\end{cases}\label{eq:delta}
\end{eqnarray}

\red{Schematic model predictions are shown in Figure X. }

\red{Explain priors as a distribution over animals.}

<!-- 
- Figure:
  - 3 schematic priors, different fixed thresholds, generic interpretation 
  - possibly with bar plot summarizing means
  
- Figure:
  - comparison to a fixed threshold ("some") model, fixed threshold ("most") model, and L1 model (?)
-->

# Experiment 1: Replication and extension of Cimpian et al. (2010)

Our model of generic interpretation (Eq. \ref{eq:L0}) predicts that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
@Cimpian2010 found a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
Classic work in generalization suggests beliefs about the prevalence of properties include relatively fine distinctions among properties that are all biological in nature [@Nisbett1983].
For this reason, we elaborated the stimulus set from @Cimpian2010 to include three types of biological properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*) and body parts described with a vague adjective (e.g., *curly fur*). 
Here, we empirically measure the prevalence priors using a structured prior elicitation task (Expt. 1a) and use our interpretation model to predict the prevalence implied by a generic statement about a novel category (e.g., "Wugs have yellow fur"; Expt. 1b). 

<!-- %We also coded the accidental properties from Expt.~2a as either ``common'' or ``rare'' using a by-item median split based on \emph{a priori} expected prevalence when present. -->
<!-- %Most of the materials we used were from \citeauthor{Cimpian2010}.  -->
<!-- %The materials used were 30 novel animal categories (e.g. lorches, morseths, blins) each paired with a unique property.  -->
<!-- %Biological properties were made by pairing a color with a body-part (e.g. purple feathers, orange tails).  -->
<!-- %Accidental properties used the same set of body-parts but modified it with an adjective describing an accidental or disease state (e.g. broken legs, wet fur).  -->
<!-- %Each participant saw a random subset of 10 unique animal-property pairs for each type of property (biological and accidental).  -->


## Experiment 1a: Prior elicitation

This experiment measures the prevalence prior $P(p)$ in Eq. \ref{eq:L0} using a structured, prior elicitation procedure.

### Method

#### Participants

We recruited 40 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk). We chose this number of participants based on intuition from similar experiments which were designed primarily to test a quantitative model.
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
All participants were native English speakers. 
The experiment took about 5-7 minutes and participants were compensated \$0.75.

#### Procedure and materials

We constructed a stimulus set of forty different properties to explore a wide range of *a priori* beliefs about prevalence. 
These items make up four categories of properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*), body parts described with a vague adjective (e.g., *curly fur*, and body parts with in an accidental or disease state (e.g., *wet fur*).
Because pilot testing revealed more variability for items in the accidental category relative to the other types of properties, we used twice as many exemplars of accidental properties, yielding a more thorough test of the quantitative predictive power of the generic interpretation model. 
We used 8 exemplars of each of the three non-accidental properties (parts, colored parts, vague parts), and 16 exemplars of accidental properties, building on a stimulus set from @Cimpian2010.
All materials are shown in Table X of the Appendix.

In previous work, we have found that prevalence priors are well-modeled by a mixture of two Beta distributions. ^[
  The Beta distribution is chosen because the support of this distribution is numerical values between 0 - 1, which is the appropriate support for a distribution over prevalence.
]
One of these distributions represents kinds of animals who *do not have* a stable causal mechanism that could give rise to the property (e.g., *lions* and *lay eggs*), which results in prevalence or prevalence values close to or equal to 0.^[
  This assumption is similar in spirit to that employed by *Hurdle Models* of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., when modeling adverse reactions to vaccines; @hurdleModels]
]
This "null distribution" is potentially present for all features in exactly the same way (i.e., the lack of producing the feature).
The second distribution represents kinds of animals who *do have* such a mechanism, and the two parameters of this distribution are not specified *a priori* and are not the same for all properties.
Properties may also vary in the relative contribution of these two component distributions.

In this experiment, we used a two-stage structured, elicitation procedure, aimed to measure the two relevant components of the structured prior model: (1) the relative contributions of null prevalence distribution and stable prevalence distribution (i.e., the mixture parameter, or the *potential for a property to be present*), and (2) prevalence among kinds where the property is present (*prevalence when present*). 
Participants were first introduced to a "data-collection robot" that was tasked with learning about properties of animals. 
Participants were told the robot randomly selected an animal from its memory to ask the participant about (e.g., The robot says: "We recently discovered animals called feps."). 
To measure the *potential to be present* (1), the robot asked how likely it was that there was a fep with *property* (e.g., "How likely is it that there is a fep that has wings?"), to which participants reported on a scale from *unlikely* to *likely*.
To measure *prevalence when present* (2), the robot then asked the likely prevalence assuming that at least one has the property (e.g., "Suppose there is a fep that has wings. What percentage of feps do you think have wings?"). 

Participants completed a practice trial using the property *are female* to make sure they understood the meanings of these two questions.
For example, it is very likely that there is a fep that is female because almost all animals have female members (high potential to be present).
Additionally, when present, the property is only expected in about 50\% of the category.


### Data analysis and results

Question 1 elicits the potential for a property to be present in a kind (the mixture parameter of a 2-Betas mixture model). 
Question 2 elicits the prevalence in a kind where the property is present.
We assume participants' responses to both questions ($i \in \{\text{Question 1}, \text{Question 2}\}$) are generated from Beta distributions: $d_{i} \sim \text{Beta}(\gamma_i, \xi_i)$. 
Each item was modeled independently.
We implemented the model in the probabilistic programming language WebPPL [@dippl].
We learned about the credible values of the parameters of the model by running MCMC for 100,000 iterations, discarding the first 50,000 for burn-in.

The priors elicited cover a range of possible parameter values as intended Figure\ \@ref(fig:expt1-prior)A:
<!-- Figure \ref{fig:prior2}a shows a summary of the elicited priors, in terms of the diversity of $d_{potential}$ and $d_{expected}$. -->
Biological properties are expected to be \emph{a priori} more prevalent within a kind when present than accidental properties, with additional fine-grained differences within biological and accidental properties.
\red{describe scatter plot}.

To construct prevalence prior distributions, we built a Bayesian mixture-model for this prior elicitation task.
We assume that kinds for which the property is absent have prevalence levels sampled from a Beta distribution that heavily favors numbers close to 0: $\text{Beta}(\gamma = 0.01; \xi = 100)$.^[
  Note that we use the noncanonical mean $\gamma$ and concentration $\xi$ (or, inverse-variance) parameterization of the Beta distribution rather than the canonical shape (or pseudocount) parameterization for ease of posterior inference. The shape parameterization can be recovered using: $\alpha = \gamma \cdot \xi; \beta = (1 - \gamma) \cdot \xi$.
]
With that assumption, the prevalence distribution is given by:

\begin{align}
\phi & \sim \text{Beta}(\gamma_1, \xi_1) \nonumber \\ 
p & \sim \begin{cases}
		\text{Beta}(\gamma_2, \xi_2) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{T} \label{eq:priorModel}  \\
		\text{Beta}(\gamma = 0.01; \xi = 100) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{F} \\
		\end{cases}
\end{align}

This mixture model specifies the correct way to combine our two prior-elicitation questions.
Figure\ \@ref(fig:expt1-prior)B shows example reconstructed priors.
Biological properties (*biological*, *vague*, and *color* body parts) have prevalence distributions that are bimodal with peaks at 0\% and near-100\% prevalence. 
\mht{will you plot prevalence posteriors as well? probably...}
Interpretations of generics about these properties ($L$ model, Eq. \ref{eq:L0}) update these distributions to concave posteriors peaked at 100\% (Figure \ref{fig:prior2}a; red, blue and green insets); the model predicts these novel generics will be interpreted as implying the property is widespread in the category.
By contrast, accidental properties (both *rare* and *common*) follow unimodal prior distributions and update to convex posterior distributions, predicting weaker and more variable interpretations of novel generics for these properties. 


<!-- In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior resolves two technical difficulties. -->
<!-- First, it smooths effects that are clearly results of the response format.  -->
<!-- For example, a very common rating for certain events is \emph{1 time per year}. -->
<!-- Presumably participants would be just as happy reporting \emph{approximately} 1 time per year (e.g., on average, 1.2 times per year); the raw data does not reflect this due to demands of the dependent measure. -->
<!-- Second, this methodology better captures the tails of the prior distribution (i.e., very frequent or very infrequent rates) which have relatively little data and need to be regularized by the analysis. -->



<!-- We used the same structured, statistical model for the prior data from Expt.~1. -->
<!-- The only difference from Expt.~1a. is that our experimental data comes from inquiring about the parameters of the priors directly, as opposed to asking about particular samples from the prior (i.e. particular kinds) as was done in Expt.~1a.  -->
<!-- <!-- %For Expt.~2a, participants are asked questions directly targeting $\theta$ and $\gamma$ in the above model (see {\it Expt. 2a}). --> -->
<!-- We assume these two measurements follow Beta distributions ($d_{potential} \sim \text{Beta}(\gamma_{1}, \xi_{1})$; $ -->
<!-- d_{expected} \sim \text{Beta}(\gamma_{2}, \xi_{2})$), and construct single prevalence distributions, $P(x)$, by sampling from the posterior predictive distribution of prevalence as we did before: $P(x) = \int [ \phi\cdot \text{Beta} (x \mid \gamma_{2}, \xi_{2}) + (1 -  \phi) \cdot \delta_{x=0} ] \cdot \text{Beta}(\phi \mid \gamma_{1}, \xi_{1}) d\phi$. -->
<!-- We used the same uninformative priors over parameters $\phi, \gamma_{i}, \xi_{i}$ as in Expt.~1a. -->


<!-- %These convex posteriors show that generics about accidental or temporary properties come with highly uncertain interpretations, plausibly as a consequence of theory-driven considerations \cite{Cimpian2010c}.  -->


<!-- \begin{figure*} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{prevalence-implied-wPriors} -->
<!--     \caption{Understanding novel generics. (a) Prevalence prior distributions empirically elicited for 40 animal properties. -->
<!--     Parameters of the structured statistical model---$\phi$ and $\gamma$---reveal quantitative differences in beliefs about the prevalence of conceptually different types of properties (scatterplot).  -->
<!--     Inset plots show differences in shapes between biological properties (red, green, blue; bimodal) and accidental properties (orange, purple; unimodal).    -->
<!--   These differences in the prior (darker shade) give rise to the variability of $L_1$ interpretations of generic utterances (lighter shade). -->
<!--   (b) -->
<!--   Human interpretation of prevalence upon hearing a generic compared with the $L_1$ model posterior predictive.  -->
<!--     Participants and the model interpret generics differently for different property types: Generics of biological properties (red, blue, green) have  strong interpretations while generics of accidental properties (purple, orange) are weaker.  -->
<!--       Error bars denote Bayesian 95\% credible intervals. -->
<!--   } -->
<!--   \label{fig:prior2} -->
<!-- \end{figure*} -->


## Experiment 1b: Generic interpretation

This experiment measures the prevalence implied by a generic statement about a novel category in order to compare to the predictions of the generic interpretation model $L(p \mid u)$ (Eq. \ref{eq:L0}).

<!-- %The full cover story is described in {\it SI Section C} and is the same for Expt.~2c. -->

### Method

#### Participants

We recruited 40 participants over MTurk. 
The experimental design is very similar to @Cimpian2010, and we chose to have a sample size at least twice as large as the original study (original n=15). 
This is a quantitative experiment with only quantitative comparisons planned.
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with a generic about a novel category (e.g., "Feps have yellow fur.") and asked to judge prevalence: "What percentage of feps do you think have yellow fur?" 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental.
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 

### Results



<!-- We look at the posterior predictive distribution of the generic interpretation model $L$. -->
We first explore two qualitative trends predicted by the interpretation model, before proceeding to a quantitative model analysis.
Prevalence implied by a generic is predicted to vary according to the *a priori* expected prevalence: the expected value, or mean, of the prevalence prior distrbution.
\red{A mixed-effects linear model with random by-participant effects of intercept and slope indeed reveals the more prevalent a property is expected to be \emph{a priori}, the stronger the implications of a generic statement.}

Secondly, it has been suggested that generics embody a *default mode of generalization* analagous to the human inductive capacity for generalization from observations [@Leslie2007].
If generics reduce to generalization from observation, we would expect the *prevalence when present* judgments (Expt. 1a) to track the implied prevalence judgments.
Our generic interpretation model, however, predicts the generic does more than signal the presence of feature: The uncertain threshold leads *a posteriori* to preferring higher prevalence levels (Figure\ \@ref(fig:schematic-predictions)).
Consisntent with this qualitative prediction, a mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the mean prevalence when present judgments ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$).

## Model-based analysis and results

\red{Flesh out alternative models}

## Discussion

We replicated @Cimpian2010's finding of a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
We extended these findings with a broader stimulus set, discovered even more gradability in interpretations of generic statements. 
We empirically measured the prevalence priors and used our generic interpretation model to predict prevalence implied by a generic statement.

The items that receive the lowest implied prevalence are those of accidental or diseased states (e.g., *fungus-covered claws*, *broken legs*). 
As @Cimpian2010 noted (p.1472), "properties of this type do not lend themselves very well to generic predication (Cimpian & Markman, 2008; Gelman, 1988), so generics about broken legs, itchy skin, etc. are infrequent outside the laboratory."
It, thus, remains a possibility that participants treat statements such as "Lorches have broken legs" as an existential claim about the here-and-now, analagous to how "Dogs are on my front lawn" describes a particular state of affairs as opposed to something generalizable about dogs. 
We thus aim to replicate these findings of low prevalence interpretations using properties that more naturally lend themselves to generic interpretation. 

# Experiment 2

## Experiment 2a: Prevalence prior elicitation

### Methods

#### Participants

We recruited 180 participants from MTurk.
This number was arrived at with the intention of getting approximately 40 independent sets of ratings for each unique item in the experiment.
Participants were restricted to those with U.S. IP addresses and with at least a 95\% MTurk work approval rating (these same criteria apply to all experiments reported).
The experiment took on average X minutes and participants were compensated \$K.

#### Materials

We created a stimulus set composed of 74 properties.
Items were generated by the first author by considering six different classes of properties: physical characteristics (e.g., *have brown spots*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find the more obscure properties. 

#### Procedure and materials

On the first trial, participants were asked to list three kinds of animals for each of five different classes of animals: mammals, fish, birds, insects/bugs, amphibeans/reptiles. 
The five classes of animals were presented in a randomized order on the screen and there were three text boxes for each in which participants could type an animal kind.
On subsequent trials, participants were shown a random subset of five animal kinds and asked how many of each of the categories they believed had a property (e.g., "Out of 100 cheetas, how many do you think attack hikers?").
Pilot results indicated similar responses were generated by a question about percentage (e.g., "What percentage of cheetahs do you think attack hikers?").
Participants responded using a slider bar with endpoints labeled 0 and 100, and the 
exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant saw a random selection of twelve properties.

As an attention check, at the end of the prior elicitation trials, participants were asked to select, from a list of ten, all of the properties they could remember being tested on. 
The list included five properties that they had been tested on and five distractors. 

### Results

Each item received between X and Y responses. 
<!-- Distributions of responses for each item were smoothed using nonparametric density estimation for ordinal categorical variables (Li & Racine, 2003) with the np package in R (Hayfield & Racine, 2008).  -->
For each item, we then computed its belief distributionâ€™s expected value.
As intended, items covered a wide range of probabilities and expected values (see Figure 2).


## Experiment 2b: Generic interpretation

### Methods

#### Participants

We recruited N participants from MTurk. 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average X minutes and participants were compensated \$K.

#### Procedure and materials

The materials were the same as in Expt. 2a.
Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would be told facts about the new animals and be asked to translate it into the how many (out of 100) of that animal it applies to.
On each trial, participants read a bare plural statement about a familiar property $F$ (e.g., *attack hikers*) applying to a novel animal category $K$ (e.g., *Javs*).
They were then asked "How many *Ks* do you think *F*?" (e.g., "How many javs do you think attack hikers?").
Novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Participants responded using a slider bar with endpoints labeled 0 and 100, and the 
exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant completed thirty-six trials, corresponding to a random subset of the full stimulus set.

### Results

## Model-based analysis and results

## Discussion

# Experiment 3: Prior Manipulation

In this final experiment, we aim to test the causal role of the prevalence prior in generic interpretation. 
We do this by manipulating the prevalence prior and measuring the resulting influence on interpretation.

## Methods

### Participants

We recruited N participants from MTurk. 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average X minutes and participants were compensated \$K.

### Materials

From the stimulus set of 74 properties used in Expt. 2, we selected a subset of 24 items that received intermediate interpretation judgments (Expt. 2b: implied prevalence between: 40 - 65\%), which we believed would make them most susceptible to our prior manipulation.

Participants were told that scientists had recently discovered lots of new animals that we did not know existed, and that scientists were trying to understand the properties of these new animals. 
Participants were then shown "data" that other scientists had collected about a particular property.
These data were in terms of how many (out of 100) of different categories had the particular property.
These data followed one of \red{N} distributions.

### Procedure

In order to avoid potential influence from learning the structure of the task, participants completed only a single trial: Each participant saw only one property with one prevalence distribution.
Participants then click a button to show the data for each previously studied novel animal.
Data appear one at a time (upon a click), and are described in an *evidence statement* (e.g., \red{"Your team observed 100 different cheebas. Of those 100 observed, 98 cheebas had property."}) as well as displayed in a table showing the corresponding numerical data (e.g., "number with property"; \red{Figure}). 

Participants see the data from eleven different categories, though they are told the data for one kind were lost and a "?" was placed in the table (\red{Figure}).^[
  These lost results would be found in the *generic interpretation* task, Expt. 3b.
  We chose to present a "lost data" category to imply that the sampling procedure for the referent-category was random. The alternative would be to present data from ten categories and have participants rate the eleventh, which could imply that the scientists are collecting data until they find one for which the property applied generically. \red{[clean up]}
]
After participants viewed the data for ten categories (and one missing category), they are told to review the data before continuing (\red{Figure}).

Upon clicking the continue button, the data table was removed and participants were told more animals were going to be observed.
Participants were asked to predict the results for the next five categories (\red{Figure}).
Participants were given five slider bars ranging from 0 - 100, and asked to predict the next five categories.

After responding, participants then completed an attention check survey where they were asked what property the team of scientists was investigating (choosing a response from a drop-down menu with 12 options) and to input one of the data points they saw on the familiarization screen.
This attention check served to confirm that participants had encoded both relevant aspects of the experiment (the property and the prevalences).

## Results

## Discussion

# General Discussion

## Implications for social categories

## Generic identification 

## Development of weak interpretations

# References


```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
