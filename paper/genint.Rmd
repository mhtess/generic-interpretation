---
title             : "Generic statements have weak (and strong) implications"
shorttitle        : "Context-sensitive generic interpretations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
library(viridis)
theme_set(theme_few())
remove(list= ls())
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)

```

# Introduction

- is there a way to connect to the claims of Cimpian2009 about generics influencing feature-as-cause vs. feature-as-effect explanations?
  - perhaps not due to the generic vs. specific distiction, but to prior beliefs about the property?
  
- in generic endorsement: connect double generalizations ("Dogs bark") to so-called "non-generics" ("Dogs are on my front lawn"). Since "are on my front lawn" is a stative (?) property, there is only one generalizations about dogs which gets heavily restricted by the property?

<!-- Learning from others comes in many forms.  -->
<!-- An expert may tolerate onlookers, a demonstrator may slow down when completing a particularly challenging part of the task, and a teacher may actively provide pedagogical examples and describe them with language [@Kline2014; @Csibra2009].  -->
<!-- Informative demonstrations may be particularly useful for procedural learning (e.g., hunting seals, riding a bicycle).  -->

Learning from language is uniquely powerful because language can convey information that is abstract, difficult to observe, or information that otherwise does not have a way of being safely acquired [e.g., learning that certain plants are poisonous, staring at the sun causes blindness; @Gelman2009]. 
The premier example of language's ability to transmit abstract knowledge comes in the form of statements that convey generalizations, otherwise known as *generic language* [e.g., "Birds fly"; @Carlson1977]. 
One of the most important roles for generic language is to provide learners with information about new or poorly understood categories.
As such, generics are ubiquitous in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], and learning from generics is thought to be central to conceptual development [@Gelman2004; @Cimpian2009:explanations].
Understanding how beliefs are updated from generic language is thus a central question in cognitive science.

<!-- But unlike statements about concrete events or exemplars  (e.g., "This bird flys"), generics convey information that is not directly observable and, in fact, resilient to counterexamples (e.g., penguins do not fly). -->
<!-- Here, we investigate how generic language is interpreted  -->

@Abelson1966 argued that generics, "...once accepted psychologically, [...] appear to be commonly taken in a rather strong sense, as though the qualifier *always* had implicitly crept into their interpretation" (p. 172).
@Gelman2002 provided adults with generics about animals with non-obvious properties (e.g., "Bears like to eat ants") and found that the *implied prevalence* of the feature among category members (e.g., the percentage of bears that like to eat ants) was high ($\sim 85\%$). 
Similarly strong interpretations were reported using novel categories with biological properties [e.g., "Lorches have purple feathers"; @Cimpian2010, Expt. 1].
The fact that generics seem to convey strong implications provides some explanations as to why social stereotypes get propagted and maintained [@Rhodes2012].

Do all generics carry strong implications?
"Mosquitos carry malaria" really seems to only have the quantificational force of an existential claim, meaning *some mosquitos carry malaria*.
This may not be a unique example: @Cohen2004 argues for a panolply of conditions that give rise to *existential generics* (e.g., \red{what is Cohen's condition here?} in "Birds lay eggs. Mammals also lay eggs.": the second sentence can be interpreted as true because of the existence of platypuses and echidnas, both of which reproduce by laying eggs).
Using a similar implied prevalence task as @Gelman2002, @Cimpian2010 (Expt. 3) found somewhat weaker interpretations for features that could be construed as accidental [e.g., "Crullets have fungus-covered claws"; see also @Khemlani2009; @Khemlani2012 for evidence for weak inferences from generics].
Such property-specific interpretations have been explained as theory-based expectations [@Leslie2008; @Cimpian2010theory; @Prasada2013], but these notions have not been made sufficiently precise to provide a quantative, predictive account of how generics update beliefs.
In addition, in these previous studies, the actual implied prevalence from "weak" generics is still quite high [in @Cimpian2010 Expt. 3, mean implied prevalence $= 70\%$], leaving open the question of whether generics can actually have weak interpretations (e.g., minority interpretations: $< 50\%$).

<!-- Interpreting a generic as *most* can also be too weak: "Triangles have three sides" is a statement about *all triangles*; if a shape does not have three sides, it is not a triangle.  -->
<!-- Indeed, the differences between quantified statements and generic statements are appreciated by children as young as three years [@Gelman2015genericsQuantifiers]. -->

<!-- Flexible interpretations of generic language may be in part attributed to beliefs about the property [@Nisbett1983]. -->
<!-- Properties that are biological in nature (e.g., "Wugs have wings") are interpreted as applying to most or all of the category [@Gelman2002; @Brandone2014], while features that could be construed as incidental (e.g., "Crullets have fungus-covered claws") are interpreted as applying to relatively fewer members of the category [@Cimpian2010]. -->

@TesslerLangGenUnderReview proposed a quantitative model of generic language, which combines the tools of formal semantics with a Bayesian model to show a generic statement can update a listener's prior beliefs.
This model accounted for truth judgments (e.g., is "Mosquitos cary malaria" true or false?) about a wide range of linguistic stimuli including generics about animals, habitual statements about actions (e.g., "John runs"), and causal language (e.g., "This herb makes animals sleepy").
This endorsement model is cast a speaker deciding whether or not to assert a generic statement to a naive listener, who is able to update its beliefs from the generic statement.
The generic listener model, which provides a mapping from a generic statement to implied prevalence, is such a quantitative account of how generics update beliefs.
This paper tests the predictions of this model.
To do so, we also look for evidence for *weak generics*, which provide the strongest test of a unified quantitative account. 

The paper is organized as follows.
First, we review @TesslerLangGenUnderReview's computational model of generic interpretation. 
In Expt. 1, we replicate and extend the findings of @Cimpian2010 that biological and accidental properties receive differential interpretations. 
In Expt. 2, we extend these findings using a larger and more diverse stimulus set better able to test the quantitative predictions of our model and explore the existence of "weak generics".
In Expt. 3, we experimentally manipulate the background knowledge (the prevalence prior) and test its effect on interpretations of novel generics.
In each experiment, we compare our model's predictions to alternative models, finding that our quantitative model is best able to account for context-sensitive generic interpretations. 
Together, these results provide strong evidence for an quantitative understanding of generic language.

<!-- First, we test whether or not weak interpretations of generics are possible by testing interpretations of generics about a wide range of properties (Expt. 1a).  -->
<!-- Then, we measure interlocutors' prior beliefs about the prevalence of the property (Expt. 1b) using the prior elicitation paradigm in @TesslerGenerics.  -->
<!-- We compare our interpretation model's predictions, which use the empirically measured priors, to the implied prevalence data and find that our model is able to predict, with high quantitative accuracy, the context-sensitive interpretations of generics.  -->
<!-- Finally, we show that, rather than being merely correlationa in nature, the prevalence prior is causally related to generic interpretation by manipulating the prevalence prior and measuring participants interpretaions of novel generics (Expt. 2).  -->


<!-- Despite these variable truth conditions, generic sentences are thought to have charasterically "strong implications" [@Gelman2002; @Cimpian2010; @Brandone2014]. -->
<!-- It is these strong interpretations that make generics lead to stereotypes [@Rhodes2012]. -->

<!-- We recently proposed that generics communicate in an underspecified way about the prevalence of the feature. -->
<!-- That is, our model predicted that "Mosquitos carry malaria" is a felicitious utterance because listeners expect *carrying malaria* to not be widespread within a category, even when it is present in some of the category.  -->

<!-- @Cimpian2010 too found that generics predicating biological properties of kinds carry strong interpretations.  -->
<!-- The properties used in those studies were all related to body parts of animals (e.g., "Lorches have purple feathers" $\rightarrow$ *almost all lorches have purple feathers*).  -->
<!-- Bare plural nouns predicated with *accidental* properties (e.g., "Lorches have broken legs") had significantly weaker interpretations in terms of how many of the kind were expected to have the property.  -->
<!-- However, these accidental properties do not lend themselves to generic interpretation and sound infelicitous.  -->
<!-- At the same time, the average implied prevalence for the *accidental property* generics was close to 70\%, leaving open the question as to whether or not generics can have truly weak (i.e., minority-based) interpretations. -->


# Computational model

Generic language conveys generalizations about categories [@Carlson1977; @Leslie2008].
Probability is a useful representation for human generalization from observational data [@Shepard1987; @Tenenbaum2011], and so it makes sense that probability would be at the core of the meaning of generalizations from language. 
@TesslerLangGenUnderReview formalize such a hypothesis, positing that the semantics of generics has to do with probability but in an underspecified or vague way.

Using the truth-functional tools of formal semantics [@Montague1973], the literal meaning of a generic statement can be modeled as a threshold function operating on the prevalence $p$ of the feature in the category (e.g., the proportion of mosquitos that carry malaria): $\denote{generic} = \{p > \theta\}$.
The literal meaning of quantifiers can also be described by a threshold-function (e.g., $\denote{some} = \{p > 0\}$, $\denote{most} = \{p > 0.5\}$, $\denote{all} = \{p = 1\}$).
@TesslerLangGenUnderReview posit that the corresponding threshold for the generic $\theta$ is *a priori* uncertain---formally, is drawn from a context-invariant, uniform prior distribution thresholds $P(\theta)$---and is resolved by context. 

In order for a statement to update beliefs about prevalence, a listener must have prior beliefs about prevalence.
To interpret a generic sentence, a listener draws upon abstract knowledge about properties, formalized by a prior distribution over likely prevalence levels $P(p)$. ^[
  Though not the direct focus of this work, the prevalence prior distribution can be seen as a relatively shallow formalization of theory-based expectations. 
  Different theory-based relationships between kinds and properties will give rise different prior distributions over prevalence. 
  The prevalence prior, thus, acts as a statistical layer between conceptual knowledge and natural language semantics. 
]
Thus the quantitative model for generic interpretation is given by:

\begin{eqnarray}
L(p, \theta \mid u) &\propto& {\delta_{\denote{u}(p, \theta)} \cdot P(p) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

Formally, the truth-functional meaning is represented by the Kronecker delta function  $\delta_{\denote{u}(p, \theta)}$ that returns probabilities proportional to $1$ when the utterance is true (i.e., when $p > \theta$) and $0$ otherwise.

\begin{eqnarray}
\delta_{\denote{u_{gen}}(p, \theta)} &\propto  & \begin{cases}
1 & \text{if } p > \theta \\
0 & \text{otherwise}
\end{cases}\label{eq:delta}
\end{eqnarray}

The interpretation model computes a posterior distribution on prevalence by considering different possible thresholds $\theta$.
Given a particular value of $\theta$, the literal semantics rules out all prevalences $p$ below $\theta$.
With a uniform prior over prevalence ($P(p) = \text{Uniform}(0, 1)$), the posterior on prevalence will be uniform over all prevalence levels above $\theta$ (Figure\ \@ref(fig:modelSimulations)B; top row).
Because higher prevalence levels are consistent with more thresholds, the generic interpretation model returns a non-uniform posterior that favors higher prevalence levels (Figure\ \@ref(fig:modelSimulations)B; top row, right-most column).
<!-- , which has uncertainty over the threshold and marginalizes over different possible thresholds,  -->
This behavior is consistent with the intuition that, without strong background knowledge, generics imply that most or all of the category have the property.

Context-sensitive interpretations come from different prior distributions over prevalence $P(p)$.
The prevalence prior for a purely abstract property may be given by a uniform distribution, while prevalence priors for familiar properties may display interesting structure reflecting domain-specific beliefs about the property.
Figure\ \@ref(fig:modelSimulations)A shows hypothetical prevalence priors for the features *fly* and *carry malaria* as well as an abstract property *Y* without structured background knowledge. 
The distribution over a biological property like *fly* (which would give rise to interpreting generics like "Ducks fly") is bimodal with modes near 0\% and 100\%, reflecting the idea that for an arbitrary category, it is likely that 0\% of them fly (e.g., dogs, rabbits, etc...); however, among categories with at least some members who fly, we would expect all or nearly all of them to fly (hence, the second mode near 100\%).
The distribution over prevalence for a somewhat accidental property like *carry malaria* (as in "Mosquitos carry malaria") also has substantial probability mass at or near 0\%, since most categories do not have the property present at all. 
However, among categories with at least a few members who carry malaria, it is very unlikely that all or even many carry malaria; the second mode of this distribution is near 10\%. 
In this way, the prevalence prior can be viewed as a distribution over different categories (and their associated prevalence of the feature).
<!-- which can be thought of as levels of prevalence given different categories within a comparison class. -->
<!-- For simplicity, we assume the comparison class relevant for interpreting generics about novel animal categories is *other animals*. -->

The generic interpretation model balances the prior probability of different prevalence levels with the overall preference for higher prevalence levels (as described above), producing interpretations that appear similar to different quantified statements depending on the context (Figure\ \@ref(fig:modelSimulations)B). 
For the biological property prior, "Xs fly" is interpreted most similarly to "Most Xs fly" and not "Some Xs fly".
For the accidental property prior, "Xs carry malaria" is more similar to "Some Xs carry malaria" than  "Most Xs carry malaria". 
We test these context-sensitive interpretations in the following experiments. 

<!-- Considering different fixed-thresholds in conjunction with structured background knowledge also illuminates the unique behavior of the uncertain threshold model. -->

<!-- The literal interpretation of the quantifier "some" (threshold = 0.01) rules out only the lowest prevalence levels, returning an intuitively compelling interpretation for "Xs carry malaria" (*very few Xs carry malaria*); however, even given strong prior knowledge about the distribution of the property *fly*, "Some Xs fly" leaves open the possibility that very few Xs fly -->

<!-- A threshold that is slightly higher (threshold = 0.33; "more than a third"), when coupled with strong prior knowledge (e.g., for "Xs fly") can give  -->
<!-- The quantifier "most" would correspond to a threshold at 0.5, which like "more than a third", can result in intuitive interpretations for "Xs fly" (the posterior over prevalence suggests *almost all Xs fly*); -->

<!-- Different prevalence priors give rise to different interpretations. -->
<!-- Figure\ \@ref(fig:modelSimulations)B shows different interpretations (posterior distributions over prevalence) given different fixed thresholds (corresponding to different quantifiers) as well as the uncertain threshold, corresponding to the generic. -->

<!-- The interpretation model computes a posterior distribution on prevalence by considering different possible thresholds $\theta$. -->
<!-- Figure\ \@ref(fig:modelSimulations)B (top-row) shows what the model would believe given different thresholds (columns). -->
<!-- If the threshold were to be very high (e.g., $\theta = 0.75$), only the highest prevalence levels would be consistent with the utterance. -->
<!-- As the threshold decreases, more and more prevalence levels would be true. -->
<!-- Roughly speaking, the uncertain-threshold interpretation model averages over these possibilities, resulting in a posterior distribution that favors higher prevalence levels because they are consistent with more thresholds. -->
<!-- The endorsement model inverts the interpretation model and predicts higher rates of endorsement as the speaker's belief about the prevalence (referent-prevalence) increases. -->


```{r modelSimulations, fig.width = 10, fig.asp = 0.6, fig.cap="Model simulations. A: Prevalence priors reflect knowledge about properties. An abstract property could be given a uniform distribution, while familiar properties display more complex structure. B: Posterior distributions over prevalence (interpretations) given different fixed-threshold quantifiers and the uncertain threshold generic."}
load(file = "cached_results/modelSims-priors_fixedT_uncertainT.RData")
get.colors <- function(pal) brewer.pal(brewer.pal.info[pal, "maxcolors"], pal)
spectrum.color.palette <- get.colors("Blues")


fig.sims.priors <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src %in% c("priors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("Xs Y (uniform)", "Xs fly (biological)", "Xs carry malaria (accidental)")),
                src = factor(src, levels = c( "priors"),
                             labels = c(
                               '\n prevalence prior'
                                        ))), 
       aes(x = state))+
    geom_density(aes(y = ..scaled..), fill= 'black', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Probability density (scaled)") +
    xlab("Prior Prevalence")+
    scale_color_solarized()+
    scale_fill_solarized()+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_blank(),
          legend.position = "none"
          )


fig.sims.distributions <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "accidental_rare"),
           src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "posteriors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "accidental_rare"),
                                    labels = c("Xs Y\n[uniform]", "Xs fly\n[biological]", "Xs carry malaria \n [accidental]")),
                src = factor(src, levels = c( "fixed_0.1", "fixed_0.33",
                                              "fixed_0.5", "posteriors"),
                             labels = c(
                           #    'prevalence prior',
                               '"some"\n(threshold = 0.01)',
                               '"more than a third"\n(threshold = 0.33)',
                              '"most"\n(threshold = 0.5)',
                               'generic\n(uncertain threshold)'
                                        ))), 
       aes(x = state, fill = src, color = src))+
    geom_density(aes(y = ..scaled..), 
                 color ='black', size = 0.6, alpha = 0.7, adjust = 1.1)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Scaled posterior density") +
    xlab("Implied Prevalence")+
    # scale_fill_brewer(palette=2, type = "seq", direction = -1)+
    # scale_color_brewer(palette=2, type = "seq", direction = -1)+
    scale_color_manual( values =  c(spectrum.color.palette[c(8,5,2)], "#238b45"))+
    scale_fill_manual( values = c(spectrum.color.palette[c(8,5,2)], "#238b45") )+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_text(angle = 0, size = 12),
          legend.position = "none",
          axis.title.y = element_blank())

sims.combined.summary <- sims.combined %>%
         filter(
           PriorShape %in% c("uniform","biological_rare", "accidental_rare"),
           src %in% c("fixed_0.1", "fixed_0.5", "posteriors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, 
                                    levels = c( "accidental_rare", "biological_rare", "uniform"),
                                    labels = c("uniform", "Xs carry malaria\n[accidental]", 
                                               "Xs fly\n[biological]")),
                src = factor(src, 
                             levels = c( 
                               "fixed_0.1", "fixed_0.5", "posteriors"),
                             labels = c(
                               '"some" (threshold = 0.01)',
                              '"most" (threshold = 0.5)',
                               'generic (uncertain threshold)'
                                        ))) %>%
    group_by(PriorShape, src) %>%
    summarize(expval = mean(state))
  
  
fig.sims.bars <- ggplot(sims.combined.summary %>% mutate(blank = '\n'), 
       aes(x = PriorShape, y = expval, fill = src))+
    geom_col(color = 'black', 
             position = position_dodge(), alpha = 0.8, width = 0.5)+
    theme_few() +
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Average\nimplied prevalence")+
    scale_color_manual( values = spectrum.color.palette )+
    scale_fill_manual( values = spectrum.color.palette )+
    #scale_color_solarized()+
    #scale_fill_solarized()+
    facet_wrap(~blank)+
    coord_flip()+
    guides(fill = F)+
    theme(axis.title.y = element_blank(),
          axis.text.y = element_blank())

cowplot::plot_grid(
  fig.sims.priors + theme(plot.margin = unit(c(6, 3, 6, 0), "pt")), 
  fig.sims.distributions + theme(plot.margin = unit(c(6, 0, 6, 0), "pt")),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(0.25, 1)
)
```

<!-- 
- Figure:
  - 3 schematic priors, different fixed thresholds, generic interpretation 
  - possibly with bar plot summarizing means
  
- Figure:
  - comparison to a fixed threshold ("some") model, fixed threshold ("most") model, and L1 model (?)
-->

# Experiment 1: Replication and extension of Cimpian et al. (2010)

Our model of generic interpretation (Eq. \ref{eq:L0}) predicts that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
@Cimpian2010 found a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
Classic work in generalization suggests beliefs about the prevalence of properties include relatively fine distinctions among properties that are all biological in nature [@Nisbett1983].
For this reason, we elaborated the stimulus set from @Cimpian2010 to include three types of biological properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*) and body parts described with a vague adjective (e.g., *curly fur*). 
Here, we empirically measure the prevalence priors using a structured prior elicitation task (Expt. 1a) and use our interpretation model to predict the prevalence implied by a generic statement about a novel category (e.g., "Wugs have yellow fur"; Expt. 1b). 

<!-- %We also coded the accidental properties from Expt.~2a as either ``common'' or ``rare'' using a by-item median split based on \emph{a priori} expected prevalence when present. -->
<!-- %Most of the materials we used were from \citeauthor{Cimpian2010}.  -->
<!-- %The materials used were 30 novel animal categories (e.g. lorches, morseths, blins) each paired with a unique property.  -->
<!-- %Biological properties were made by pairing a color with a body-part (e.g. purple feathers, orange tails).  -->
<!-- %Accidental properties used the same set of body-parts but modified it with an adjective describing an accidental or disease state (e.g. broken legs, wet fur).  -->
<!-- %Each participant saw a random subset of 10 unique animal-property pairs for each type of property (biological and accidental).  -->


## Experiment 1a: Prior elicitation

This experiment measures the prevalence prior $P(p)$ in Eq. \ref{eq:L0} using a structured, prior elicitation procedure.

### Method

#### Participants

We recruited 40 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk). We chose this number of participants based on intuition from similar experiments which were designed primarily to test a quantitative model.
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
All participants were native English speakers. 
The experiment took about 5-7 minutes and participants were compensated \$0.75.

#### Procedure and materials

We constructed a stimulus set of forty different properties to explore a wide range of *a priori* beliefs about prevalence. 
These items make up four categories of properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*), body parts described with a vague adjective (e.g., *curly fur*), and body parts with in an accidental or disease state (e.g., *wet fur*).
Because pilot testing revealed more variability for items in the accidental category relative to the other types of properties, we used twice as many exemplars of accidental properties, yielding a more thorough test of the quantitative predictive power of the generic interpretation model. 
We used eight exemplars of each of the three non-accidental properties (parts, colored parts, vague parts), and sixteen exemplars of accidental properties, building on a stimulus set from @Cimpian2010.
All materials are shown in Table X of the Appendix.

In previous work, we have found that prevalence priors are well-modeled by a mixture of two Beta distributions. ^[
  The Beta distribution is chosen because the support of this distribution is numerical values between 0 - 1, which is the appropriate support for a distribution over prevalence.
]
One of these distributions represents kinds of animals who *do not have* a stable causal mechanism that could give rise to the property (e.g., *lions* do not have a stable mechanism by which they could *lay eggs*), which results in prevalence levels close to or equal to 0.^[
  This assumption is similar in spirit to that employed by *Hurdle Models* of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., when modeling adverse reactions to vaccines; @hurdleModels]
]
This "null distribution" is potentially present for all features in exactly the same way (i.e., the lack of producing the feature).
The second distribution represents kinds of animals who *do have* such a mechanism; we cannot specify the parameters of this second distribution *a priori* as they are likely different for different properties.
Finally, the relative contribution of these two component distributions can also not be specified *a priori*. 
Thus, we create an elicitation task to measure these two aspects of the prior: (1) the relative contributions of null prevalence distribution and stable prevalence distribution (i.e., the mixture parameter, or $P(\text{feature is present}) = P(p > 0)$), and (2) prevalence among kinds where the property is present (*prevalence when present* or $P(p \mid p > 0)$)). 

Participants were first introduced to a "data-collection robot" that was tasked with learning about properties of animals. 
Participants were told the robot randomly selected an animal from its memory to ask the participant about (e.g., The robot says: "We recently discovered animals called feps."). 
To measure the mixture parameter ($P(\text{feature is present})$), the robot asked how likely it was that there *was a fep with property* (e.g., "How likely is it that there is a fep that has wings?"), to which participants reported on a scale from *unlikely* to *likely*.
To measure *prevalence when present* ($P(p \mid p > 0)$), the robot then asked the likely prevalence assuming that at least one has the property (e.g., "Suppose there is a fep that has wings. What percentage of feps do you think have wings?"). 

Participants completed a practice trial using the property *are female* to make sure they understood the meanings of these two questions.
For example, it is very likely that there is a fep that is female because almost all animals have female members (high $P(\text{feature is present})$).
Additionally, when present, the property is only expected in about 50\% of the category.


### Data analysis and results

Question 1 elicits the potential for a property to be present in a kind (the mixture parameter of a 2-Betas mixture model). 
Question 2 elicits the prevalence in a kind where the property is present.
The priors elicited display a range of possible parameter values Figure\ \@ref(fig:cimpian-prevPrior)A.
<!-- Figure \ref{fig:prior2}a shows a summary of the elicited priors, in terms of the diversity of $d_{potential}$ and $d_{expected}$. -->
Biological properties are likely to be present and when present, are likely to be widespread (top right corner of scatter plot).
More specific properties (either using gradable adjectives or color adjectives) are expected to be slightly less prevalent among the kinds where the property is present, perhaps reflecting the fact that the same kind of animal can come in many different colors or that gradable properties (e.g., *big claws*) might not be ubiquitous in a category.
Finally, accidental properties are as likely to be present in a category as the color or gradable properties (same $P(\text{feature is present})$), but are not expected to be widespread even when present in the category (low *prevalence when present*).

From these two elicitation questions, we can reconstruct what the prevalence priors should look like. 
We assume that kinds for which the property is absent have prevalence levels sampled from a Beta distribution that heavily favors numbers close to 0: $\text{Beta}(\gamma = 0.01; \xi = 100)$.^[
  Note that we use the noncanonical mean $\gamma$ and concentration $\xi$ (or, inverse-variance) parameterization of the Beta distribution rather than the canonical shape (or pseudocount) parameterization for ease of posterior inference. The shape parameterization can be recovered using: $\alpha = \gamma \cdot \xi; \beta = (1 - \gamma) \cdot \xi$.
]
With that assumption, the prevalence distribution is given by:^[
All modeling results hold when this null distribution is assumed to be even more left-skewed $\text{Beta}(\gamma = 0.001; \xi = 1000)$ or just a delta-function at zero $\delta_{p=0}$.
]

\begin{align}
\phi & \sim \text{Beta}(\gamma_1, \xi_1) \nonumber \\ 
p & \sim \begin{cases}
		\text{Beta}(\gamma_2, \xi_2) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{T} \label{eq:priorModel}  \\
		\text{Beta}(\gamma = 0.01; \xi = 100) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{F} \\
		\end{cases}
\end{align}

We assume participants' responses to both questions ($i \in \{\text{Question 1}, \text{Question 2}\}$) are generated from Beta distributions: $d_{i} \sim \text{Beta}(\gamma_i, \xi_i)$, and put uninformative priors over the parameters of each: $\gamma_i \sim \text{Uniform}(0, 1); \xi_i \sim \text{Uniform}(0, 100)$. 
We implemented this Bayesian mixture-model in the probabilistic programming language WebPPL [@dippl].
To learn about the credible values of the parameters of the model, we ran MCMC on each item independently for 100,000 iterations, discarding the first 50,000 for burn-in.

<!-- This mixture model specifies the correct way to combine our two prior-elicitation questions. -->
Figure\ \@ref(fig:cimpian-prevPrior)B shows example reconstructed priors.
Biological properties (*biological*, *vague*, and *color* body parts) have prevalence distributions that are bimodal with peaks at 0\% and near-100\% prevalence, but differ in their variance around the 100\%-mode. 
By contrast, accidental properties do not have a substantial second mode.
This variability in prevalence priors leads the generic interpretation model to predict different prevalence levels implied by the generic.

<!-- predicting weaker and more variable interpretations of novel generics for these properties.  -->
<!-- \mht{will you plot prevalence posteriors as well? probably...} -->
<!-- Interpretations of generics about these properties ($L$ model, Eq. \ref{eq:L0}) update these distributions to concave posteriors peaked at 100\% (Figure \ref{fig:prior2}a; red, blue and green insets); the model predicts these novel generics will be interpreted as implying the property is widespread in the category. -->




<!-- To construct prevalence prior distributions, we built a Bayesian mixture-model for this prior elicitation task. -->


```{r cimpian-prevPrior, fig.width = 9, fig.asp = 0.4, fig.cap="Prevalence priors for items from Cimpian et al. (2010). A: Latent parameters governing prevalence priors are different for different kinds of properties. B: The diversity in parameters gives rise to different underlying distributions over prevalence, which the generic interpretation model uses to make predictions."}
load("cached_results/cimpian-prevPrior-interpretations.Rdata")
# m.rs.priors.reconstructed.subset,
     # df.c.prior.bs.wide, 
     # df.c.int.bs,

fig.prevPriors <- ggplot(m.rs.priors.reconstructed.subset, 
       aes( x = prevalence,
                                    fill = stim_type))+
    stat_density ( aes(y = ..scaled.. ), color = 'black')+
    facet_wrap(~property, nrow = 2)+
    scale_x_continuous(limits = c(-0.01,1.01), breaks = c(0, 1)) +
    scale_y_continuous(limits = c(-0.01,1.01), breaks = c(0, 1)) +
    theme(strip.text.y = element_text(angle = 0))+   guides(fill=guide_legend(title="Property type"))+
  ylab("Probability density (scaled)")+
  xlab("Prevalence")

fig.prevParams <- ggplot(df.c.prior.bs.wide %>%
         mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "gradable adj + part",
                                                    "color adj + part",
                                                    "accidental"))), 
       aes( x = mixture_mean, xmin = mixture_lower, xmax = mixture_upper,
            y = pwp_mean, ymin = pwp_lower, ymax = pwp_upper,
            fill = `Property type`))+
  geom_errorbar(alpha = 0.3) + geom_errorbarh(alpha = 0.3) + 
  geom_point(shape = 21, size = 2.6, color = 'black')+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  ylab("Prevalence when present")+
  xlab("P(feature is present)")



cowplot::plot_grid( 
  cowplot::plot_grid(
    fig.prevParams + theme(plot.margin = unit(c(6, 6, 0, 0), "pt"),
                           legend.position = 'none'),
    fig.prevPriors + theme(plot.margin = unit(c(6, 0, 0, 0), "pt"),
                           legend.position = 'none'), 
    nrow = 1,
    labels = c("A", "B"),
    rel_widths = c(0.33, 0.66)
  ), 
  cowplot::get_legend(
    fig.prevPriors + theme(
      legend.position="bottom"
      #legend.text = element_text(size = 16),
      #legend.title = element_text(size = 16)
                           )
    ), ncol = 1, rel_heights = c(1, .2))

```



<!-- In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior resolves two technical difficulties. -->
<!-- First, it smooths effects that are clearly results of the response format.  -->
<!-- For example, a very common rating for certain events is \emph{1 time per year}. -->
<!-- Presumably participants would be just as happy reporting \emph{approximately} 1 time per year (e.g., on average, 1.2 times per year); the raw data does not reflect this due to demands of the dependent measure. -->
<!-- Second, this methodology better captures the tails of the prior distribution (i.e., very frequent or very infrequent rates) which have relatively little data and need to be regularized by the analysis. -->



<!-- We used the same structured, statistical model for the prior data from Expt.~1. -->
<!-- The only difference from Expt.~1a. is that our experimental data comes from inquiring about the parameters of the priors directly, as opposed to asking about particular samples from the prior (i.e. particular kinds) as was done in Expt.~1a.  -->
<!-- <!-- %For Expt.~2a, participants are asked questions directly targeting $\theta$ and $\gamma$ in the above model (see {\it Expt. 2a}). -->
<!-- We assume these two measurements follow Beta distributions ($d_{potential} \sim \text{Beta}(\gamma_{1}, \xi_{1})$; $ -->
<!-- d_{expected} \sim \text{Beta}(\gamma_{2}, \xi_{2})$), and construct single prevalence distributions, $P(x)$, by sampling from the posterior predictive distribution of prevalence as we did before: $P(x) = \int [ \phi\cdot \text{Beta} (x \mid \gamma_{2}, \xi_{2}) + (1 -  \phi) \cdot \delta_{x=0} ] \cdot \text{Beta}(\phi \mid \gamma_{1}, \xi_{1}) d\phi$. -->
<!-- We used the same uninformative priors over parameters $\phi, \gamma_{i}, \xi_{i}$ as in Expt.~1a. -->


<!-- %These convex posteriors show that generics about accidental or temporary properties come with highly uncertain interpretations, plausibly as a consequence of theory-driven considerations \cite{Cimpian2010c}.  -->




## Experiment 1b: Generic interpretation

This experiment measures the prevalence implied by a generic statement about a novel category in order to compare to the predictions of the generic interpretation model $L(p \mid u)$ (Eq. \ref{eq:L0}).

<!-- %The full cover story is described in {\it SI Section C} and is the same for Expt.~2c. -->

### Method

#### Participants

We recruited 40 participants over MTurk. 
The experimental design is very similar to @Cimpian2010, and we chose to have a sample size at least twice as large as the original study (original n=15). 
This is a quantitative experiment with only quantitative comparisons planned.
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with a generic about a novel category (e.g., "Feps have yellow fur.") and asked to judge prevalence: "What percentage of feps do you think have yellow fur?" 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental.
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 

### Results



<!-- We look at the posterior predictive distribution of the generic interpretation model $L$. -->
We first explore two qualitative trends predicted by the interpretation model, before proceeding to a quantitative model analysis.
Prevalence implied by a generic is predicted to vary according to the *a priori* expected prevalence: the expected value, or mean, of the prevalence prior distrbution.
\red{A mixed-effects linear model with random by-participant effects of intercept and slope indeed reveals the more prevalent a property is expected to be \emph{a priori}, the stronger the implications of a generic statement.}

Secondly, it has been suggested that generics embody a *default mode of generalization* analagous to the human inductive capacity for generalization from observations [@Leslie2007].
If generics reduce to generalization from observation, we would expect the *prevalence when present* judgments (Expt. 1a) to track the implied prevalence judgments.
Our generic interpretation model, however, predicts the generic does more than signal the presence of feature: The uncertain threshold leads *a posteriori* to preferring higher prevalence levels (Figure\ \@ref(fig:schematic-predictions)).
Consisntent with this qualitative prediction, a mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the mean prevalence when present judgments ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$).

## Model-based analysis and results

\red{Flesh out alternative models}

```{r cimpian-modelingResults, fig.width = 10, fig.asp = 0.57, fig.cap="Implied prevalence data and model predictions for items from Cimpian et al. (2010). A: Mean implied prevalence ratings for forty stimuli. B: Implied prevalence and generics model prections collapsed across property type. C: Posterior predictive model fits for quantitative models based on (i) the mean of the prevalence prior, (ii) a threshold semantics fixed at 0.01 (\"some\"), (iii) a threshold semantics fixed at 0.5 (\"most\"), and (iv) an uncertain threshold semantics (generic). Error bars denote bootstrapped 95% confidence intervals for the behavioral data and 95% highest posterior density intervals for the model predictions."}

load(file = "cached_results/cimpian-modelFits-interpretations.RData")
load(file = "cached_results/cimpian-interpretations-95ci.RData")



fig.cimpian.interpretation.95ci <- df.c.int.bs %>%
  ungroup() %>%
  mutate(stim_property = factor(stim_property, 
                           levels =  with(df.c.int.bs, stim_property[order(-mean)])),
         stim_type = ifelse(stim_type == "disease", "accidental", stim_type),
          "Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = stim_property, y = mean, ymin = ci_lower, ymax = ci_upper, 
                 fill = `Property type`))+
  geom_col(position = position_dodge(), color = 'black')+
  geom_errorbar(position = position_dodge(), alpha = 0.3)+
  #geom_pirate(bars = F, violins = F, width_points = 0.2)+
  #coord_flip()+
  #scale_color_viridis(discrete = T)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.title.x = element_blank())+
  ylab("Implied prevalence")


fig.cimpian.interpretation.stimtype <- md.impprev.type %>%
  mutate("Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")),
         src  = factor(src, levels = c("data","model"),
                       labels = c("data", "generics model"))) %>%
  ggplot(., aes( x = src,
                        y = mean, ymin = ci_lower, ymax = ci_upper,
                 fill = `Property type`))+
  geom_col(position = position_dodge(0.7), width = 0.7, color = 'black')+
  #geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_errorbar(alpha = 0.7, position = position_dodge(0.7), width = 0.3)+
  #geom_point(shape = 21, size = 2.6, color = 'black')+
  #facet_wrap(~src, nrow = 1)+
  #scale_x_continuous(limits = c(0,1.01), breaks = c(0, 0.5, 1)) +
  scale_y_continuous(limits = c(0,1.01), breaks = c(0,0.5, 1)) +
  #coord_fixed()+
  #xlab("Model prediction")+
  ylab("Implied prevalence")+
  theme(axis.title.x = element_blank())


md.impprev.plotting <- md.impprev %>% 
  filter((Parameter == "posterior") | (semantics == "uncertain")) %>%
  mutate(src = paste(Parameter, "_", semantics, sep =""),
         src = factor(src, levels = c("prior_uncertain", "posterior_some",
                                      "posterior_most", "posterior_uncertain"),
                      labels = c("Prevalence prior mean",
                                 '"some"\n(threshold = 0.01)',
                                 '"most" + noise\n(threshold = 0.5)',
                                 'generic\n(uncertain threshold)')),
         "Property type" = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "color adj + part",
                                                    "gradable adj + part",
                                                    "accidental")))



fig.cimpian.interpretation.models <- ggplot(md.impprev.plotting, 
       aes( x = model_MAP, xmin = model_lower, xmax = model_upper, 
                        y = mean, ymin = ci_lower, ymax = ci_upper,
                 fill = `Property type`))+
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
  geom_point(shape = 21, size = 2.6, color = 'black')+
  facet_wrap(~src, nrow = 1)+
  scale_x_continuous(limits = c(0,1.01), breaks = c(0, 1)) +
  scale_y_continuous(limits = c(0,1.01), breaks = c(0, 1)) +
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Implied prevalence")+
  theme(legend.position = 'right')

cowplot::plot_grid( 
  fig.cimpian.interpretation.95ci + theme(
    plot.margin = unit(c(30, 6, 0, 6), "pt"),
                         legend.position = 'none'),
  cowplot::plot_grid(
    fig.cimpian.interpretation.stimtype + theme(plot.margin = unit(c(20, 6, 12, 6), "pt"),
                           legend.position = 'none'),
    fig.cimpian.interpretation.models + theme(plot.margin = unit(c(0, 6, 0, 6), "pt"),
                           legend.position = 'none'), 
    nrow = 1,
    labels = c("B", "C"),
    rel_widths = c(1, 2.5)
  ), 
  cowplot::get_legend(
    fig.cimpian.interpretation.stimtype + theme(
      legend.position="bottom"
      #legend.text = element_text(size = 16),
      #legend.title = element_text(size = 16)
                           )
    ), ncol = 1, rel_heights = c(1.2, 1.2, 0.2),
  labels = c("A", "", "")
  )

```


## Discussion

We replicated @Cimpian2010's finding of a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
We extended these findings with a broader stimulus set, discovered even more gradability in interpretations of generic statements. 
We empirically measured the prevalence priors and used our generic interpretation model to predict prevalence implied by a generic statement.

The items that receive the lowest implied prevalence are those of accidental or diseased states (e.g., *fungus-covered claws*, *broken legs*). 
As @Cimpian2010 noted, "properties of this type do not lend themselves very well to generic predication (Cimpian & Markman, 2008; Gelman, 1988), so generics about broken legs, itchy skin, etc. are infrequent outside the laboratory." (p.1472)
It, thus, remains a possibility that participants treat statements such as "Lorches have broken legs" as an existential claim about the here-and-now, analagous to how "Dogs are on my front lawn" describes a particular state of affairs as opposed to something generalizable about dogs. 
We thus aim to replicate these findings of low prevalence interpretations using properties that more naturally lend themselves to generic interpretation. 

# Experiment 2

This experiment is designed to measure the prevalence implied by a generic about a diverse set of properties (Expt. 2b). 
In addition, we elicit participants' beliefs about the likely prevalence levels expected for these different properties (Expt. 2a). 
We use our computational model and alternative models together with the elicited prevalence priors to make predictions about the prevalence implied by a generic.

## Experiment 2a: Prevalence prior elicitation


```{r load expt 2a meta data}
load(file = "cached_results/genint_priors_metaData.RData")
# expt2.prior.n,
#      expt2.prior.nonEnglish,
#      expt2.prior.catchFail,
#      expt2.prior.n.passCatch.English,
#      df.prior.3.filtered.meanTime, 
#     df.prior.3.filtered.n_subj_per_item



```


### Methods

#### Participants

We recruited `r expt2.prior.n` participants from MTurk.
This number was arrived at with the intention of getting approximately 23 independent sets of ratings for each unique item in the experiment.
Participants were restricted to those with U.S. IP addresses and with at least a 95\% MTurk work approval rating (these same criteria apply to all experiments reported).
The experiment took on average `r round(df.prior.3.filtered.meanTime$mean_time, 1)` minutes and participants were compensated \$0.80.

#### Materials

We created a stimulus set composed of seventy-five properties.
Items were generated by the first author by considering six different classes of properties: physical characteristics (e.g., *have brown spots*, *have four legs*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find the more obscure properties. 

#### Procedure and materials

On the first trial, participants were asked to list three kinds of animals for each of five different classes of animals: mammals, fish, birds, insects/bugs, amphibeans/reptiles. 
The five classes of animals were presented in a randomized order on the screen and there were three text boxes for each in which participants could type an animal kind.
On subsequent trials, participants were shown a random subset of five animal kinds and asked how what percentage each of the categories they believed had a property (e.g., "Out of all of the cheetas in the world, what percentage do you think attack hikers?").
Pilot results indicated similar responses were generated by a question about frequency (e.g., "Out of 100 cheetahs, how many do you think attack hikers?").
Participants responded using a slider bar with endpoints labeled 0% and 100%, and the exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant saw a random selection of twelve properties.

As an attention check, at the end of the prior elicitation trials, participants were asked to select, from a list of ten, all of the properties they could remember being tested on. 
The list included five properties that they had been tested on and five distractors. 
The experimental paradigm can be seen here: http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/prior-3.html.

### Results

We used the same exclusion criteria that were preregistered for the subsequent generic interpretation study (Expt. 2b): https://osf.io/bwn4t/register/5771ca429ad5a1020de2872e. 
Participants who did not have at least 4 out of 5 hits and at least 4 out of 5 correct rejections during the memory trial were excluded ($n = `r expt2.prior.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt2.prior.nonEnglish`$). 
This left a total of $n = `r expt2.prior.n.passCatch.English`$ participants, with items receiving on average `r round(df.prior.3.filtered.n_subj_per_item$n_subj_per_item)` (range = [`r round(df.prior.3.filtered.n_subj_per_item$lb)`, `r round(df.prior.3.filtered.n_subj_per_item$ub)`]) unique participants responses (each participant provides five ratings).

A response in this task can be thought of as a sample from a property's prevalence prior distribution.
Thus, the distribution of responses for an item are an estimate of the prevalence prior distribution.
Figure\ \@ref(fig:genInt-prevPrior)A shows eight example items' distributions of responses.
The property *have four legs* is most likely either completely present (prevalence = 100%) or completely absent (prevalence = 0%) from categories.
*Eat insects* looks similar, though there is considerable probability mass spread among the non-binary alternatives ($0\% <$ prevalence $< 100\%$). 
*Get in fights with other animals* has substantial less probability mass at 100% prevalence: It is unlikely that this property is widespread in a category. 
*Live in urban areas* shows a monotonically decreasing probability function; the higher the prevalence, the less likely it is. 
*Live in zoos* is expected to be even less prevalent, and the property *has seizures* is expected to not be widespread at all. 
The property *get erections* is only expected to be present in 50% of the population (presumably, the males) when it is present at all.

Most of the elicited prevalence prior distributions are at least bi-modal. 
To visualize all properties simultaneously, we represent each distribution by its relative probability mass at 0%---$P(\text{feature is present})$---and the expected value (mean) of the distribution conditional on the prevalence being greater than 0\%---$\mathbb{E}[P(p \mid p>0)]$, which we refer to as *prevalence when present* (Figure\ \@ref(fig:genInt-prevPrior)B).
Our stimulus set covers a wide range of possible values of both of these parameters; the set of parameters that summarize these priors is also largely non-overlapping with those of Expt. 1 (compare with Figure\ \@ref(fig:cimpian-prevPrior)A).
This suggests we have sampled items with priors that exhibit a lot of quantitative variability.
Given these priors, our model makes quantitative predictions about the prevalence implied by a novel generic sentence (e.g., "Lorches live in zoos").

```{r genInt-prevPrior, fig.width = 11.5, fig.asp = 0.36, fig.cap="Prevalence priors for a broad set of animal properties.  A: Ten example prevalence priors elicited in Expt. 2a. Different prevalence priors give rise to different model predicted implied prevalence. B: Prevalence priors summarized by their relative probability mass at zero-prevalence P(feature is present), and their expected value among non-zero prevalence levels: Prevalence when present. Error bars denote bootstrapped 95% confidence intervals. "}
#load(file = "cached_results/genInt_priors.RData")
# df.prior.prevalence.2.isZero.conditionalEv,
#      df.prior.prevalence.2,

load(file = "cached_results/genInt_priors3.RData")

# df.prior.prevalence.3.isZero.conditionalEv,
#      df.prior.prevalence.3,
#      df.prior.3.filtered.bs,

fig.prevParams.genInt <- ggplot(df.prior.prevalence.3.isZero.conditionalEv, 
       aes( x = 1-isZero, xmin = 1-zero_low, xmax = 1-zero_high,
            y = pwp_mean, ymin = pwp_low, ymax = pwp_high))+
  geom_errorbar(alpha = 0.3) + geom_errorbarh(alpha = 0.3) + 
  geom_point(shape = 21, size = 2.6, color = 'black', fill = 'black')+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  # ggrepel::geom_label_repel(data = df.prior.prevalence.3.isZero.conditionalEv %>% 
  #                             filter(property %in% c("have four legs", 
  #                      "eat insects", 
  #                      #"feed on the carcasses of dead animals",
  #                      "get in fights with other animals",
  #                      "know when earthquakes are about to happen",
  #                      "live in zoos",
  #                      #"drink alcohol left behind by tourists",
  #                      #"chase their tails",
  #                      "have seizures")), 
  #                           aes(label = property))+
  ylab("Prevalence when present")+
  xlab("P(feature is present)")+
  coord_fixed()

target.properties <- c("have four legs", 
                       "eat insects", 
                       "eat garbage",
                       #"feed on the carcasses of dead animals",
                       "are afraid of loud noises",
                       "get in fights with other animals",
                       "live in urban areas",
                       "know when earthquakes are about to happen",
                       "get erections",
                       "live in zoos",
                       #"drink alcohol left behind by tourists",
                       #"chase their tails",
                       "have seizures")

var_width <- 22

fig.prevPriors.genInt <- df.prior.3.filtered.bs %>% ungroup() %>%
  filter(property %in% target.properties) %>%
  mutate(property = factor(property, levels = target.properties,
                           labels = stringr::str_wrap(target.properties, 
                                                      width = var_width))) %>%
  mutate(state = as.numeric(state)) %>%
  group_by(property, state) %>%
  summarize( lower = quantile(prop, 0.025),
             mean = mean(prop),
             upper = quantile(prop, 0.975)) %>%
  ggplot(., aes( x = state, y = mean, ymin = lower, ymax = upper))+
  geom_col(position= position_dodge(), color = 'black', fill= 'white')+
  geom_errorbar(position = position_dodge(), width= 0.036, alpha = 0.4 )+
  facet_wrap(~property, nrow = 2) +
  scale_x_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(-0, 0.8), breaks = c(0, 0.25, 0.5, 0.75))+
  ylab("Proportion of responses")+
  xlab("Prevalence")

cowplot::plot_grid(
  fig.prevPriors.genInt + theme(plot.margin = unit(c(6, 6, 6, 0), "pt"),
                         legend.position = 'none'), 
  fig.prevParams.genInt + theme(plot.margin = unit(c(18, 6, 6, 6), "pt"),
                         legend.position = 'none'),

  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(2.5, 1)
)

```



## Experiment 2b: Generic interpretation


```{r load expt2b meta data}
load(file = "cached_results/genint_int_metaData.RData")

# expt2.int.n,
#      expt2.int.nonEnglish,
#      expt2.int.catchFail,
#      expt2.int.n.passCatch.English,
#      df.int.6.meanTime, 
#      df.int.6.filtered.n_subj_per_item,
#      df.int.56.filtered.means.r, df.int.56.filtered.means.n,
#   df.int.56.filtered.means.spearman,

```

### Methods

#### Participants

We recruited `r expt2.int.n` participants from MTurk. 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average `r round(df.int.6.meanTime$mean_time,1)` minutes and participants were compensated \$0.80.

#### Procedure and materials

The materials were the same as in Expt. 2a.
Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would be told facts about the new animals and be asked to translate it into the percentage of that animal it applies to.
On each trial, participants read "You are told: *generic sentence*", where the generic sentence was a bare plural statement about a familiar property $F$ applying to a novel animal category $K$ (e.g., "Javs attack hikers").
They were then asked "Out of all of the *K*s on the planet, what percentage do you think *F*?" (e.g., "Out of all of the javs on the planet, what percentage do you think attack hikers?").
Novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Participants responded using a slider bar with endpoints labeled 0% and 100$, and the exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant completed thirty-five trials, corresponding to a random subset of the full stimulus set.

After the generic interpretation trials, participants completed the same memory check trial as was done in the prior elicitation. 
They were shown ten properties and asked to click on those they had seen in the experiment.
Following the memory check trials and depending on their ratings in the task, participants completed up to five explanation trials. 
On an explanation trial, participants saw a rating they had given for a property they had rated as applying to less than 50% and asked if they could explain why they gave the response that they gave. 
Participants also had the option of changing their response after providing an explanation. 
(These data were used in an exploratory analysis.)
If participants gave no ratings less than 50%, they did not complete any explanation trials. 

The experiment paradigm can be viewed at http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/interpretations-6.html

### Results

We used preregistered exclusion criteria, which were also used for Expt. 2a. 
Participants who did not have at least 4 out of 5 hits and at least 4 out of 5 correct rejections during the memory trial were excluded ($n = `r expt2.int.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt2.int.nonEnglish`$). 
This left a total of $n = `r expt2.int.n.passCatch.English`$ participants, with items receiving on average `r round(df.int.6.filtered.n_subj_per_item$n_subj_per_item)` responses (range = [`r round(df.int.6.filtered.n_subj_per_item$lb)`, `r round(df.int.6.filtered.n_subj_per_item$ub)`]).

To assess the reliability of these data, we ran a replication ($n=140$) using a slightly different dependent measure.^[
  By experimenter error, only seventy-four of the seventy-five items were collected in the replication data set.
] Instead of being asked a question about percentages (e.g., "Out of all of the Ks in the world, what percentage F?"), participants were asked a question in terms of frequency: "Out of 100 Ks, how many do you think F?".
The empirical by-item means between these data and the original data are highly correlated ($r(`r df.int.56.filtered.means.n`) = `r round(df.int.56.filtered.means.r, 2)`$, $r_{spearman}(`r df.int.56.filtered.means.n`)= `r round(df.int.56.filtered.means.spearman, 2)`$), indicating very high data reliability.

```{r load interpretation results, cache=F}
load(file = "cached_results/genInt_interpretation6_prior3_modelData.RData")
#df.int.filtered, df.int.filtered.bs, md.impprev
load(file = "cached_results/genInt_interpretation6_prior3_mostNoise.RData")
format_empirical_ci <- function(
  df, estimate = "mean", low = "ci_lower",
  high = "ci_upper", sigfigs = 2){
  return(paste(
    round(df[[1,estimate]], sigfigs),
    " [",
    round(df[[1,low]], sigfigs),
    ", ",
    round(df[[1,high]], sigfigs),
    "]", sep = ""))
}

md.impprev.stats <- md.impprev %>%
  group_by(parameter, semantics) %>%
  summarize( mse  = mean((model_MAP-mean)^2),
             r2 = cor(model_MAP, mean)^2,
             n = n())
```

#### Descriptive results

Interpreting a novel generic sentence "Ks have F" often has the possibility of being understood as a universal claim (*all or almost all Ks Fs)*. 
Across our seventy-five items, however, we observe a clear gradient in the implied prevalence ratings (Figure\ \@ref(fig:genint-modelingResults)A).
On one end of the continuum, we have the generic "Wugs have four legs", which is interpreted as a universal---applying to exactly 100% of wugs---by `r sum(filter(df.int.filtered, property == "have four legs")$response == 1)` out of `r length(filter(df.int.filtered, property == "have four legs")$response)` participants and which received a mean implied prevalence rating of $`r format_empirical_ci(filter(df.int.filtered.bs, property == "have four legs"))`$. 
^[
  Note the novel category term is randomized for each participant and property. 
  We use particular novel category terms in the text for ease of exposition.
]
On the other end is "Glippets perform in the circus", which is interpreted as applying to less than 25% of glippets by `r sum(filter(df.int.filtered, property == "perform in the circus")$response < 0.25)` out of `r length(filter(df.int.filtered, property == "perform in the circus")$response)` participants and which receives a mean rating below 50% ($`r format_empirical_ci(filter(df.int.filtered.bs, property == "perform in the circus"))`$).
<!-- Several items have mean ratings below 50%.  -->
Additionally remarkable is the distribution of responses for individual items: Though "Feps live in zoos" probably means around 25% of feps live in zoos, it is still quite possible that almost 100% live in zoos (e.g., in the real world, 100% of Micronesian Kingfishers live in zoos).

Several properties about the reproductive behavior of animals have interpretation distributions that are bimodal. 
For example, "Morseths have a menstrual cycle" is interpreted primarily as applying to 50% of morseths (`r sum(with(filter(df.int.filtered, property == "have a menstrual cycle"), ((response < 0.55) & (response > 0.45))))` out of `r length(with(filter(df.int.filtered, property == "have a menstrual cycle"), response))` give a response between 45% and 55%); however, some participants (`r sum(with(filter(df.int.filtered, property == "have a menstrual cycle"), response ==1 ))` of `r length(with(filter(df.int.filtered, property == "have a menstrual cycle"), response))`) rate this as applying to 100% of morseths.^[
  The scientifically correct term for this is actually an "estrous cycle". We are grateful to one of our MTurk participants for pointing this out to us. 
]
This behavior could result from a participant interpreting the question as pragmatically restricting the domain to only the members of that category that *could* have the property (e.g., 100% of female morseths have a menstrual cycle).
We notice this bimodal interpretation in our prevalence elicitation task (Expt. 2a) as well: The distribution over several of the reproductive properties is tri-modal, with peaks at 0% (the animals that don't have a menstrual cycle), 50% ("only females") and 100% (perhaps, "all females").
The fact that this multi-modality appears in the prior elicitation task as well means that the generic interpretation model will be well-positioned to predict a bi-modal distribution for the implied prevalence data.

#### Model-based analysis and results

Our primary analysis concerns how well the uncertain threshold generic interpretation model can accomodate the implied prevalence data in comparison to alternative models. 
To fit these models, we construct a Bayesian data analysis (BDA) model to jointly predict the prevalence prior data (Expt. 2a) and the generic interpretation data (Expt. 2b). 
Such a joint-inference mode allows us to retain the uncertainty in the measurement of the prevalence prior data when using it in our model to predict the implied prevalence results. 

The joint cognitive-statistical model attempts to predict both data sources (prevalence prior and generic interpretation) simultaneously; the former is modeled as a mixture of three Beta distributions and the latter is modeled by our generic interpretation model (Eq. \ref{eq:L0}).
Beta is an appropriate choice for the prevalence prior data because it is a distribution over numbers on the unit interval (the same as our slider ratings), and we allow each individual property's prevalence prior to be composed of up to three constituent Betas to allow the statistical model the flexibility to accomodate the complex prevalence prior distributions.
Each prevalence prior, thus, has eight parameters governing its shape: a mean $\gamma$ and variance parameter $\xi$ for each of the three Beta components and two parameters $\phi_1$, $\phi_2$ that describe the relative weighting among the three components.
We put uninformative priors over these parameters $\gamma \sim \text{Uniform}(0, 1)$, $\xi \sim \text{Uniform}(0, 100)$, $\phi \sim \text{Dirichlet}(1,1,1)$.
The inferred prevalence prior distributions are then used as $P(p)$ in Eq. \ref{eq:L0} in order to generate predictions for the generic interpretation data.^[
  In reality, this procedure is done simultaneously, not sequentially.
]
The generic interpretation model is fully specified by Eq. \ref{eq:L0} and has no free parameters.
The model is implememented in the probabilistic programming language WebPPL [@dippl].

To learn about the credible values of the parameters governing the prevalence priors as well as generate model predictions for the generic interpretation data, we performed Bayesian inference on the model by running three chains of an incrementalized version of MCMC [@Ritchie2016] for 750,000 iterations, removing the first 250,000 iterations for burn-in.
A qualitative examination showed that the prevalence prior distributions were well-modeled as a mixture of three Beta distributions. 
Often one Beta distribution is devoted to accounting for the very small numbers (0% or near 0% prevalence ratings), while the two others attempt to fit the other parts of the distribution, the details of which depend upon the property.

As a first attempt to understand the implied prevalence data in a quantitative way, we compare the means of the inferred prevalence priors to the empirical means of the implied prevalence data (Figure\ \@ref(fig:genint-modelingResults)B top facet).
The means of the prevalence prior distributions do a rather poor job at predicting the implied prevalence ratings, consistently underpredicting the ratings ($r^2(`r filter(md.impprev.stats, semantics == "uncertain", parameter == "prior")[["n"]]`) = `r filter(md.impprev.stats, semantics == "uncertain", parameter == "prior")[["r2"]]`$, $MSE = `r format(filter(md.impprev.stats, semantics == "uncertain", parameter == "prior")[["mse"]], digits = 2)`$).
The fact that the prevalence prior data does not fit the generic interpretation data is good evidence that participants are actually *updating* their beliefs about the property in the generic interpretation task, as opposed to simply sampling from their prior.

We ran alternative interpretation models using the same data analytic approach (i.e., joint cognitive-statistical models) but which had access to only a single fixed-threshold rather the uncertain threshold of the generic interpretation model. 
These alternative models correspond to models of the quantifier "some" and "most" and provide strict tests of our generic interpretation model because they too have access to the full prior distribution over prevalence as well as the flexibility endowed by the Bayesian data analysis method.
Figure\ \@ref(fig:genint-modelingResults)B shows each of the alternative model's predictions (technically, the BDA models' posterior predictive distributions) for the mean implied prevalence of each of the seventy-novel generic sentences, in comparison to the empirical means. 
We see that the model based on quantifier "some" (i.e., a fixed threshold at the lowest possible value) consistently *underpredicts* the implied prevalence, consistent with the intuition that assigning the semantics of "some" for generics is too weak.
Though the model explains a lot of the variance $r^2(`r filter(md.impprev.stats, semantics == "some", parameter == "posterior")[["n"]]`) = `r filter(md.impprev.stats, semantics == "some", parameter == "posterior")[["r2"]]`$, it does so with a relatively high mean squared error $MSE = `r format(filter(md.impprev.stats, semantics == "some", parameter == "posterior")[["mse"]], digits = 2)`$.

It is impossible for the model based on the quantifier "most" to accomodate these data: *Most* has a strict threshold at 0.5 and thus assigns probability 0 to all responses less than 0.5.
In order to alleviate this issue, we endow the BDA model for "most" with a external noise parameter, which allows the model to randomly guess on some proportion of trial (the exact proportion is a parameter to be inferred from the data).
The "most" model assigns `r format_empirical_ci(m.samp.most.noise, "MAP", "cred_lower", "cred_upper")` proportion of the data to be noise, because of the high prevalence of minority (i.e., less than 50%) interpretations. 
A model based on the semantics for "most" is not well-suited for these data $r^2(`r filter(md.impprev.stats, semantics == "most", parameter == "posterior")[["n"]]`) = `r filter(md.impprev.stats, semantics == "most", parameter == "posterior")[["r2"]]`$,  $MSE = `r format(filter(md.impprev.stats, semantics == "most", parameter == "posterior")[["mse"]], digits = 2)`$.

Finally, we examine the uncertain threshold model's predictions for the mean implied prevalence ratings. 
Unlike each of the alternative models, the uncertain threshold is able to perfectly accomodate the highly variable generic interpretation data $r^2(`r filter(md.impprev.stats, semantics == "uncertain", parameter == "posterior")[["n"]]`) = `r filter(md.impprev.stats, semantics == "uncertain", parameter == "posterior")[["r2"]]`$, $MSE = `r format(filter(md.impprev.stats, semantics == "uncertain", parameter == "posterior")[["mse"]], digits = 2)`$.
The generic interpretation model is able to flexibly adjust the threshold depending on the prevalence prior, resulting in context-sensitive generic interpretation that finely tracks human judgments.


```{r genint-modelingResults, fig.width = 11, fig.asp = 0.95, fig.cap="Implied prevalence data and model predictions for a broad set of seventy-four items. A: Mean implied prevalence ratings (points are individual empirical judgments). B: Posterior predictive model fits for quantitative models based on (i) the mean of the prevalence prior, (ii) a threshold semantics fixed at 0.01 (\"some\"), (iii) a threshold semantics fixed at 0.5 (\"most\"), and (iv) an uncertain threshold semantics (generic). The model for \"most\" is outfitted with a noise parameter to accomodate data points that are logically impossible given the fixed semantics. Error bars denote bootstrapped 95% confidence intervals for the behavioral data and 95% highest posterior density intervals for the model predictions.", cache = F}
fig.genint.interpretation.ci <- df.int.filtered %>%
  mutate(property = factor(property, 
                           levels =  with(df.int.filtered.bs, property[order(mean)]))) %>%
  ggplot(., aes( x = property, y = response))+
  geom_pirate(bars = F, violins = F, width_points = 0.2, alpha_points = 0.35)+
  coord_flip()+
  #scale_color_viridis(discrete = T)+
  scale_y_continuous(limits= c(0, 1), breaks = c(0, 0.5, 1))+
  theme(axis.title.y = element_blank())+
  ylab("Implied prevalence rating")


fig.genint.interpretation.modelss <- md.impprev %>% 
  filter((parameter == "posterior") | (semantics == "uncertain")) %>%
  mutate(src = paste(parameter, "_", semantics, sep =""),
         src = factor(src, levels = c("prior_uncertain", "posterior_some",
                                      "posterior_most", "posterior_uncertain"),
                      labels = c("Prevalence prior mean",
                                 '"some"\n(threshold = 0.01)',
                                 '"most" + noise\n(threshold = 0.5)',
                                 'generic\n(uncertain threshold)'))) %>%
  ggplot(., aes( x = model_MAP, xmin = model_lower, xmax = model_upper, 
                        y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
  geom_point(alpha = 1)+
  facet_wrap(~src, ncol = 1)+
  scale_x_continuous(limits = c(-0.01,1.01), breaks = c(0, 0.5, 1)) +
  scale_y_continuous(limits = c(-0.01,1.01), breaks = c(0, 0.5, 1)) +
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human implied prevalence")


cowplot::plot_grid(
  fig.genint.interpretation.ci + theme(plot.margin = unit(c(12, 6, 6, 6), "pt"),
                         legend.position = 'none'),
  fig.genint.interpretation.modelss + theme(plot.margin = unit(c(0, 6, 6, 6), "pt"),
                         legend.position = 'none'), 
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(3.5, 1)
)
```


# Experiment 3: Prior Manipulation

In the previous experiments, we see how participants' prior beliefs about the prevalence of the property, either measured by asking about alternative categories (Expt. 2) or abstract questions about the distribution of the property (Expt. 1), related to the prevalence implied by a novel generic via a model that reasons about an uncertain threshold on prevalence (Eq. \ref{eq:L0}).
In both of the previous experiments, we measured both generic interpretation and the prevalence prior and related the two via our model. 
Though our model is able to finely track interpretations in a way that alternative models cannot, the evidence so far for the influence of the prevalence prior on interpretation is merely correlational. 
We have shown that the two are related but not that intervening on the prevalence prior can change interpretations.

In this final experiment, we test the causal role of the prevalence prior in generic interpretation. 
We do this by manipulating the prevalence prior and measuring the resulting influence on interpretation (*generic interpretation* condition).
To serve as a manipulation check, we first elicit predictions about prevalences following our prior manipulation procedure; these predictions also serve as the measurements of the (manipulated) prevalence priors used in our model.

## Experiment 3a: Prior elicitation (manipulation check)

### Methods

#### Participants

We recruited 200 participants from MTurk. 
This number was arrived at with the intention of getting approximately 30 ratings for each unique item in the experiment.
The experiment took on average X minutes and participants were compensated \$K.

#### Materials 

The goal of this experiment was to manipulate the prevalence prior. 
To focus on the influence of our prior manipulation cover story (described below), we chose to use a single property (*know when earthquakes are about to happen*) whose prevalence prior elicited in Expt. 2a suggested participants had a lot of uncertainty as to what prevalence levels to expect for this property, and thus could be particularly amenable to manipulation.^[
  Pilot testing using a completely novel property (e.g., *daxing*) appeared to make the task too artificial and participants treated the task as a number game. 
]

Participants were familiarized with one of ten prevalence prior distributions, shown in Figure\ \@ref(fig:priorManipulationExpt)E). 
Nine of the ten distributions are bimodal with modes either at 0%, 25%, 50%, 75%, or 100%; the tenth distribution is uniform over all prevalence levels between 0% and 100%.
The mixture component between the two modes was always 50% (half of samples were from one mode and half from the other, as shown in Figure\ \@ref(fig:priorManipulationExpt)B).

#### Procedure

```{r priorManipulationExpt, fig.cap="Overview of Experiment 3. A: Prevalences of feature in animals are shown one at a time, described in text and displayed in a table. B: Participants are asked to review previous results once all displayed. C: Prior elicitation task: Participants predict the results of the next five animals. D: Generic interpretation task: Participants rate prevalence after reading generic sentence. E: Experimentally manipulated prevalence prior distributions. The distribution shown in B is the \\emph{weak or deterministic} distribution.", fig.width = 11,fig.asp = 0.6}
zero.samples <- rbeta(n = 5000, shape1 = 1, shape2 = 100)
weak.samples <- rnorm(n = 5000, mean = 0.25, sd = 0.06)
half.samples <- rnorm(n = 5000, mean = 0.50, sd = 0.06)
strong.samples <- rnorm(n = 5000, mean = 0.75, sd = 0.06)
deterministic.samples <-rbeta(n = 5000, shape1 = 50, shape2 = 1)

sample.dists <- bind_rows(
  data.frame( dist = "0/25", x = c(zero.samples, weak.samples) ),
  data.frame( dist = "0/50", x = c(zero.samples, half.samples) ),
  data.frame( dist = "0/75", x = c(zero.samples, strong.samples) ),
  data.frame( dist = "0/100", x = c(zero.samples, deterministic.samples) ),
  
  data.frame( dist = "25/50", x = c(weak.samples, half.samples) ),
  data.frame( dist = "25/75", x = c(weak.samples, strong.samples) ),
  data.frame( dist = "25/100", x = c(weak.samples, deterministic.samples) ),

  data.frame( dist = "50/100", x = c(half.samples, deterministic.samples) ),
  data.frame( dist = "75/100", x = c(strong.samples, deterministic.samples) ),
  
  data.frame( dist = "uniform", x = rbeta(n = 50000, shape1 = 1, shape2 = 1))
)



distribution.order.levels <- c("rare_weak",
                                        "rare_half",
                                        "weak_or_half",
                                        "rare_strong",
                                        "weak_or_strong",
                                        "rare_deterministic", 
                                        "weak_or_deterministic",
                                        "half_or_deterministic",
                                        "strong_or_deterministic",
                                        "uniform")

# distribution.order.labels <- c("rare weak",
#                                         "rare half",
#                                         "weak or half",
#                                         "rare strong",
#                                         "weak or strong",
#                                         "rare deterministic", 
#                                         "weak or deterministic",
#                                         "half or deterministic",
#                                         "strong or deterministic",
#                                         "uniform")

distribution.order.labels <- c("0/25","0/50","25/50","0/75","25/75","0/100", "25/100",
                                        "50/100",
                                        "75/100",
                                        "uniform")
                                        
fig.manipulatedPriors <- sample.dists %>%
  mutate(dist = factor(dist, levels = distribution.order.labels)) %>%
  ggplot(., aes(x = x))+ 
  geom_density(aes( y = ..scaled..))+ #
  # geom_histogram(bins = 20,
  #   aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
  #   color = 'black', fill= 'grey')+
  scale_x_continuous(limits = c(-0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  #scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  #scale_fill_solarized()+scale_color_solarized()+
  #scale_color_manual(values = c("#636363", "#d7191c"))+
  ylab("Scaled Prior Probability")+
  xlab("Prevalence")+
  facet_wrap(~dist, scales = 'free', nrow = 2)+
  guides(fill = F, color = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal')


prior.manip.exp.1 <- ggdraw() + draw_image("figs/prior-manip-paradigm-1.jpeg", scale = 0.9) +
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1)) 
prior.manip.exp.2 <- ggdraw() + draw_image("figs/prior-manip-paradigm-2.jpeg", scale = 0.9)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1)) 
prior.manip.exp.3 <- ggdraw() + draw_image("figs/prior-manip-paradigm-priorMeasure.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))
prior.manip.exp.4 <- ggdraw() + draw_image("figs/prior-manip-paradigm-intMeasure.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))




prior.manip.toprow <- plot_grid(prior.manip.exp.1, 
                             prior.manip.exp.2, labels = c("A", "B"), nrow = 1, align = 'vh')

title <- ggdraw() + draw_label("Prevalence prior manipulation")
plot_grid(
  plot_grid(title, prior.manip.toprow, ncol=1, rel_heights=c(0.15, 1)),
  plot_grid(
    plot_grid(ggdraw() + draw_label("Manipulation check"), 
              prior.manip.exp.3, ncol=1, rel_heights=c(0.15, 1) ), 
    plot_grid(ggdraw() + draw_label("Generic interpretation"), 
              prior.manip.exp.4, ncol=1, rel_heights=c(0.15, 1) ),
              labels = c("C", "D"), nrow = 1),
  # plot_grid(add_sub(prior.manip.exp.3, "Manipulation check"), 
  #           add_sub(prior.manip.exp.4,"Generic interpretation"), labels = c("C", "D"), nrow = 1),
  fig.manipulatedPriors, labels = c("","", "E"),
  rel_heights = c(1,1,1.5),
  nrow = 3)
```

In order to avoid participants learning the structure of the task, participants completed only a single trial: Each participant saw only one prevalence distribution.
Participants were given a cover story in which they were asked to imagine they were an astronaut-scientist exploring a distant planet with lots of new animals on it.
They were studying these animals with another scientist to understand the animals' ability to "know when earthquakes are about to happen".
Participants were then shown data that they and their fellow scientist had collected about the prevalence of the property among different novel animals.
In order to minimize distraction, novel animals were simply labeled by a letter (e.g., "Animal C" or just "Cs").
Information about the animal appeared in an *evidence statement* (e.g., "After studying the animal, you and your partner determine that 98\% of Cs *know when earthquakes are about to happen*.") and were displayed in a table showing the corresponding numerical data (Figure\ \@ref(fig:priorManipulationExpt)A). 
Participants proceed in a self-paced way by clicking a button to reveal information about the next animals.
After participants viewed the data for ten categories, they are told to review the information about the animals before continuing (Figure\ \@ref(fig:priorManipulationExpt)B).

Then, the data table is removed and participants are provided the following prompt:

> Tomorrow, you and your partner will study 5 other new species of animals. What percentage of each do you think will know when earthquakes are about to happen?

Participants were given five slider bars ranging from 0% - 100%, and asked to predict the prevalence for the next five categories (Animals K - P; Figure\ \@ref(fig:priorManipulationExpt)C).
After rating the slider bars, participants were asked if they could explain why they gave the responses that they did. 
Then, they completed an attention check survey where they were asked what property was being investigated (choosing a response from a list of 10 options) and to input one of the prevalence levels they saw on the familiarization screen.

### Results

Participants who failed to either select the correct property or list a correct number from the familiarization period were excluded.

Similar to the data from Expt. 2a, a response in this task can be thought of as a sample from a prevalence prior distribution and the distribution of responses as estimates for the whole distribution.
We visualize the distributions by discretizing the responses so that each response goes into one of twenty equally spaced bins (effectively turning the 101-pt scale into a 20-pt scale).
Figure\ \@ref(fig:priorManipulationResults)A (top row) shows the empirically elicited predicted prevalence  distributions.
These distributions clearly reflect the familiarization distributions supplied to participants in the experiment, with some idiosyncractic features.
The distribution that is most different from the familiarization distribution is the uniform distribution; the empirical predicted distribution is more unimodal with a peak at 50%. 
Many distributions exhibit a regression to the mean in some participants' predictions: In the *rare or deterministic* distribution (which featured prevalence levels either around 0% or 100%), some participants guess that the next animal will have 50% prevalence; a similar phenomenon can be observed in the *weak or strong* distribution (25% or 75%) and to a lesser extent in the other bimodal distributions. 

Because our generic interpretation model has no free parameters, we use these empirically measured prevalence istributions to generate *a priori* model predictions using the generic interpretation data.
To account for our uncertainty in the measurement of the prevalence priors, we bootstrap the priors by resampling subjects (with replacement), calculating the empirical prevalence distributions (by binning, as above), and generating model predictions. 
We repeated this procedure 10,000 times to generate distributions of predictions.
The boostrapped mean and 95% quantiles for each prevalence bin in each distribution are shown in Figure\ \@ref(fig:priorManipulationResults)A (middle row).
We see that context-sensitive interpretations are predicted, with idiosyncracies that reflect empirically measured distributions.
We then tested these predictions against interpretations in the same empirical paradigm.


```{r priorManipulationResults, fig.cap="Experiment 3 results. A: Empirical distribution of responses for the prior elicitation task (Expt. 3a), generics model predictions based on those empirical distributions, and empirical distributions of responses for the generic interpretation task (Expt. 3b). B: Model predicted means for several alternative models and empirical means. C: Same as B shown in scatterplot.", fig.width = 13.5,fig.asp = 0.6}
load(file = "cached_results/gentInt_manipPriors_dists_expvals.RData")
# md.pm.int.3.filtered.expvals.bs.summary, md.pm.int.3.filtered.dists.bs.summary,
#      md.pm.int.3.filtered.dists.bs.summary.wide, md.pm.int.3.filtered.expvals.bs.summary.wide

distribution.order.levels <- with(md.pm.int.3.filtered.expvals.bs.summary %>%
                         filter(model == "uncertain"), distribution[order(mean)])
distribution.order.labels <- gsub("_", " ", distribution.order.levels)

distribution.order.labels <- gsub(" ", "/", gsub(" or", "", gsub("deterministic", "100", gsub("strong", "75", gsub("half", "50", gsub("weak", "25", gsub("rare","0", distribution.order.labels)))))))



fig.int.dists.bs <- md.pm.int.3.filtered.dists.bs.summary %>%
  filter(model %in% c("prior", "uncertain", "data")) %>%
  mutate(distribution = factor(distribution,  levels = distribution.order.levels, labels = distribution.order.labels),
         model = factor(model, levels = c("prior", "uncertain", "data"),
                        labels = c("prevalence prior", "generics model", "data"))) %>%
  ggplot(., aes( x = state, y = mean, ymin = lower, ymax = upper, fill = distribution))+
  geom_col(position= position_dodge(), alpha = 0.8, color = 'black')+
  geom_errorbar(position = position_dodge(),  alpha = 0.3)+
  geom_text(data = data.frame(model = c("prevalence prior", "generics model", "data"),
    x = c(0.5, 0.5, 0.5),
    y = c(0.4, 0.5, 0.6),
    distribution = c("0/100","0/100","0/100")), 
    aes( x= x ,y =y, label = model), inherit.aes = F)+
  facet_grid(model~distribution, scales = 'free')+
  scale_x_continuous(breaks = c(0, 1))+
  scale_fill_viridis(discrete = T)+
  ylab("predicted / empirical probability")+
  xlab("prevalence")+
  theme(strip.text.y = element_blank())+
  guides(fill = F)


fig.int.expval.bs <- md.pm.int.3.filtered.expvals.bs.summary %>%
  ungroup() %>%
  filter(!(model %in% c("fixed_0.625", "fixed_0.875"))) %>%
  mutate(distribution = factor(distribution,  levels = distribution.order.levels, labels = distribution.order.labels),
         model = factor(model,
                        levels = c("prior", "uncertain", "data",
                                   "fixed_0.0175", "fixed_0.125",
                                   "fixed_0.375"),
                        labels = c("prevalence prior", "generics model", "data",
                                   '"some" (threshold = 0.01)', '"not rare" (threshold = 0.10)', 
                                   '"more than third" (threshold= 0.33)'))) %>%
  ggplot(., aes( x = distribution, y = mean, ymin = ci_lower, ymax = ci_upper, fill = distribution))+
  geom_col(position= position_dodge(), color  = 'black', alpha = 0.8)+
  geom_errorbar(position = position_dodge())+
  scale_fill_viridis(discrete = T)+
  ylab("Mean implied prevalence")+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank())+
  facet_wrap(~model, nrow = 2)+ guides(fill = guide_legend(reverse=T))



fig.int.expval.bs.scatters <- md.pm.int.3.filtered.expvals.bs.summary.wide %>%
  filter(!(model %in% c("fixed_0.625", "fixed_0.875","fixed_0.375"))) %>%
  mutate(distribution = factor(distribution, levels = distribution.order.levels),
         model = factor(model, levels = c("prior", "fixed_0.0175", "fixed_0.125", "uncertain"),
                        labels = c("prevalence prior", '"some"\n(fixed threshold = 0.01)', '"not rare" (fixed threshold = 0.10)',
                                   "generics model \n (uncertain threshold)"))) %>%
  ggplot(., aes(x = model_prediction, xmin = model_lower, xmax = model_upper, ymin = ci_lower, y = mean, ymax = ci_upper, fill = distribution))+
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_errorbar(position = position_dodge(), alpha = 0.8)+
  geom_errorbarh(position = position_dodge(), alpha = 0.4)+
  geom_point(shape = 21, color = 'black')+
  scale_fill_viridis(discrete = T)+
  ylab("Empirical mean implied prevalence")+
  xlab("Model prediction")+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  coord_fixed()+
  facet_wrap(~model, nrow = 2)+
  guides(fill = F)




cowplot::plot_grid(
  fig.int.dists.bs + theme(plot.margin = unit(c(6, 6, 6, 0), "pt"),
                         legend.position = 'none'), 
  cowplot::plot_grid(
    fig.int.expval.bs + theme(plot.margin = unit(c(6, 6, 6, 6), "pt"),
                         legend.position = 'none'),
    fig.int.expval.bs.scatters + theme(plot.margin = unit(c(6, 0, 0, 0), "pt"),
                           legend.position = 'none'),
    get_legend(fig.int.expval.bs), rel_widths = c(1, 0.65,0.3),
    nrow = 1,  labels = c("B", "C", "")
  ),
  nrow = 2,
  labels = c("A", "B"),
  rel_widths = c(3.5, 1, 1)
)
```


## Experiment 3b: Generic interpretation

### Methods

```{r load prior manip meta data}
load(file = "cached_results/priorManip_int_metaData.RData")
# expt3.int.catchFail,
#      expt3.int.nonEnglish,
#      expt3.int.n,
#      expt3.int.n.passCatch.English,
#      expt3.int.meanTime,
#df.prior_manip.int.filtered.n_subj_per_item
```

The sample size, exclusion criteria, and planned statistical contrasts were all preregistered: https://osf.io/n342q/register/5771ca429ad5a1020de2872e. 

#### Participants

We recruited `r expt3.int.n` participants from MTurk.
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average `r round(expt3.int.meanTime$mean_time, 1)` minutes and participants were compensated \$0.30.

#### Materials and procedure

The materials and procedure were identical to those of Expt. 3a with the following exception. 
After the familiarization period, the data table is removed and participants are provided the following prompt:

> The next day, you are busy and your partner studies a new animal on their own: Animal K.
> Your partner tells you: "Ks know when earthquakes are about to happen."

In order to encourage participants to pay attention to the language, this text is on the screen for five seconds before participants are asked: "What percentage of Ks do you think know when earthquakes are about to happen?" and provided with a slider bar ranging from 0%-100% (Figure\ \@ref(fig:priorManipulationExpt)D). 
As in Expt. 3a, participants then completed an attention check survey where they were asked what property was being investigated and to input one of the prevalence levels they saw on the familiarization screen.
The experiment can be viewed at http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/prior-manipulation-2.html. 
<!-- Participants see the data from eleven different categories, though they are told the data for one kind were lost and a "?" was placed in the table (\red{Figure}).^[ -->
<!--   These lost results would be found in the *generic interpretation* task, Expt. 3b. -->
<!--   We chose to present a "lost data" category to imply that the sampling procedure for the referent-category was random. The alternative would be to present data from ten categories and have participants rate the eleventh, which could imply that the scientists are collecting data until they find one for which the property applied generically. \red{[clean up]} -->
<!-- ] -->


<!-- scientists had recently discovered lots of new animals that we did not know existed, and that scientists were trying to understand the properties of these new animals.  -->

<!-- From the stimulus set of 74 properties used in Expt. 2, we selected a subset of 24 items that received intermediate interpretation judgments (Expt. 2b: implied prevalence between: 40 - 65\%), which we believed would make them most susceptible to our prior manipulation. -->

### Results

```{r load prior manip regression}
load(file = "cached_results/gentInt_manipPriors_lm.RData")
# rs.lm.priorManip
# rs.lm.priorManip.contrast1,
# rs.lm.priorManip.contrast2,
# rs.lm.priorManip.contrast3,
# rs.lm.priorManip.contrast4,
# rs.lm.priorManip.contrast5,
# rs.lm.priorManip.contrast6,

rs.lm.priorManip.contrast1.summ <- summary(rs.lm.priorManip.contrast1)
rs.lm.priorManip.contrast2.summ <- summary(rs.lm.priorManip.contrast2)
rs.lm.priorManip.contrast3.summ <- summary(rs.lm.priorManip.contrast3)
rs.lm.priorManip.contrast4.summ <- summary(rs.lm.priorManip.contrast4)
rs.lm.priorManip.contrast5.summ <- summary(rs.lm.priorManip.contrast5)
rs.lm.priorManip.contrast6.summ <- summary(rs.lm.priorManip.contrast6)
rs.lm.priorManip.summ <- summary(rs.lm.priorManip)

```

We used preregistered exclusion criteria, which were also used for Expt. 3a.
Participants who failed to either select the correct property or list a correct number from the familiarization period were excluded ($n = `r expt3.int.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt3.int.nonEnglish`$). 
This left a total of $n = `r expt3.int.n.passCatch.English`$ participants, with items receiving on average `r round(df.prior_manip.int.filtered.n_subj_per_item$n_subj_per_item)` responses (range = [`r round(df.prior_manip.int.filtered.n_subj_per_item$lb)`, `r round(df.prior_manip.int.filtered.n_subj_per_item$ub)`]).

Our main qualitative hypothesis is that there would be a difference across conditions in interpretations of the same generic sentence ("Ks know when earthquakes are about to happen").
Specifically, we predict that the prevalence implied by the generic sentence will, to a first approximation, track the mean of the prevalence prior distribution.
To test this, we ran a linear model predicting responses with a fixed effect of the prior mean (as estimated by the data from Expt. 3a).
There was a significant main effect of prior mean, with a coefficient close to 1 ($\beta_1 = `r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "Estimate"]]`$, $SE=`r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "Std. Error"]]`$, $t = `r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "t value"]]`$), indicating a strong linear relationship between the mean of the prevalence prior and the resulting interpretations.
Furthermore, there was a significant mean effect of intercept, indicating that participants' responses were significantly greater than the mean of the corresponding prevalence priors ($\beta_0 = `r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "Estimate"]]`$, $SE=`r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "Std. Error"]]`$, $t = `r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "t value"]]`$).
These results show that participants interpret the novel generic sentence in a manner sensitive to the background distribution and that the generic utterance updates participants' beliefs to indicate a prevalence significantly higher than the prior.
There is a lot more to be learned from these data, however; this simple linear model explains less than a quarter of the variance in the data $r^2 = `r rs.lm.priorManip.summ[["r.squared"]]`$.

From the generics model, we are able to make a number of *a priori* predictions about quantitative differences in mean implied prevalence ratings across the ten conditions. 
These predictions were derived by examining pairs of distributions with small quantitative differences but for which the bootstrapped 95% confidence interval around the generics model predictions did not overlap (Figure\ \@ref(fig:priorManipulationResults)B, "generics model" panel).
<!-- For example, Figure\ \@ref(fig:priorManipulationResults)B ("generics model" panel) -->
With those criteria in mind, we preregistered the following contrast predictions (\{\}s indicate no predicted difference): *rare weak* $<$ \{*weak or half*, *rare half*\} $<$ \{*weak or strong*, *uniform*\} $<$ *rare_strong* $<$ \{*weak or deterministic*, *half or deterministic*\}  $<$ \{*strong or deterministic*, *rare or deterministic*\}.
We compare these predictions to those made by three fixed threshold models: a literal "some" model (fixed threshold at 0.01), and fixed threshold models with thresholds at 0.1 and 0.35.
The fixed thresholds of 0.1 and 0.35 were chosen to lie in between the 0% and 25% distributions, and the 25% and 50% distributions, respectively.
Predictions of these models, as well as a model based on the prevalence prior, for the average implied prevalence given the ten distributions are shown in Figure\ \@ref(fig:priorManipulationResults)B.

The generics model predicts that the *weak or half* and the *rare half* distributions will receive a higher implied prevalence than the *rare weak* distribution. This prediction is also made by the "not rare" model, though that model also predicts that the  *rare half* distribution receives a stronger interpretation that the *weak or half* distribution. As predicted by the generics model, the *weak or half* distribution received significantly stronger interpretations than the *rare weak* distribution ($\beta = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Pr(>|t|)"]]`$) as did the rare or half distribution ($\beta = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Pr(>|t|)"]]`$).
The *rare half* distribution was not significantly different from the *weak or half* distribution ($\beta = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Pr(>|t|)"]]`$).
 
All models predict that the *uniform* distribution will receive higher ratings than both the *weak or half* and the *rare half* distributions. 
The *weak or strong* distribution is also predicted to be greater than *weak or half* or the *rare half*, by all models except the "not absent" model, which predicts no difference for this comparison. 
In fact, both the *uniform* ($\beta = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "t value"]]`$, $p = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Pr(>|t|)"]]`$) and the *weak or strong* ($\beta = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "t value"]]`$, $p = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Pr(>|t|)"]]`$) distributions received significantly greater implied prevalence than the *weak or half* distributions, providing some evidence against the "not absent" model. 

 <!-- [n.s.] -- [weak_or_strong, uniform] vs.  rare_strong -->
The *rare or strong* distribution was predicted (by most models) to receive stronger interpretations than either the *weak or strong* and/or the *uniform*. 
Contra these predictions, such a difference was not observed in the empirical data


<!-- - Some sort of regression tests to see the test the following infleunces: -->
  <!-- - The type of the weaker distribution and the type of the stronger distribution? -->

The computational models we articulate do more than just predict the *mean* implied prevalence; they generate predictions about the full distribution of responses.
To quantitatively compare the models in their ability to predict the empirical distributions of responses for these ten different conditions, we computed the likelihood of the data under each model.^[
  As is evident from Figure\ \@ref(fig:priorManipulationResults)A (bottom row, "data"), several participants responded that 0% of Ks know when earthquakes were about to happen. Examining participants' explanations revealed that the majority of these respondents believed they were answering a question more akin to the prior elicitation / manipulation check task: Predict the next animal. This is a source of noise in the data and one that affects all models to an equal degree. All of the models we articulate (the uncertain threshold and the fixed threshold models) have no way to account for 0% responses (this is literally impossible given the utterance "Ks know when earthquakes were about to happen". To compute the likelihood of the complete data set under these models, we make the further assumption that the true generative process of the data is a mixture of responding according to the model and responding according to the prior. We calculate Bayes Factors under a variety of different reasonable "noise" probabilities (noise probabilities $= \{0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.15\}$). The conclusions derived from the Bayes Factors are unchanged under different noise probabilities.   
]
Since the models have no free parameters, the ratio of these likelihood values corresponds to the *Bayes Factor*, which quantifies the evidence in favor of one model over another. 
To account for uncertainty in the measurement of the prior, we again bootstrap the model predictions by resampling the prior.

```{r bayes factors, results="asis"}
load("cached_results/genint_manipPriors_bf.RData") # model.likelihoodRatios.summarized

tab1 <- xtable::xtable(
  model.likelihoodRatios.summarized %>% 
  ungroup() %>%
  filter(noiseProb == 0.01, 
         alternative_model %in% c("prior", "fixed_0.0175", "fixed_0.125", "fixed_0.375")) %>%
  mutate(`Alternative model` = factor(alternative_model, levels = c("prior", "fixed_0.0175",
                                                                  "fixed_0.125", "fixed_0.375"),
                                    labels = c("Prior", "Threshold = 0.01",
                                               "Threshold = 0.1",
                                               "Threshold = 0.35")),
    `log Bayes Factor` = paste(
    round(expected_log_bf, 1), " (", round(lower_log_bf,1), ", ", round(upper_log_bf,1), ")", sep=""
  )) %>% select(`Alternative model`, `log Bayes Factor`),
  caption = "Bayes Factors quantifying evidence in favor of the uncertain threshold generics model.")

print(tab1, type="latex", comment = F, table.placement = "H", size="\\fontsize{9pt}{10pt}\\selectfont", include.rownames=FALSE)
```


In addition to the qualitative analyses afforded by the regression analysis, we examine the generic interpretation model's *a priori* quantitative predictions and compare those to a set of alternative models. 
The first alternative model we consider is prevalence prior; we have already seen via the regression analyses that this will not be sufficient. 
We also consider three fixed threshold models which have increasing sophisticating as to their knowledge of the prior distributions: a literal "some" model (fixed threshold at the lowest possible value), a "not absent" model where the fixed threshold is placed in between the "absent" distribution and the "weak distribution" (threshold $\sim 10\%$), and a "highest mode" model where the fixed threshold is placed, in a context-sensitive manner, in between the two component distributions thus always signalling the higher of the two. 
Finally, we have our uncertain threshold model, which integrates the prevalence prior with the pressure from the literal semantics that favors higher prevalences.
Predictions of these models for the ten distributions of interest are shown in \red{Figure X}.



The differences in interpretations are evident in the empirical distributions of responses (Figure \@ref(fig:priorManipulationResults)A, bottom row).

# General Discussion

Much of what we come to learn about the world comes from other people, often expressed in language.
Generic language is the foremost case study of how abstract, generalizable knowledge is transmitted in language.
Despite its ubiquity and relative morphosyntactic simplicity, there have been few quantitative studies of generics and no quantitative models that make predictions about how generic language is interpreted.
Here we test a quantitative model on a broad set of quantitative data, including a replication data set from @Cimpian2010.
We find reliable, fine-grained, context-sensitive differences in interpretations, which only our uncertain threshold model can account for.
This work, thus, provides the computational formalism necessary to describe how beliefs are updated from generic language.

In what follows, we relate our account to a related kind of account based on the idea of *domain restriction*, discuss how our answer to the problem of generic interpretation can inform an answer to the problem of a parallel problem *generic identification*, and speculate about the origins of context-sensitive generic interpretations.
<!-- - Finer points about the data? Discussion of explanation data? Have relationship to DR theories come here? -->

## Domain restriction

In this paper, we test a particular model of generic interpretation.
This model use a prior distribution over the prevalence of a feature across relevant categories and assumes the truth-functional meaning of the generic is an uncertain threshold on the prevalence. 
The prevalence prior distributions, which we elicit from participants, are likely already influenced by the conceptual knowledge of our participants.
For example, the prevalence priors over properties that have to do with reproduction (e.g., *have menstrual cycles*, *get erections*) are multi-modal with one mode around 50%, clearly reflecting participants' knowledge that these properties are specific to one sex of the animal.

Other formal theories of generic language make use of a mechanism by which the domain over which the prevalence of the feature in the category is calculated is contextually restricted [e.g., @Cohen1999; @Declerck1991].
In such a theory, the statement "Lorches have a menstrual cycle" gets evaluated as "Female lorches have a menstrual cycle"; under such an account, the prevalence of the feature would be 100% (or near 100%) because 100% of the *relevant lorches* (i.e., the females) have a menstrual cycle.
Domain restriction is an attractive mechanism because it would allow the semantic theory to be a context-invariant fixed threhsold [e.g., at 50%; @Cohen1999].
However, such a theory still needs to explain why and formalize how the domain gets restricted; without such a formal account to accompany the domain restriction hypothesis, the hypothesis is vacuous: it explains but does not predict.

The uncertain threshold model that we present here can be seen as a particular instantiation of a domain restriction theory. 
In our model, the statement "Lorches have a menstrual cycle" is resolved by a literal interpretation model to mean mean "50% of lorches have a menstrual cycle".
Were a listener to have the background theory that the reason why half of a category would have such a property is for reproductive purposes, the listener's interpretation would most naturally be expressed as "Female lorches have a menstrual cycle".
Indeed, this kind of conceptual knowledge is present in our participants, \red{as evidence by their explanations}.
Our model restricts the domain by reasoning about a threshold that would make sense for that particular property, given its background distribution on prevalence. 
Reasoning about an uncertain threshold via backgroudn knowledge is one mechanism by which the domain can be restricted.

## Generic identification

It is often noted that bare plurals (e.g., "Dogs have four legs") though often interpreted as generics can also manifest as *non-generics* (e.g., "Dogs are on my front lawn").
Figuring out when a sentence expresses a generic meaning vs. a non-generic meaning is called the problem of generic identification is a parallel challenge to the problem of generic meaning, which we have addressed in this paper.

Our answer to the question "what do generics mean?" involves vagueness concerning the prevalence. 
Interpreting the uncertain threshold as a kind of domain restriction might be sufficient to extend the model to interpreting "non-generic" bare plurals (e.g., "Dogs are on my front lawn").

First, let us note that many generics actually expressed two or more generalizations. 
For example, "Lorches eat insects" says that $[gen \text{ lorches}] [hab \text{ eat}] [gen \text{ insects}]$.
<!-- In this paper, we have looked at the first aspect of generic understanding, the generalization over the head noun category (e.g., lorches) though in previous work we have shown that the same uncertain threshold model can explain habitual language as well [e.g., "John smokes"; @TesslerLangGenUnderReview]. -->
In this paper, we have looked at the first aspect of generic understanding, the generalization over the head noun category (e.g., lorches) though in the next chapter we will see that the same uncertain threshold model can explain habitual language as well (e.g., "John smokes").
An old intuition with generics is that at least some express properties that are especially timeless and enduring [@Lyons1977; @Gelman2003; @Gelman2007], though it is not clear what generalization this should be attributed to. 
For instance, "Dogs are on my front lawn" exhibits only a single generalization (e.g., over *dogs*); it lacks the habitual verb phrase (*eat*).
The habitual aspect of the verb may influence the expected prevalence in the head noun category: the more vague, habitual predicate may apply to more instances of the category (e.g., it's easy to tell whether or not a dog is on your front lawn, but harder to tell whether or not an animal eats insects *habitually*).
For non-habitual predicates, the expected prevalence would be substantially more restricted. 
Thus, we may find continuity in interpretations of underspecified noun phrases which include so-called "non-generics": "Tigers have stripes", "Tigers are in the savanna", "Tigers are in zoos", "Tigers are in my local zoo", "Tigers are on my front lawn".
The uncertain threshold model may be such a way as to pragmatically infer a continuum of interpretations, from generics that are interpreted as universals ("Tigers are mammals") to those that are interpreted existentially but habitually ("Tigers are in zoos") to those that are interpreted existentially and non-habitually ("Tigers are on my front lawn").

<!-- ## Implications for social categories -->

## Development of weak interpretations

The experimental data we present provides strong evidence for the existence of "weak generics", which we operationalize here as generic statements that imply a prevalence less than 50%. 
Statements like "Lorches live in zoos", "Feps perform in the circus", and "Wugs have seizures" all received average implied prevalence ratings below 50%, with many individual participants providing ratings of 10% or less (Expt. 2b).
When manipulating the prevalence prior distributions to suggest only prevalence levels of around 0% or aroud 25%, participants interpret the statement "Ks know when earthquakes are about to happen" to mean about 25% (Expt. 3b). 
Adults readily use their causal knowledge of the world to restrict the interpretation of a generic statement to a reasonable population, \red{as evidenced by their explanations}.
This is an aspect of generic language understanding often swept under the rug: Generics are believed to (in a generic sense) carry strong implications.
Here, we show that generics also carry weak implications.

It is often argued that children have adult-like understanding of generics from a very young age \red{(cite Gelman, Leslie, Brandone, Cimpian)}.
These studies often look at a small range of properties that do not exhibit much context-sensitivity. 
It is an open-question whether or not children understand "weak generics".

There are theoretical reasons to think that the understanding of weak generics may be later developing. 
Prevalence priors are naturally modeled as Beta distributions, which are probability distributions over numbers ranging from 0 - 1: In other words, Beta distributions are probability distributions over probabilities. 
These probabilities can be thought of as the weight of a biased coin, and a common interpretation of the parameters of the Beta distribution are as *pseudo-counts*, imaginary outcomes of a previous experiment of flipping the coin.
For example, a Beta(1, 9) distribution describes beliefs about the weight of the coin upon observing 1+9 = 10 flips of that coin, 1 of which landed on heads and 9 of which landed on tails. 
This probability distribution heavily favors numbers around 0.1, as a coin of that weight is most likely to have generated the observed 1 out of 10 heads. 

If we take the pseudo-counts as indices of worldly experience, it is interesting to note what happens as the pseudo-counts approach 0. 
As the parameters approach zero, the Beta probability distribution increasingly prefers extreme values: 0 or 1. 
If our knowledge about predictive probabilities are represented by Beta distributions, then early on, our belief distributions would favor categorical thinking. 
However, once we start to learn about different kinds of categories and properties, probably via labeling, we may begin to represent multiple distributions in our prevalence priors. 
Such a development would predict that children start out interpreting all generics as strong generics, and that the understanding of weak generics is later developing ability.

<!--   - since properties tend to be rare of objects (e.g., Oaksford & Chater wason), false-positives are not as bad as false-negatives (from a purely informational standpoint, because false-negatives are less likely? because most things are going to be correct rejections?) -->
<!--   - howver, for social categories this can be bad, as social categories are socially-constructed, reflecting what the speaker believes is a relevant categorization to make (Foster-Hansen, maybe Steve Roberts?) -->
<!-- - does this *strong interpretations before weak interpretations* relate at all to Gelman & Bloom's developmental changes? -->

## Conclusion

# References


```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
