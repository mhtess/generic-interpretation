---
title             : "Learning from generic language"
shorttitle        : "Learning from generic language"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "tessler@mit.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{xcolor}
  - \usepackage{bbm}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Generic language (e.g., "Birds have hollow bones") conveys generalizations about categories and is believed to be a fundamental and ubiquitous mechanism of learning without direct experience. Understanding generics, however, involves an extreme dependence on context that has made the precise articulation of how generic language updates beliefs beyond the reach of any extant theory of generics. Here, we formalize the null hypothesis that generics convey a single, positive observation to the listener and show that it is mathematically equivalent to a recent proposal by  @Tessler2019psychrev that generics are a kind of vague quantifier. Here, we test the adequacy of this model to account for the belief-updating mechanism of generic statements---how novel generics are interpreted---and compare it to a model wherein understanding a generic involves reasoning about the intentions of the speaker. We find that generics are best modeled as single positive observations sampled with a pedagogical intent. The results are consistent with a recent proposal that generics can be understood as non-linguistic phenomenon.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
# install.packages(c("tidyverse", "cowplot", "ggthemes", "ggpirit", "viridis"))
# devtools::install_github("crsh/papaja")
library("papaja")
library(tidyverse)
library(cowplot)
library(ggthemes)
library(RColorBrewer)
library(ggpirate)
library(viridis)
library(brms)
theme_set(theme_few())
remove(list= ls())
```


```{r analysis_preferences}
# Seed for random number generation
set.seed(42)

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)
format_regression_effects <- function(brm_summary, fixed_effect_name, n_digits = 3){
   #print(fixed_effect_name)
   e1 <- brm_summary[["fixed"]][[fixed_effect_name, "Estimate"]]
   e_lower <- brm_summary[["fixed"]][[fixed_effect_name, "l-95% CI"]]
   e_upper <- brm_summary[["fixed"]][[fixed_effect_name, "u-95% CI"]]
   return(paste(
     format(e1, digits = n_digits), " [", 
     format(e_lower, digits = n_digits), ", ", 
     format(e_upper, digits = n_digits), "]", sep = ""))
}

compute_r2 <- function(df,v1, v2, sigfigs = 3){
  return(format(cor(df[[v1]], df[[v2]])^2, digits = sigfigs))
}

compute_mse <- function(df, v1, v2, sigfigs = 3){
  return(format(mean( (df[[v1]]-df[[v2]])^2), digits = sigfigs))
}


```

# Introduction


<!-- MH: Refer to the model as the "vague quantifier" or "vague threshold" account? Easier to discuss alternative models. -->

<!-- - is there a way to connect to the claims of Cimpian2009 about generics influencing feature-as-cause vs. feature-as-effect explanations? -->
  <!-- - perhaps not due to the generic vs. specific distiction, but to prior beliefs about the property? -->
  
<!-- - in generic endorsement: connect double generalizations ("Dogs bark") to so-called "non-generics" ("Dogs are on my front lawn"). Since "are on my front lawn" is a stative (?) property, there is only one generalizations about dogs which gets heavily restricted by the property? -->

<!-- Learning from others comes in many forms.  -->
<!-- An expert may tolerate onlookers, a demonstrator may slow down when completing a particularly challenging part of the task, and a teacher may actively provide pedagogical examples and describe them with language [@Kline2014; @Csibra2009].  -->
<!-- Informative demonstrations may be particularly useful for procedural learning (e.g., hunting seals, riding a bicycle).  -->

Much of what we learn about the world comes not from our own experience but by learning from others, often with language.
A learner can induce a generalization from examples, but language can go beyond examples, encoding information that is itself a generalization.
Generalizations about categories are conveyed using *generic language* [e.g., "Birds have hollow bones"; @Carlson1977], a clear case of abstract knowledge transmitted via language [@Gelman2009].
Generics are ubiquitous in everyday conversation and child-directed speech [@Gelman1998; @Gelman2008; @GelmanEtAl2004], and learning from generics is thought to be central to conceptual development [@Gelman2004; @Cimpian2009:explanations].
Understanding how beliefs are updated from generic language is thus an important question for cognitive science.

Generics exhibit extreme sensitivity to context, however, that has made it difficult to define quantitatively how they update beliefs. 
Intuitively, generics  "[...] appear to be commonly taken in a rather strong sense, as though the qualifier *always* had implicitly crept into their interpretation" [p. 172; @Abelson1966], and indeed adults learning from generics about both familiar and novel animals (e.g., "Bears like to eat ants"; "Lorches have purple feathers") rate the *implied prevalence* of the feature (e.g., the percentage of bears that like to eat ants) as very high [$\sim 85\%$; @Gelman2002; @Cimpian2010, Expt. 1].
Yet other true generics like "Mosquitos carry malaria" only seem has the quantificational force of an existential claim, meaning *some mosquitos carry malaria* or *mosquitos can carry malaria*.
<!-- Some empirical evidence points to the existence of such *weak generics*:  -->
Consistent with this intuition, @Cimpian2010 (Expt. 3) found lower implied prevalence from generics about features that could be construed as accidental [e.g., "Crullets have fungus-covered claws"; see also @Khemlani2009; @Khemlani2012 for evidence of weak inferences from generics such as "Lions eat people"].
What sort of literal meaning could account for the panoply of interpretations that come with generic sentences?

If, in fact, "Mosquitos carry malaria" conveys something very weak, analogous to *some mosquitos, under some circumstances, carry malaria*, then the knowledge conveyed by the generic could be similar to learning (i) one mosquito, in one particular circumstance, was once observed carrying malaria and (ii) given that properties covary with kinds in systematic way, if one mosquito once had this property, then it is reasonable to think there is some larger (but unknown) subset of mosquitos in some larger (but unknown) class of situations, also carry malaria.
From this intuition, one could formalize the meaning of a generic statement as equivalent to Bayesian belief-updating of structured prior knowledge [@Tenenbaum2011] after observing a single positive example of the category with a property.
In this paper, we show how this formalism is mathematically identical to a recent model by @Tessler2019psychrev that treat generics as a kind of vague quantifier. 
This earlier work tested only such a model via its implications for *truth judgments* or *endorsements* (e.g., is "Mosquitos carry malaria" true or false?); the descriptive adequacy of such a model for belief updating or interpretations has not been tested. 

Even with sufficiently structured prior knowledge about categories and properties, however, the information conveyed by a generic may be more than that encoded by just a single positive observation.
One plausible source for deriving stronger interpretations is social reasoning: Why did the speaker bother to tell me this generic sentence?
That is, pragmatic enrichment of the literal content of generics could manifest as a stronger implied prevalence than just a single positive observation.
The idea that generics convey a communicatively relevant observation to the listener is consistent with a view of generics as an essentially non-verbal but communicative phenomenon [@Csibra2015].
In this paper, we additionally set out to test a literal model of generic interpretation against a pragmatic model, which we formalize in the Rational Speech Act framework [@Frank2012; @Goodman2016]. 
<!-- That is, given that interpretations of linguistic expressions manifest in the communicative context between a speaker and hearer [@Grice1975; @Levinson2000; @Clark1996], the inferences that listeners derive from generics may too be strongly determined by a theory of pragmatics. -->

In addition to these theoretical questions, basic empirical questions about context-sensitive generic interpretation still remain. 
Though the intuitive implied prevalence of "Mosquitos carry malaria" is very low (i.e., very few mosquitos carry malaria), the empirical data supporting this conclusion is mixed. 
In @Cimpian2010 Expt. 3, the mean implied prevalence of "weak generics" was $70\%$, significantly lower than that observed for generics about biological properties, but still conveying well above a majority having the property. 
Similarily, inferences that members of a category have a property (e.g., "Leo is a lion. Thus, Leo eats people.") can be quite weak, though still the reported ratings were greater than 50\% [@Khemlani2012].^[
  In this study of @Khemlani2012, the measurement was actually a confidence rating on -3 to +3 scale. All reported mean confidence ratings were greater than zero. 
]
Thus, though intuitively generics can have weak implications, the actual implied prevalence from weak generics in these previous studies was still quite high, leaving open the question of whether bonafide weak interpretations (e.g., implied prevalence $<50\%$) can result from learning novel generics.

The rest of this paper is organized as follows.
In the first section, we formalize a model for learning from generic langauge that takes a generic statement to literally convey that a single instance of the category has the property.
We show that this model is mathematically equivalent to the interpretation model proposed by @Tessler2019psychrev, wherein generics were conceived as a kind of vague quantifier.
We then propose an extension to this model where generic language can be understood pragmatically via recursive Bayesian reasoning [@Baker2009; @Frank2012].
We compare the literal and pragmatic models against additional control models that use the same prior knowledge but which update their beliefs via different semantic mechanisms: fixed-thresholds analogous to the semantics of quantifiers like *some* and *most*. 
In a preliminary study, we replicate and extend the findings of @Cimpian2010 that biological and accidental properties receive differential interpretations and find substantial variability in implied prevalence by-item; the items that exhibit the most variability, however, have a potential confound which leads us to design our main experiments.
In Experiment 1, we use a large, diverse stimulus set better able to reliably elicit high variability in implied prevalence; we use these data to test our models of generic interpretation against each other and against control models. 
In Experiment 2, we experimentally manipulate background knowledge and test the model-predicted effects on interpretations of novel generics.
Together, these results provide strong evidence that generics update beliefs in a context-sensitive manner and are best modeled as a pedagogically-transmitted observation. 

<!-- We show that this model is mathematically equivalant to a listener model proposed by -->
<!-- The @Tessler2019psychrev model was used not used to predict interpretations, however, but rather endorsements (e.g., is "Mosquitos carry malaria" true or false?), modeled as speaker deciding whether or not to produce a generic statement to a listener. -->
<!-- The adequacy for the belief-updating component of the model (the listener) has not yet been tested. -->
<!-- A model of generics as conveying a single positive observation can be distinguished from a bonafide communicative model wherein generics are understood with a pedagogical intent.  -->
<!-- This latter model connects to a recent proposal by @Csibra2015 that generics can be understood nonverbally but with a communicative intent.  -->



<!-- Generics provide information that is difficult to gain access to or dangerous to observe -->
<!-- Learning from language is uniquely powerful because language can convey abstract content that goes beyond direct experience. -->


<!-- that difficult to gain access to [e.g., birds have hollow bones] or dangerous to observe [e.g.,  certain plants are poisonous, staring at the sun causes blindness].  -->
<!-- The premier example of language's ability to transmit abstract knowledge comes in the form of statements that convey generalizations, otherwise known as *generic language*  -->
<!-- One of the most important roles for generic language is to provide learners with information about new or poorly understood categories. -->

<!-- But unlike statements about concrete events or exemplars  (e.g., "This bird flys"), generics convey information that is not directly observable and, in fact, resilient to counterexamples (e.g., penguins do not fly). -->
<!-- Here, we investigate how generic language is interpreted  -->

<!-- The standard model of generic sentences is that they communicate about categories "as a whole", while permitting exceptions. -->
<!-- For example, argued that generics, "...once accepted psychologically, -->
<!-- The fact that generics convey strong implications may explain why social stereotypes are propagated and maintained, even in the face of contradictory evidence [@Rhodes2012]. -->

<!-- The simple "category + exceptions" view of generics can only take us so far, however. -->


<!-- There are open theoretical questions about how the putative difference between *weak* and *strong* generics comes about. -->
<!-- Property-specific generic interpretations (e.g., "Lorches have purple feathers" vs. "... fungus-covered claws") have been explained as differences in *theory-based expectations* [@Leslie2008; @Cimpian2010theory; @Prasada2013], but these notions have not been made sufficiently precise to provide a quantative, predictive account of how beliefs will be updated given a specific generic statement. -->
<!-- @Cohen2004 argues for a panolply of conditions that give rise to existential interpretations of generics (e.g., focus-sensitive additives, as in "Birds lay eggs. Mammals lay eggs too.")^[ -->
<!--  At least two mammals, Platypuses and echidnas, reproduce by laying eggs. -->
<!-- ], but the empirical support for these factors influencing interpretations is limited. -->


<!-- proposed a quantitative model of generic language as a kind of vague quantifier operating on inductive beliefs, combining tools from formal semantics with Bayesian cognitive modeling. -->
<!-- This endorsement model is cast as a speaker deciding whether or not to produce a generic statement to a listener, who updates their prior beliefs about the prevalence of the feature in the category using a vague, quantificational meaning for a generic. -->
<!-- The listener model thus provides a mapping from a generic statement to implied prevalence---it is a quantitative account of how generics update beliefs. -->
<!-- This paper tests the predictions of this listener model directly. -->
<!-- In doing so, we also look for evidence for weak generics, which are predicted to exist and provide the strongest test of the uncertain threshold account.  -->

<!-- Interpreting a generic as *most* can also be too weak: "Triangles have three sides" is a statement about *all triangles*; if a shape does not have three sides, it is not a triangle.  -->
<!-- Indeed, the differences between quantified statements and generic statements are appreciated by children as young as three years [@Gelman2015genericsQuantifiers]. -->

<!-- Flexible interpretations of generic language may be in part attributed to beliefs about the property [@Nisbett1983]. -->
<!-- Properties that are biological in nature (e.g., "Wugs have wings") are interpreted as applying to most or all of the category [@Gelman2002; @Brandone2014], while features that could be construed as incidental (e.g., "Crullets have fungus-covered claws") are interpreted as applying to relatively fewer members of the category [@Cimpian2010]. -->



<!-- First, we test whether or not weak interpretations of generics are possible by testing interpretations of generics about a wide range of properties (Expt. 1a).  -->
<!-- Then, we measure interlocutors' prior beliefs about the prevalence of the property (Expt. 1b) using the prior elicitation paradigm in @TesslerGenerics.  -->
<!-- We compare our interpretation model's predictions, which use the empirically measured priors, to the implied prevalence data and find that our model is able to predict, with high quantitative accuracy, the context-sensitive interpretations of generics.  -->
<!-- Finally, we show that, rather than being merely correlationa in nature, the prevalence prior is causally related to generic interpretation by manipulating the prevalence prior and measuring participants interpretaions of novel generics (Expt. 2).  -->


<!-- Despite these variable truth conditions, generic sentences are thought to have charasterically "strong implications" [@Gelman2002; @Cimpian2010; @Brandone2014]. -->
<!-- It is these strong interpretations that make generics lead to stereotypes [@Rhodes2012]. -->

<!-- We recently proposed that generics communicate in an underspecified way about the prevalence of the feature. -->
<!-- That is, our model predicted that "Mosquitos carry malaria" is a felicitious utterance because listeners expect *carrying malaria* to not be widespread within a category, even when it is present in some of the category.  -->

<!-- @Cimpian2010 too found that generics predicating biological properties of kinds carry strong interpretations.  -->
<!-- The properties used in those studies were all related to body parts of animals (e.g., "Lorches have purple feathers" $\rightarrow$ *almost all lorches have purple feathers*).  -->
<!-- Bare plural nouns predicated with *accidental* properties (e.g., "Lorches have broken legs") had significantly weaker interpretations in terms of how many of the kind were expected to have the property.  -->
<!-- However, these accidental properties do not lend themselves to generic interpretation and sound infelicitous.  -->
<!-- At the same time, the average implied prevalence for the *accidental property* generics was close to 70\%, leaving open the question as to whether or not generics can have truly weak (i.e., minority-based) interpretations. -->


# Computational Model

Generic language conveys generalizations about categories [@Carlson1977; @Leslie2008].
<!-- We propose that generics convey generalizations by providing a minimal example---a single positive observation---communicated with a social force.  -->
Human generalization from observational data can be well described using the language of probability [@Shepard1987] and Bayesian belief-updating with structured, prior knowledge [@Tenenbaum2011].
We propose a null hypothesis that generics convey the same information-content as a single, positive observation interpreted with respect to structured background konwledge.
Formally, if a listener integrates their prior knowledge about the underlying probability or prevalence of the property $r \in [0, 1]$ with a single positive observation according to Bayes' rule, the model is:

\begin{align}
L_0(r, \mid u = gen) &= \frac{P(u = gen \mid r) \cdot P(r)}{\int_0^1 {P(u = gen \mid r') \cdot P(r') \diff r'}}  \nonumber \\
 &= \frac{P(x = 1 \mid n = 1; r) \cdot P(r)}{\int_0^1 {P(x = 1 \mid n = 1; r') \cdot P(r') \diff r'}}  \nonumber \\
 &= \frac{r \cdot P(r)}{\int_0^1 {r' \cdot P(r') \diff r'}} \label{eq:L0}
\end{align}

\noindent where $x$ denotes the number of positive examples and $n$ denotes the number of observed examples. 
We take the conditional probability of the generic utterance $u$ given some prevalence $r$---$P(u\mid r)$---to represent the literal semantic content of the generic utterance [@Frank2012], which we define to be belief-updating from a single positive observation: $P(x =1 \mid n = 1; r)$. 
The latter is simply the probability of a coin with weight $r$ landing on hands once, given that was flipped once, which is simply $r$.

Belief-updating based on a single positive observation is mathematically equivalent to the interpretation model proposed by @Tessler2019psychrev, as belief-updating based on a threshold-function (e.g., $\denote{gen} = x > \theta$) whose threshold value is uncertain and is drawn from a uniform prior ($\theta \sim \text{Uniform}(0, 1)$):

\begin{align}
L_0(r, \theta \mid u) \propto {\delta_{\denote{u}(r, \theta)} \cdot P(r) \cdot P(\theta)} \label{eq:L0a}
\end{align}

Here, the truth-functional meaning of a generic is a threshold-function mandating that the prevalence $r$ is greater than the threshold $\theta$, represented by the Kronecker delta  $\delta_{\denote{u}(r, \theta)}$ that returns $1$ when the utterance is true (i.e., when $r > \theta$) and $0$ otherwise.

\begin{align}
\delta_{\denote{u_{gen}}(r, \theta)} &\propto  \begin{cases}
1 & \text{if } r > \theta \\
0 & \text{otherwise}
\end{cases}\label{eq:delta}
\end{align}

This version of the model uses the standard truth-functional tools of formal semantics [@Montague1973] and relates to the literal semantics of quantifiers, which can also be described by threshold functions (e.g., $\denote{some} = \{r > 0\}$, $\denote{most} = \{r > 0.5\}$, $\denote{all} = \{r = 1\}$).
Note that Equation \ref{eq:L0a} can be rewritten as: $L_0(r, \theta \mid u) \propto P(u \mid r, \theta)$ where 

\begin{align}
P(u \mid r, \theta)  &\propto \begin{cases}
P(r) & \text{if } r > \theta \\
0 & \text{otherwise} \end{cases} \label{eq:delta2}
\end{align}

\noindent since $\theta \sim \text{Uniform}(0, 1)$ hence $P(\theta) \propto 1$. To arrive at $L_0(r \mid u)$ from Equation \ref{eq:delta2}, we integrate out $\theta$.

<!-- \noindent and $\mathbbm{1}_{r > \theta} \sim \text{Bernoulli}(P(r > \theta))$. -->
<!-- \begin{array} -->
<!-- L(r, \mid u) &=& \int_{0}^{1} P(r, \theta \mid u) \diff \theta \nonumber \\ -->
<!-- &\propto& \int_{0}^{1} P(r) \cdot \mathbbm{1}_{r > \theta} \diff \theta \nonumber \\ -->
<!-- &=& \int_{0}^{x} P(r) \cdot \mathbbm{1}_{r > \theta} \diff \theta + \int_{x}^{1} P(r) \cdot \mathbbm{1}_{r > \theta} \diff \theta  \label{eq:L0c} -->
<!-- \end{array} -->
\begin{align}
L_0(r, \mid u) =& \int_{0}^{1} L_0(r, \theta \mid u) \diff \theta \nonumber \\
\propto& \int_{0}^{1} P(u \mid r, \theta)  \diff \theta \nonumber \\
=& \int_{0}^{r} P(u \mid r, \theta) \diff \theta + \int_{r}^{1}P(u \mid r, \theta) \diff \theta \nonumber \\
=& \int_{0}^{r} P(r) \diff \theta + \int_{r}^{1} 0 \diff \theta \nonumber  \\ 
   = &  P(r) \int_{0}^{r} \diff \theta \nonumber \\
     =&   P(r) \cdot r \label{eq:L0d}
\end{align}

<!-- The second integral is for $\theta$ for values between $r$ and $1$, which is equal to 0 because the threshold is greater than the prevalence. Thus, it is simply: -->

<!-- \begin{eqnarray} -->
<!-- L(r, \mid u) &\propto& \int_{0}^{r} P(r) \diff \theta \\ -->
<!--   & = &  P(r) \int_{0}^{r} \diff \theta \\ -->
<!--   & = &  P(r) \cdot r \label{eq:L0d} -->
<!-- \end{eqnarray} -->

## Pragmatic enrichment

The generic interpretation model used by @Tessler2019psychrev makes the strong claim that the literal contribution of a generic sentence is informationally equivalent to observing a single, positive example.
In context, however, generics could convey a stronger generalization than that implied by just a single example, if the listener reasons that the speaker intentionally produced the generic. 
We model this in the Rational Speech Act framework [@Frank2012; @Goodman2016; @problang], wherein a listener $L_1$ reasons about why a speaker $S_1$ produced an utterance:

\begin{eqnarray}
L_1(r, \mid u) &\propto& S_1(u \mid r) \cdot P(r) \label{eq:L1}\\ 
S_1(u \mid r) &\propto& \exp{(\alpha \cdot \ln L_0(r \mid u) - \text{cost}(u))} \label{eq:S1}
\end{eqnarray}


Equation \ref{eq:S1} is the endorsement model of @Tessler2019psychrev, where an approximately-rational speaker decides whether or not to produce a generic utterance to convey information to a literal listener (Equation \ref{eq:L0}).
Equation \ref{eq:L1} is a model of a pragmatic listener who understands that the generic utterance was produced by an intentional speaker $S_1$. 
Following @Tessler2019psychrev, we assume the speaker's only alternative utterance is an information-less silent utterance and we allow the act of producing the generic to incur some production cost to the speaker (plausibly, $\text{cost}(generic)$ $>$ $\text{cost}(silence)$).
Under the interpretation of a generic sentence as communicating a single, positive observation, the alternative is *not* producing that observation. 
For purposes of quantitative modeling, we allow both the cost of the generic ($\text{cost}(generic)$) and the speaker's rationality parameter $\alpha$ to be free parameters that we infer from the behavioral data. 


```{r cartoon, fig.cap="Sketch of the pragmatic generic interpretation model. The literal contribution of a generic (e.g., \"Dorbs have yellow crests\") is equivalent to observing a single positive example (i.e., observing an actual dorb with a yellow crest). A pragmatic listener interprets the utterance as coming from a speaker who intentionally produced the utterance in order to convey information to a naive listener ($L_0$). The literal listener interprets the utterance on drawing on their prior knowledge about other categories and properties.", fig.width = 6, cache=F}
ggdraw() + draw_image("figs/cartoon.png")
```


<!-- Using the truth-functional tools of formal semantics [@Montague1973], the literal meaning of a generic statement can be modeled as a threshold function operating on the prevalence $r$ of the feature in the category (e.g., the proportion of mosquitos that carry malaria): $\denote{generic} = \{r > \theta\}$.^[ -->
<!--   The prevalence $r$ can be thought as a "prevalence in the mind", or a subjective degree of belief about the property in the category, which may not accurately track the empirical frequency. -->
<!-- ] -->
<!-- The literal meaning of quantifiers can also be described by threshold functions (e.g., $\denote{some} = \{r > 0\}$, $\denote{most} = \{r > 0.5\}$, $\denote{all} = \{r = 1\}$). -->
<!-- @Tessler2019psychrev posit that the corresponding threshold for the generic $\theta$ is *a priori* uncertain---formally, it is drawn from a context-invariant, uniform prior distribution on thresholds $P(\theta)$---and is resolved by context. -->
<!-- Of course, different threshold distributions could be learned over time for different properties. -->
<!-- The choice of a uniform distribution here is to explore how well context-sensitive generic interpretations  can be modeled by differences in corresponding prior beliefs about properties, without having to posit semantic differences between different generic sentences. -->

<!-- In order for a statement to update beliefs about prevalence in a given category, a listener must have prior beliefs about prevalence abstracted across categories. -->
<!-- To interpret a generic sentence, a listener draws upon her abstract knowledge about properties, formalized by a prior distribution over likely prevalence levels $P(r)$. -->
<!-- Though not the direct focus of this paper, the prevalence prior distribution can be seen as a relatively minimal formalization of theory-based expectations.  -->
<!-- Different theory-based relationships between kinds and properties will give rise to different prior distributions over prevalence.  -->
<!-- The prevalence prior, thus, acts as a statistical layer between conceptual knowledge and natural language semantics.  -->

<!-- The interpretation model computes a posterior distribution on prevalence by considering different possible thresholds $\theta$. -->
<!-- Given a particular value of $\theta$, the literal semantics rules out all prevalences $r$ below $\theta$. -->
<!-- Consider the model behavior when the prevalence prior is a uniform distribution---$P(r) = \text{Uniform}(0, 1)$ (Figure\ \@ref(fig:modelSimulations)A; top). -->
<!-- Given a particular value of $\theta$, the posterior on prevalence will be uniform over all prevalence levels above $\theta$ (Figure\ \@ref(fig:modelSimulations)B; top row); -->
<!-- thus, higher prevalence levels $r$ are consistent with more thresholds $\theta$. -->
<!-- Therefore, given a uniform prevalence prior, the generic interpretation model returns a non-uniform posterior that favors higher prevalence levels (Figure\ \@ref(fig:modelSimulations)B; top row, right-most column). -->
<!-- This behavior captures the intuition that, without strong background knowledge, generics probably imply that most or all of the category has the property. -->


## The influence of prevalence priors and pragmatic reasoning

Both the literal and the pragmatic generic interpretation models (Equations \ref{eq:L0} and \ref{eq:L1}, respectively) balance the prior probability of different prevalence levels $P(r)$ with the overall preference for higher prevalence levels $r$.
Thus, interpretations strongly depend on the prevalence prior $P(r)$, which may display interesting structure reflecting domain-specific beliefs about the property.
Figure\ \@ref(fig:modelSimulations)A shows hypothetical prevalence priors for the features *fly* and *carry malaria* as well as an abstract property *Y* without structured background knowledge. 

The prevalence prior can be understood as a distribution over different categories (and their associated prevalence of the feature).
For a biological property (e.g., *Xs fly*), many categories have 0\% of members who have property (e.g., 0\% of dogs, rabbits, etc... *fly*); however, among categories with at least some members who fly, we would expect all or nearly all of them to fly (robins, falcons, pterodactyls, etc...), hence the distribution would have a second mode near 100\% (Figure\ \@ref(fig:modelSimulations)A second row).
In a more abstract sense, the prevalence prior reflects beliefs about kinds and properties (e.g., there is something internal or external to the entity which causes it to have the property); if such a causal mechanism is believed to exist for some kinds and not others, we would expect the prevalence prior distribution to follow a mixture distribution (e.g., a mixture of Beta distributions as is shown for the *fly*, *lay eggs*, and *carry malaria* cases above). ^[
  This assumption is similar in spirit to that employed by *Hurdle Models* of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., when modeling adverse reactions to vaccines; @hurdleModels]
]
Such a structured prior distribution could vary by the mixture component (e.g., for how many different kinds does the mechanism exist?) as well as for the parameters of the component(s) that govern the prevalence for the kinds with the mechanism (i.e., given that there is such a mechanism, how strong do we expect that mechanism to be and do we expect further distinctions among those with a causal mechanism?).
In the folowing simulations, we assume some mixture distribution structure. 
In Experiment 1, we use a three-component mixture-distribution structure to model the elicited prevalence prior data.
In Experiment 2, we manipulate the prevalence prior.

Interpretations from the literal generic interpretation model (Figure\ \@ref(fig:modelSimulations)B) are similar to different quantified statements for generics about different properties.
For a biological property, a generic ("Xs fly") is interpreted most similarly to "Most Xs fly".
This results from the prior distribution over a biological property like *fly* being bimodal with modes near 0\% and 100\%.
For the accidental property prior, "Xs carry malaria" is most similar to "Some Xs carry malaria" because the property is expected to be rare within categories in which the property is present.
If a listener knows the existence of the property depends upon the sex of the creature (e.g., *lays eggs*), the prevalence interpretation from a generic ("Xs lay eggs") should be roughly that 50% of the Xs lay eggs (only the females).
<!-- Similar to the *fly* prevalence prior, the prior distribution over prevalence for a property like *carry malaria* has substantial probability mass at or near 0\%, since most categories do not have the property present at all; however, among categories with at least a few members who carry malaria, it is very unlikely that all or even many carry malaria; the second mode of this prior distribution is near 10\%.  -->

<!-- We test these property-sensitive interpretations in the following experiments.  -->
<!-- Different interpretations for this model come from different prior distributions over prevalence $P(r)$, which -->

Interpreting a generic sentence as providing a single positive observation to the listener provides a way to understand generics in a property-specific manner. 
Listeners may derive a stronger interpretation of a generic, however, by reasoning about the communicative nature of the sentence they hear [@Grice1975; @Clark1996; @Levinson2000].
That is, a generic sentence is produced by a speaker who has the goal of conveying information to them. 
The rightmost column of Figure\ \@ref(fig:modelSimulations)B shows the predictions of the pragmatic model (Eq. \ref{eq:L1}).
The primary difference between a literal and a pragmatic generic interpretation model is that the pragmatic model is able to derive stronger interpretations of a generic sentence. 
When learning about an abstract property with a uniform prevalence prior (top-row), the pragmatic model infers more strongly that most of the category have the property. 
For the other prevalence priors, the pragmatic model predicts lower-variance interpretations.
In Experiment 3, we conduct a generic interpretation study with seventy-five items designed to elicit a wide range of variability in interpretations; we measure the prevalence priors in a nonparametric way that allows to quantitatively distinguish the literal from the pragmatic generic interpretation model.

<!-- which can be thought of as levels of prevalence given different categories within a comparison class. -->
<!-- For simplicity, we assume the comparison class relevant for interpreting generics about novel animal categories is *other animals*. -->

<!-- \ndg{i think we should have a little (sub)section here on the structure of the prevalence priors. since one of the main things that changes between the three experiments is the prior elicitation / manipulation, we want to set the stage. so we could talk a bit about how different causal mechanisms would give rise to different mixture components and how a beta-mixture model should be a good approximation to that. then maybe set the stage saying in the first expt we assume this and design elicitation accordingly, in the second expt we measure the prevalence prior in a more direct (non parametric) way and show that a beta-mixture fits reasonably, in the third experiment we directly manipulate the prior.} -->

\ndg{note: i changed "context" to other things (eg "property") in a bunch of places because what we're talking about is differences in target property and it's corresponding background knowledge. this is context in some sense, but i worry it would confuse people with more pragmatic / situated notions of context.}
\mht{wonder if we should make those changes more widespread (e.g., the running-head) or have an explicit statement about the kind of context we are considering here...}

## Alternative semantic models

Both the literal and the pragmatic generic interpretation models treat the literal meaning of a generic as equivalent to observing a single positive example from the category.
We compare these to alternative models that update beliefs according to a different literal meaning for the generic.
This comparison allows us to better understand how necessary the literal meaning is for the model predictions. 
In order to permit this comparison, we endow the alternative models with the same background knowledge in the form of the prevalence prior distribution $P(r)$. 
The alternative models take the literal meaning of a generic to be a fixed-threshold function, corresponding to models of quantifier semantics.^[
  Note that a full model of quantifier interpretation would include their pragmatic interpretations (e.g., that "some" often implies "not all"). Such a model can be implemented in the same probabilistic modeling framework [@Goodman2013] but does not directly address our question of whether a fixed-threshold semantics is tenable for generics.
]
The first is a model of the quantifier "some" which rules out the lowest possible prevalence level:

\begin{eqnarray}
L_0^{some}(r \mid u) &\propto&  P(r \mid r > 0)  \label{eq:someModel}
\end{eqnarray}

The second is analogous to "most", with a threshold at 50% prevalence:

\begin{eqnarray}
L_0^{most}(r \mid u) &\propto&  P(r \mid r > 0.5)  \label{eq:mostModel}
\end{eqnarray}

A model that assumes a fixed threshold at 50% is a very restricted model; it places zero probability on any response less than 50%. 
In case there are ratings in the experimental data sets that are less than 50%, we must supplement the "most" model with an extrinsic noise process in order for this model to actually generate predictions for those data; that is, with some probability, participants respond at random.
We infer this noise probability parameter for the "most" model from the data.
Finally, we will compare the generic interpretation models to a model that does not update beliefs and predicts prevalence ratings according to the prior distribution $P(r)$. 
This comparison informs us as to whether or not participants actually *update* their beliefs based on the generic statement.


<!-- \ndg{see comments below that we should have brief discussions of joint BDA analysis strategy and of alternative models up here, to avoid duplication and make the expt sections smoother.} -->

<!-- Considering different fixed-thresholds in conjunction with structured background knowledge also illuminates the unique behavior of the uncertain threshold model. -->

<!-- The literal interpretation of the quantifier "some" (threshold = 0.01) rules out only the lowest prevalence levels, returning an intuitively compelling interpretation for "Xs carry malaria" (*very few Xs carry malaria*); however, even given strong prior knowledge about the distribution of the property *fly*, "Some Xs fly" leaves open the possibility that very few Xs fly -->

<!-- A threshold that is slightly higher (threshold = 0.33; "more than a third"), when coupled with strong prior knowledge (e.g., for "Xs fly") can give  -->
<!-- The quantifier "most" would correspond to a threshold at 0.5, which like "more than a third", can result in intuitive interpretations for "Xs fly" (the posterior over prevalence suggests *almost all Xs fly*); -->

<!-- Different prevalence priors give rise to different interpretations. -->
<!-- Figure\ \@ref(fig:modelSimulations)B shows different interpretations (posterior distributions over prevalence) given different fixed thresholds (corresponding to different quantifiers) as well as the uncertain threshold, corresponding to the generic. -->

<!-- The interpretation model computes a posterior distribution on prevalence by considering different possible thresholds $\theta$. -->
<!-- Figure\ \@ref(fig:modelSimulations)B (top-row) shows what the model would believe given different thresholds (columns). -->
<!-- If the threshold were to be very high (e.g., $\theta = 0.75$), only the highest prevalence levels would be consistent with the utterance. -->
<!-- As the threshold decreases, more and more prevalence levels would be true. -->
<!-- Roughly speaking, the uncertain-threshold interpretation model averages over these possibilities, resulting in a posterior distribution that favors higher prevalence levels because they are consistent with more thresholds. -->
<!-- The endorsement model inverts the interpretation model and predicts higher rates of endorsement as the speaker's belief about the prevalence (referent-prevalence) increases. -->


```{r modelSimulations, fig.width = 11, fig.asp = 0.6, fig.cap="Model simulations. A: Prevalence priors reflect knowledge about properties. An abstract property could be given a uniform distribution, while familiar properties display more complex structure. B: Posterior distributions over prevalence (interpretations) given different fixed-threshold quantifiers and the uncertain threshold generic."}
load(file = "cached_results/modelSims-priors_fixedT_uncertainT_L1.RData")
get.colors <- function(pal) brewer.pal(brewer.pal.info[pal, "maxcolors"], pal)
spectrum.color.palette <- get.colors("Blues")


fig.sims.priors <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare", "sexed_rare","accidental_rare"),
           src %in% c("priors")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare",  
                                                           "sexed_rare","accidental_rare"),
                                    labels = c("Xs Y (uniform)", "Xs fly (biological)", 
                                               "Xs lay eggs (sexed)", 
                                               "Xs carry malaria (accidental)")),
                src = factor(src, levels = c( "priors"),
                             labels = c(
                               '\n prevalence prior'
                                        ))), 
       aes(x = state))+
    geom_density(aes(y = ..scaled..), fill= 'black', 
                 size = 0.6, alpha = 0.7, adjust = 1.2)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Scaled probability density") +
    xlab("Prior Prevalence")+
    scale_color_solarized()+
    scale_fill_solarized()+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_blank(),
          legend.position = "none"
          )


fig.sims.distributions <- ggplot(sims.combined %>%
         filter(
           PriorShape %in% c("uniform", "biological_rare","sexed_rare", "accidental_rare"),
           # src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "literal", "pragmaticUnlifted"),
           src %in% c("fixed_0.1", "fixed_0.33", "fixed_0.5", "literal")
           ) %>%
         mutate(PriorShape = factor(PriorShape, levels = c("uniform", "biological_rare", "sexed_rare", "accidental_rare"),
                                    labels = c("Xs Y\n[uniform]", "Xs fly\n[biological]",
                                               "Xs lay eggs\n[sexed]", "Xs carry malaria \n [accidental]")),
                src = factor(src, levels = c( "fixed_0.1", "fixed_0.33",
                                              "fixed_0.5", 
                                              "literal"#,
                                              #"pragmaticUnlifted"
                                              ),
                             labels = c(
                           #    'prevalence prior',
                               '"some"\n(threshold = 0.01)',
                               '"more than a third"\n(threshold = 0.33)',
                              '"most"\n(threshold = 0.5)',
                               'literal generic\n(single positive example)'#,
                              #'pragmatic generic'
                              ))), 
       aes(x = state, fill = src, color = src))+
    geom_density(aes(y = ..scaled..), 
                 color ='black', size = 0.6, alpha = 0.7, adjust = 1.2)+
    theme_few() +
    scale_x_continuous(breaks = c(0, 1), limits= c(0, 1))+
    scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
    ylab("Scaled posterior density") +
    xlab("Implied Prevalence")+
    # scale_fill_brewer(palette=2, type = "seq", direction = -1)+
    # scale_color_brewer(palette=2, type = "seq", direction = -1)+
    scale_color_manual( values =  c(spectrum.color.palette[c(3, 6, 9)],
                                    #"#99d8c9", 
                                    "#2ca25f"))+
    scale_fill_manual( values = c(spectrum.color.palette[c(3, 6, 9)],
                                  #"#99d8c9", 
                                  "#2ca25f") )+
    facet_grid(PriorShape~src, scales = 'free')+
    theme(strip.text.y = element_text(angle = 0, size = 12),
          legend.position = "none",
          axis.title.y = element_blank())

# sims.combined.summary <- sims.combined %>%
#          filter(
#            PriorShape %in% c("uniform","biological_rare", "accidental_rare"),
#            src %in% c("fixed_0.1", "fixed_0.5", "literal", "pragmaticUnlifted")
#            ) %>%
#          mutate(PriorShape = factor(PriorShape, 
#                                     levels = c( "accidental_rare", "biological_rare","sexed_rare", "uniform"),
#                                     labels = c("uniform", "Xs carry malaria\n[accidental]", 
#                                                "Xs lay eggs\n[sexed]",
#                                                "Xs fly\n[biological]")),
#                 src = factor(src, 
#                              levels = c( 
#                                "fixed_0.1", "fixed_0.5", "literal", "pragmaticUnlifted"),
#                              labels = c(
#                                '"some" (threshold = 0.01)',
#                               '"most" (threshold = 0.5)',
#                                'literal generic',
#                               'pragmatic generic'
#                                         ))) %>%
#     group_by(PriorShape, src) %>%
#     summarize(expval = mean(state))
#   
#   
# fig.sims.bars <- ggplot(sims.combined.summary %>% mutate(blank = '\n'), 
#        aes(x = PriorShape, y = expval, fill = src))+
#     geom_col(color = 'black', 
#              position = position_dodge(), alpha = 0.8, width = 0.5)+
#     theme_few() +
#     scale_y_continuous(breaks = c(0, 1), limits= c(0, 1))+
#     ylab("Average\nimplied prevalence")+
#     scale_color_manual( values = spectrum.color.palette )+
#     scale_fill_manual( values = spectrum.color.palette )+
#     #scale_color_solarized()+
#     #scale_fill_solarized()+
#     facet_wrap(~blank)+
#     coord_flip()+
#     guides(fill = F)+
#     theme(axis.title.y = element_blank(),
#           axis.text.y = element_blank())

cowplot::plot_grid(
  fig.sims.priors + theme(plot.margin = unit(c(6, 3, 6, 6), "pt")), 
  fig.sims.distributions + theme(plot.margin = unit(c(6, 0, 6, 0), "pt")),
  #fig.sims.bars+ theme(plot.margin = unit(c(6, 0, 0, 0), "pt")),
  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(0.25, 1)
)

#ggsave("figs/modelSimulations-noPrag.pdf", width = 10, height = 6)

```

<!-- 
- Figure:
  - 3 schematic priors, different fixed thresholds, generic interpretation 
  - possibly with bar plot summarizing means
  
- Figure:
  - comparison to a fixed threshold ("some") model, fixed threshold ("most") model, and L1 model (?)
-->

# Overview Of Experiments

Our models of generic interpretation predict that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
@Cimpian2010 found a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
We begin our experiments by first replicating this result; this study serves to validate our measurement, which we will use in our main two experiments.
Experiment 1 is a larger scale version of @Cimpian2010's *implied prevalence* task, with sufficien measurements on the by-item level to reveal graded, quantitative variability between generics; additionally, we measure participants' prior beliefs and compare our generic interpretation models and control models. 
Experiment 2 is a test of the causal influence of prevalence priors on generic interpretation, in which we manipulate participants' subjective prevalence priors in 10 different between-participants conditions. 
The sample size, exclusion criteria, and planned statistical contrasts for each study were pre-registered on the Open Science Framework. 
All cognitive and Bayesian data analytic models were implemented in the probabilistic programming language WebPPL [@dippl].
Link to experiments, data, analysis scripts, and models can be found at \url{www.github.com/mhtess/generic-interpretation}. 

# Preliminary Study: Replication and Extension of Cimpian et al. (2010)

<!-- Here, we empirically measure the prevalence priors for these properties using a structured prior elicitation task (Expt. 1a) and use our interpretation model to predict the prevalence implied by a generic statement about a novel category (e.g., "Wugs have yellow fur"; Expt. 1b).  -->

<!-- %We also coded the accidental properties from Expt.~2a as either ``common'' or ``rare'' using a by-item median split based on \emph{a priori} expected prevalence when present. -->
<!-- %Most of the materials we used were from \citeauthor{Cimpian2010}.  -->
<!-- %The materials used were 30 novel animal categories (e.g. lorches, morseths, blins) each paired with a unique property.  -->
<!-- %Biological properties were made by pairing a color with a body-part (e.g. purple feathers, orange tails).  -->
<!-- %Accidental properties used the same set of body-parts but modified it with an adjective describing an accidental or disease state (e.g. broken legs, wet fur).  -->
<!-- %Each participant saw a random subset of 10 unique animal-property pairs for each type of property (biological and accidental).  -->

<!-- ## Experiment 1b: Generic interpretation -->

<!-- This experiment measures the prevalence implied by a generic statement about a novel category, which we then compare to the predictions of the generic interpretation model $L(p \mid u)$ (Eq. \ref{eq:L0}). -->

<!-- %The full cover story is described in {\it SI Section C} and is the same for Expt.~2c. -->

## Method

### Participants

We recruited 40 participants over Amazon's crowd-sourcing platform Mechanical Turk. 
The experimental design is very similar to @Cimpian2010, and we chose to have a sample size at least twice as large as the original study (original n=15). 
<!-- This is a quantitative experiment with only quantitative comparisons planned. -->
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

### Procedure and materials

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with a bare plural about a novel category (e.g., "Feps have yellow fur") and asked to judge prevalence: "What percentage of feps do you think have yellow fur?" 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental.
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 

## Results and discussion

Interpreting a novel generic sentence "Ks have F" often has the possibility of being understood as a universal or near-universal claim (*all or almost all Ks Fs)*. 
Across our the forty items in this experiment, however, we observe a gradient in the implied prevalence ratings, mostly clustered by the type of property (Figure\ \@ref(fig:cimpian-modelingResults)A)
<!-- We look at the posterior predictive distribution of the generic interpretation model $L$. -->
<!-- Figure\ \@ref(fig:cimpian-modelingResults) shows the prevalence implied by the forty novel generic sentences in the experiment.  -->
At the level of property type, the results are consistent with @Cimpian2010's findings of generally strong interpretations of novel generic statements, but that also exhibit a sensitivity to theory-based expectations such that generics about accidental properties (e.g., *fungus-covered claws*) are interpreted relatively weakly (Figure\ \@ref(fig:cimpian-modelingResults)B).
<!-- We extended these findings with a broader stimulus set and discovered even more gradability in interpretations of generic statements.  -->
<!-- We empirically measured the prevalence priors and used our generic interpretation model to predict the prevalence implied by a generic statement, with a high degree of quantitative accuracy. -->

Broadly, these results validate the measurement as one that can elicit variability, and it is this variability that is of primary concern to us.
The items that elicited the most variability in implied prevalence ratings were those of accidental or diseased states (e.g., "Lorches have broken legs", "Wugs have fungus-covered claws"). 
As @Cimpian2010 noted, "properties of this type do not lend themselves very well to generic predication (Cimpian & Markman, 2008; Gelman, 1988), so generics about broken legs, itchy skin, etc. are infrequent outside the laboratory." (p.1472)
Not only are these kinds of statements infrequent outside the laboratory, bare plurals about accidental or diseased states ("Lorches have broken legs") can easily be interpreted as non-generic, existential claims about the here-and-now, analagous to how "Dogs are on my front lawn" describes a particular state of affairs as opposed to something generalizable about dogs. 
Since the strongest test of our models of generic interpretation will come from having to predict variability in implied prevalence ratings, we aim to elicit high variability in Experiment 1 using naturalistic properties that more easily lend themselves to generic predication (i.e., you could imagine a speaker saying the sentence).


<!-- It thus remains a possibility that the low-prevalence interpretations of the bare plurals about accidental properties is attributable to noise  -->
<!-- Thus, in Experiment 1,  -->

```{r cimpian-modelingResults, fig.width = 11, fig.asp = 0.57, fig.cap="Implied prevalence data for items from Cimpian et al. (2010). A: Implied prevalence ratings for forty body-part stimuli (\"Ks have F\"). Vertical line denotes mean, horizontal lines denotes bootstrapped 95% confidence intervals, points are individual responses. B: Implied prevalence collapsed across property type."}
load(file = "cached_results/cimpian-modelFits-interpretations.RData")
load(file = "cached_results/cimpian-interpretations-95ci.RData")

load(file = "cached_results/cimpian-prevPriorParam_dists.RData")
# md.prevPriorParams

df.c.int <- read_csv("../data/asymmetry/implied-prevalence-2-trials.csv") %>%
    mutate(stim_property = factor(stim_property, 
                           levels =  with(df.c.int.bs, stim_property[order(mean)])),
         stim_type = ifelse(stim_type == "disease", "accidental", stim_type),
         stim_type= factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "gradable adj + part",
                                                    "color adj + part",
                                                    "accidental")))


fig.cimpian.interpretation.95ci <- df.c.int.bs %>%
  ungroup() %>%
  mutate(stim_property = factor(stim_property, 
                           levels =  with(df.c.int.bs, stim_property[order(mean)])),
         stim_type = ifelse(stim_type == "disease", "accidental", stim_type),
         stim_type= factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "gradable adj + part",
                                                    "color adj + part",
                                                    "accidental"))) %>%
  ggplot(., aes( x = stim_property, y = mean, ymin = ci_lower, ymax = ci_upper, 
                 fill = stim_type, color = stim_type))+
  geom_point(shape = 73, size = 4)+
  geom_linerange(size = 1.1)+
  geom_point(data = df.c.int, position = position_jitter(),
             inherit.aes = F, aes( x = stim_property, y = response, color = stim_type),
             alpha = 0.3)+
  #geom_pirate(bars = F, violins = F, width_points = 0.2)+
  #coord_flip()+
  #scale_color_viridis(discrete = T)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  #theme(axis.title.x = element_blank())+
  coord_flip()+
  ylab("Implied prevalence")+
  xlab("")

# md.impprev.plotting <- md.impprev %>% 
#   filter((Parameter == "posterior") | (semantics == "uncertain")) %>%
#   mutate(src = paste(Parameter, "_", semantics, sep =""),
#          src = factor(src, levels = c("prior_uncertain", "posterior_some",
#                                       "posterior_most", "posterior_uncertain"),
#                       labels = c("Prevalence prior mean",
#                                  '"some"\n(threshold = 0.01)',
#                                  '"most" + noise\n(threshold = 0.5)',
#                                  'generic\n(uncertain threshold)')),
#          "Property type" = factor(stim_type,
#                                          levels = c("part",
#                                                     "vague",
#                                                     "color",
#                                                     "accidental"),
#                                          labels = c("body part",
#                                                     "color adj + part",
#                                                     "gradable adj + part",
#                                                     "accidental")))
# 
# 
# 
# fig.cimpian.interpretation.models <- ggplot(md.impprev.plotting, 
#        aes( x = model_MAP, xmin = model_lower, xmax = model_upper, 
#                         y = mean, ymin = ci_lower, ymax = ci_upper,
#                  fill = `Property type`))+
#   geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
#   geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
#   geom_point(shape = 21, size = 2.6, color = 'black')+
#   facet_wrap(~src, nrow = 1)+
#   scale_x_continuous(limits = c(0,1.01), breaks = c(0, 1)) +
#   scale_y_continuous(limits = c(0,1.01), breaks = c(0, 1)) +
#   coord_fixed()+
#   xlab("Model prediction")+
#   ylab("Implied prevalence")+
#   theme(legend.position = 'right')


fig.cimpian.interpretation.stimtype <- md.impprev.type %>%
  filter(src == "data") %>%
  mutate(stim_type = factor(stim_type,
                                         levels = c("part",
                                                    "vague",
                                                    "color",
                                                    "accidental"),
                                         labels = c("body part",
                                                    "gradable adj + part",
                                                    "color adj + part",
                                                    "accidental")),
         src  = factor(src, levels = c("data","model"),
                       labels = c("data", "generics model"))) %>%
  ggplot(., aes( x = stim_type, y = mean, 
                 ymin = ci_lower, ymax = ci_upper,
                 fill = stim_type))+
  geom_col(position = position_dodge(0.7), width = 0.7, color = 'black')+
  #geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_linerange(position = position_dodge(0.7), width = 0.3)+
  #geom_point(shape = 21, size = 2.6, color = 'black')+
  #facet_wrap(~src, nrow = 1)+
  #scale_x_continuous(limits = c(0,1.01), breaks = c(0, 0.5, 1)) +
  scale_y_continuous(limits = c(0,1.01), breaks = c(0,0.5, 1)) +
  guides(fill = guide_legend(title = "Property type"))+
  #coord_fixed()+
  #xlab("Model prediction")+
  ylab("Implied prevalence")+
  coord_fixed(ratio = 2)+
  theme(axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        legend.position = 'bottom',
        legend.direction = 'vertical')

cowplot::plot_grid(
  fig.cimpian.interpretation.95ci + theme(
    plot.margin = unit(c(30, 6, 0, 6), "pt"),
                         legend.position = 'none'),
  fig.cimpian.interpretation.stimtype + theme(plot.margin = unit(c(20, 6, 12, 6), "pt")),
  rel_widths = c(1, 0.5),
  labels = c("A", "B")
)
ggsave("figs/cimpian-results.pdf", width = 10, height = 6.5)
  # cowplot::plot_grid(
  #   fig.cimpian.interpretation.stimtype + theme(plot.margin = unit(c(20, 6, 12, 6), "pt"),
  #                          legend.position = 'none'),
  #   fig.cimpian.interpretation.models + theme(plot.margin = unit(c(0, 6, 0, 6), "pt"),
  #                          legend.position = 'none'),
  #   nrow = 1,
  #   labels = c("B", "C"),
  #   rel_widths = c(1, 2.5)
  # ),
  # cowplot::get_legend(
  #   fig.cimpian.interpretation.stimtype + theme(
  #     legend.position="bottom"
  #     #legend.text = element_text(size = 16),
  #     #legend.title = element_text(size = 16)
  #                          )
  #   ), ncol = 1, rel_heights = c(1.2, 1.2, 0.2),
  # labels = c("A", "", "")

```




<!-- \mht{does this add anything? do we want to speculate a little more about where these differences in parameters might come from?} -->
<!-- The structured elicitation of prevalence knowledge in Expt. 1a provides a more detailed picture of how theory-based expectations manifest in knowledge of these properties.  -->
<!-- Properties differ in both the *a priori* probability of being present in a category as well as how widespread the property is expected to be, given that it is present in the category (Figure\ \@ref(fig:cimpian-prevPrior)). -->
<!-- These differences are the mediating statistical knowledge that gives rise to different implied prevalence ratings for generics about different properties. -->



# Experiment 1

This experiment is designed to measure the prevalence implied by a generic about a diverse set of properties (Expt. 1b). 
In addition, we elicit participants' beliefs about the likely prevalence levels expected for these different properties (Expt. 1a). 
We again use our computational models and alternative models, all using the elicited prevalence priors, to make predictions about the prevalence implied by a generic.

## Experiment 1a: Prevalence prior elicitation

In this experiment, we elicit prevalence knowledge about a diverse set of familiar properties.
Because the properties are relatively familiar, we elicit participants' knowledge by asking about the prevalence of the property among familiar categories. 
<!-- This allows for a non-parametric examination of the structure of the priors. -->

```{r load expt 2a meta data}
load(file = "cached_results/genint_priors_metaData.RData")
# expt2.prior.n,
#      expt2.prior.nonEnglish,
#      expt2.prior.catchFail,
#      expt2.prior.n.passCatch.English,
#      df.prior.3.filtered.meanTime, 
#     df.prior.3.filtered.n_subj_per_item
```


### Methods

#### Participants

We recruited `r expt2.prior.n` participants from MTurk.
This number was arrived at with the intention of getting approximately 23 independent sets of ratings for each unique item in the experiment.
Participants were restricted to those with U.S. IP addresses and with at least a 95\% MTurk work approval rating (these same criteria apply to all experiments reported).
The experiment took on average `r round(df.prior.3.filtered.meanTime$mean_time, 1)` minutes and participants were compensated \$0.80.

#### Materials

We created a stimulus set composed of seventy-five properties.
Items were generated by the first author by considering eight different classes of properties: physical characteristics (e.g., *have brown spots*, *have four legs*), psychological characteristics (e.g., *experience emotions*), dietary habits (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), aggressive behaviors (e.g., *pound their chests to display dominance*, *hunt other animals*), and other miscellaneous behaviors (e.g., *perform in the circus*, *sing beautiful songs*); online sources about strange animal behaviors were consulted in order to find the more obscure properties. Full list of materials can be found in Table 4 in the Appendix.

#### Procedure and materials

On the first trial, participants were asked to list three kinds of animals for each of five different classes of animals: mammals, fish, birds, insects/bugs, amphibeans/reptiles. 
The five classes of animals were presented in a randomized order on the screen and there were three text boxes for each in which participants could type an animal kind.
On subsequent trials, participants were shown a random subset of five animal kinds and asked what percentage each of these categories they believed had a property (e.g., "Out of all of the cheetas in the world, what percentage do you think attack hikers?").
Pilot results indicated similar responses were generated by a question about frequency (e.g., "Out of 100 cheetahs, how many do you think attack hikers?").
Participants responded using a slider bar with endpoints labeled 0% and 100%, and the exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant saw a random selection of twelve properties.

<!-- \ndg{say something about why elicitation paradigm is different for this expt than last one? partly to give a non-parametric look at priors?} -->
<!-- \mht{i did this in the introductory remarks about the expt.} -->

As an attention check, at the end of the prior elicitation trials, participants were asked to select, from a list of ten, all of the properties they could remember being tested on. 
The list included five properties that they had been tested on and five distractors. 
The experimental paradigm can be seen here: http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/prior-3.html.

### Results

We used the same exclusion criteria that were preregistered for the subsequent generic interpretation study (Expt. 2b): https://osf.io/bwn4t/register/5771ca429ad5a1020de2872e. 
Participants who did not have at least 4 out of 5 hits and at least 4 out of 5 correct rejections during the memory trial were excluded ($n = `r expt2.prior.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt2.prior.nonEnglish`$). 
This left a total of $n = `r expt2.prior.n.passCatch.English`$ participants, with items receiving on average `r round(df.prior.3.filtered.n_subj_per_item$n_subj_per_item)` (range = [`r round(df.prior.3.filtered.n_subj_per_item$lb)`, `r round(df.prior.3.filtered.n_subj_per_item$ub)`]) unique participants responses (each participant provides five ratings).

A response in this task can be thought of as a sample from a property's prevalence prior distribution.
Thus, the distribution of responses for an item are an estimate of the prevalence prior distribution.
Figure\ \@ref(fig:genInt-prevPrior)A shows eight example items' distributions of responses.
For example, the property *have four legs* is most likely either completely present (prevalence = 100%) or completely absent (prevalence = 0%) from categories.
*Eat insects* looks similar, though there is considerable probability mass spread among the non-binary alternatives ($0\% <$ prevalence $< 100\%$). 
*Get in fights with other animals* is similar but has substantially less probability mass at 100% prevalence: It is unlikely that this property is widespread in a category. 
*Live in urban areas* shows a monotonically decreasing probability function; the higher the prevalence, the less likely it is. 
*Live in zoos* is expected to be even less prevalent, and the property *has seizures* is expected to not be widespread at all. 
The property *get erections* is only expected to be present in 50% of the population (presumably, the males) when it is present at all.

Most of the elicited prevalence prior distributions are at least bi-modal. 
To visualize all properties simultaneously, we represent each distribution by its relative probability mass at 0%---$P(\text{feature is present})$, or $P(r > 0)$---and the expected value (mean) of the distribution conditional on the prevalence being greater than 0\%---$\mathbb{E}[P(r \mid r>0)]$ or the its *prevalence when present* (Figure\ \@ref(fig:genInt-prevPrior)B).
Our stimulus set covers a wide range of possible values of both of these parameters.
<!-- ; the set of parameters that summarize these priors is also largely non-overlapping with those of Expt. 1 (compare with Figure\ \@ref(fig:cimpian-prevPrior)A). -->
This suggests we have sampled items with priors that exhibit a lot of quantitative variability.
Given these priors, our model makes quantitative predictions about the prevalence implied by a novel generic sentence (e.g., "Lorches live in zoos").

```{r genInt-prevPrior, fig.width = 12, fig.asp = 0.35, fig.cap="Prevalence priors for a broad set of animal properties.  A: Ten example prevalence priors elicited in Expt. 2a. Different prevalence priors give rise to different model predicted implied prevalence. B: Prevalence priors summarized by their relative probability mass at zero-prevalence P(feature is present), and their expected value among non-zero prevalence levels: Prevalence when present. Error bars denote bootstrapped 95% confidence intervals. "}
#load(file = "cached_results/genInt_priors.RData")
# df.prior.prevalence.2.isZero.conditionalEv,
#      df.prior.prevalence.2,

load(file = "cached_results/genInt_priors3.RData")

# df.prior.prevalence.3.isZero.conditionalEv,
#      df.prior.prevalence.3,
#      df.prior.3.filtered.bs,

fig.prevParams.genInt <- ggplot(df.prior.prevalence.3.isZero.conditionalEv, 
       aes( x = 1-isZero, xmin = 1-zero_low, xmax = 1-zero_high,
            y = pwp_mean, ymin = pwp_low, ymax = pwp_high))+
  geom_errorbar(alpha = 0.3) + geom_errorbarh(alpha = 0.3) + 
  geom_point(shape = 21, size = 2.6, color = 'black', fill = 'black')+
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  # ggrepel::geom_label_repel(data = df.prior.prevalence.3.isZero.conditionalEv %>% 
  #                             filter(property %in% c("have four legs", 
  #                      "eat insects", 
  #                      #"feed on the carcasses of dead animals",
  #                      "get in fights with other animals",
  #                      "know when earthquakes are about to happen",
  #                      "live in zoos",
  #                      #"drink alcohol left behind by tourists",
  #                      #"chase their tails",
  #                      "have seizures")), 
  #                           aes(label = property))+
  ylab("Prevalence when present")+
  xlab("P(feature is present)")+
  coord_fixed()

target.properties <- c("have four legs", 
                       "eat insects", 
                       "eat garbage",
                       #"feed on the carcasses of dead animals",
                       "are afraid of loud noises",
                       "get in fights with other animals",
                       "live in urban areas",
                       "know when earthquakes are about to happen",
                       "get erections",
                       "live in zoos",
                       #"drink alcohol left behind by tourists",
                       #"chase their tails",
                       "have seizures")

var_width <- 22

fig.prevPriors.genInt <- df.prior.3.filtered.bs %>% ungroup() %>%
  filter(property %in% target.properties) %>%
  mutate(property = factor(property, levels = target.properties,
                           labels = stringr::str_wrap(target.properties, 
                                                      width = var_width))) %>%
  mutate(state = as.numeric(state)) %>%
  group_by(property, state) %>%
  summarize( lower = quantile(prop, 0.025),
             mean = mean(prop),
             upper = quantile(prop, 0.975)) %>%
  ggplot(., aes( x = state, y = mean, ymin = lower, ymax = upper))+
  geom_col(position= position_dodge(), color = 'black', fill= 'white')+
  geom_errorbar(position = position_dodge(), width= 0.036, alpha = 0.4 )+
  facet_wrap(~property, nrow = 2) +
  scale_x_continuous(limits = c(-0.1, 1.1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(-0, 0.8), breaks = c(0, 0.25, 0.5, 0.75))+
  ylab("Proportion of responses")+
  xlab("Prevalence")

cowplot::plot_grid(
  fig.prevPriors.genInt + theme(plot.margin = unit(c(6, 6, 6, 0), "pt"),
                         legend.position = 'none'), 
  fig.prevParams.genInt + theme(plot.margin = unit(c(18, 6, 6, 6), "pt"),
                         legend.position = 'none'),

  nrow = 1,
  labels = c("A", "B"),
  rel_widths = c(2.5, 1)
)

```

## Experiment 1b: Generic interpretation

```{r load expt2b meta data}
load(file = "cached_results/genint_int_metaData.RData")

# expt2.int.n,
#      expt2.int.nonEnglish,
#      expt2.int.catchFail,
#      expt2.int.n.passCatch.English,
#      df.int.6.meanTime, 
#      df.int.6.filtered.n_subj_per_item,
#      df.int.56.filtered.means.r, df.int.56.filtered.means.n,
#   df.int.56.filtered.means.spearman,

```

### Methods

#### Participants

We recruited `r expt2.int.n` participants from MTurk. 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average `r round(df.int.6.meanTime$mean_time,1)` minutes and participants were compensated \$0.80.

#### Procedure and materials

The materials were the same as in Expt. 1a.
Participants were told that scientists had recently discovered lots of new animals that we did not know existed.
On each trial, they would be told facts about the new animals and be asked to translate it into the percentage of that animal it applies to.
On each trial, participants read "You are told: *generic sentence*", where the generic sentence was a bare plural statement about a familiar property $F$ applying to a novel animal category $K$ (e.g., "Javs attack hikers").
They were then asked "Out of all of the *K*s on the planet, what percentage do you think *F*?" (e.g., "Out of all of the javs on the planet, what percentage do you think attack hikers?").
Novel animal category names were mostly taken from @Cimpian2010 and similar studies on generic language. 
Participants responded using a slider bar with endpoints labeled 0% and 100%, and the exact number corresponding to their slider bar rating was displayed once participants clicked on the slider bar.
Each participant completed thirty-five trials, corresponding to a random subset of the full stimulus set.

After the generic interpretation trials, participants completed the same memory check trial as was done in the prior elicitation. 
They were shown ten properties and asked to click on those they had seen in the experiment.
Following the memory check trials, participants completed up to five explanation trials, depending on their ratings in the task.
On an explanation trial, participants saw a rating they had given for a property they had rated as applying to less than 50% and asked if they could explain why they gave the response that they gave. 
Participants also had the option of changing their response after providing an explanation. 
These data were only used in exploratory analyses and not for the main analyses reported below.
If participants gave no ratings less than 50%, they did not complete any explanation trials. 

The experimental paradigm can be viewed at http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/interpretations-6.html


```{r load interpretation results, cache=F}
load(file = "cached_results/genInt_interpretation6_prior3_modelData2.RData")
#df.int.filtered, df.int.filtered.bs, md.impprev
load(file = "cached_results/genInt_interpretation6_prior3_mostNoise.RData")

load(file = "cached_results/genint_L1_params.RData") # m.genint.params.l1
load(file = "cached_results/genint_BF_results.RData") #rs.mll.formatted
format_empirical_ci <- function(
  df, estimate = "mean", low = "ci_lower",
  high = "ci_upper", sigfigs = 2){
  return(paste(
    round(df[[1,estimate]], sigfigs),
    " [",
    round(df[[1,low]], sigfigs),
    ", ",
    round(df[[1,high]], sigfigs),
    "]", sep = ""))
}

md.impprev.stats <- md.impprev %>%
  group_by(parameter, semantics, src) %>%
  summarize( mse  = mean((model_MAP-mean)^2),
             r2 = cor(model_MAP, mean)^2,
             n = n())
```


```{r}
# md.prevpriors.binned.summarized
load("cached_results/genInt_interpretation6_prior3_modelData_priorsSummarized.RData")

md.prevprior.stats <- md.prevpriors.binned.summarized %>%
  ungroup() %>%
  summarize(mse = mean((model - data)^2),
            r2 = cor(model, data)^2,
            n = n())
```

### Descriptive results

We used preregistered exclusion criteria, which were also used for Expt. 1a. 
Participants who did not have at least 4 out of 5 hits and at least 4 out of 5 correct rejections during the memory trial were excluded ($n = `r expt2.int.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt2.int.nonEnglish`$). 
This left a total of $n = `r expt2.int.n.passCatch.English`$ participants, with items receiving on average `r round(df.int.6.filtered.n_subj_per_item$n_subj_per_item)` responses (range = [`r round(df.int.6.filtered.n_subj_per_item$lb)`, `r round(df.int.6.filtered.n_subj_per_item$ub)`]).

As in the replication of @Cimpian2010, we observe a clear gradient in the implied prevalence ratings across our seventy-five items (Figure\ \@ref(fig:genint-empiricalData)).
On one end of the continuum, there is a generic like "Wugs have four legs", which is interpreted as a universal---applying to exactly 100% of wugs---by `r sum(filter(df.int.filtered, property == "have four legs")$response == 1)` out of `r length(filter(df.int.filtered, property == "have four legs")$response)` participants and which received a mean implied prevalence rating of $`r format_empirical_ci(filter(df.int.filtered.bs, property == "have four legs"))`$. 
^[
  Note the novel category term is randomized for each participant and property. 
  We use particular novel category terms in the text for ease of exposition.
]
On the other end is "Glippets perform in the circus", which is interpreted as applying to less than 25% of glippets by `r sum(filter(df.int.filtered, property == "perform in the circus")$response < 0.25)` out of `r length(filter(df.int.filtered, property == "perform in the circus")$response)` participants and which receives a mean rating below 50% ($`r format_empirical_ci(filter(df.int.filtered.bs, property == "perform in the circus"))`$).
<!-- Several items have mean ratings below 50%.  -->
Additionally remarkable is the distribution of responses for individual items: Though "Feps live in zoos" probably means around 25% of feps live in zoos, it is still quite possible that almost 100% live in zoos (e.g., in the real world, 100% of Micronesian Kingfishers live in zoos).

To assess the reliability of these data, we ran a replication ($n=140$) using a slightly different dependent measure.^[
  By experimenter error, only seventy-four of the seventy-five items were collected in the replication data set.
]
Instead of being asked a question about percentages (e.g., "Out of all of the Ks in the world, what percentage F?"), participants were asked a question in terms of frequency: "Out of 100 Ks, how many do you think F?".
The empirical by-item means between these data and the original data are highly correlated ($r(`r df.int.56.filtered.means.n`) = `r round(df.int.56.filtered.means.r, 2)`$, $r_{spearman}(`r df.int.56.filtered.means.n`)= `r round(df.int.56.filtered.means.spearman, 2)`$), indicating very high data reliability.

### Model-based analyses

Our model-based analyses ask how well the various semantics models accommodate the human generic interpretation data and which model provides the most parsimonous view of the data. 
We estimate various model parameters using Bayesian methods in order to determine the most likely values of the latent parameters given our hypotheses about how the both the prior elicitation and generic interpretation data were generated. ^[
  Appendix C in @Tessler2019psychrev provides a careful analysis of the behavior of this joint Bayesian inference strategy for jointly modeling the prevalence prior parameters together with other parameters of the model governing generic interpretation. They find imperceptibly small differences between the parameters inferred from the prevalence prior data in isolation and those inferred by jointly modeling the prior elicitation together and the generic interpretation data, suggesting that the prevalence priors are not significantly distorted by this method in order to provide a stronger quantitative fit to the generic interpretation data.
]
That is, we ask if there is some reasonable setting of the prevalence priors that, together with a particular model of generic interpretation, could give rise to the generic interpretation results.
<!-- The hypothesis about the prior elicitation data is a statistical, three-component mixture model; the hypothesis about the generic interpretation data is one of the generic interpretation models specified in the *Computational Model* section.  -->
We also compare the generic interpretation models to each other and a number of control models using the standard Bayesian model comparison metric: marginal likelihoods, or Bayes factors.

We posit uncertainty in the parameters governing the listener prevalence prior distribution. 
Our prior elicitation task helps constrain the values of the parameters of the prevalence priors, which we assume takes the form of a three-component mixture of Beta distributions model.^[
  We use Beta distributions to match the form of the response variable: a number between 0 - 1.
]
The only other parameters we introduce are specific to certain generic interpretation models.
The model that assumes a literal semantics identical to the quantifier "most" (threshold at 0.5) must be outfitting with an extrinsic noise parameter in order to accomodate responses that are literally false under this interpretation of a generic (i.e., implied prevalence ratings below 0.5).
The pragmatic generic interpretation model posits that listeners interpret generics by strengthening a literal meaning equivalent to a single positive example of the category with the communicative force of an utterance produced by an intentional speaker.
This model thus has two additional free parameters: The implicit cost of producing a generic in comparison to staying silent (*utterance cost*) and the degree of information-theoretic optimality with which the speaker behaves (*speaker optimality*).
Comparing models using the marginal likelihood of the data will penalize the pragmatic model because of these additional degrees of freedom [@LeeWagenmakers2014].
<!-- To preview our results, we find that the single observation / uncertain threshold model accomodates the generic interpretation results, while models that update beliefs based on a fixed value of a threshold (e.g., "some" or "most") are misspecified.  -->


<!-- , wherein we infer the latent values of the variables governing the prevalence priors from the prevalence prior data (Expt. 1a) and the generic interpretation data (Expt. 1b).  -->

#### Model definition and inference

<!-- Our primary analysis concerns how the comparison of our generic interpretation models (literal and pragmatic) to each other and to alternative models.  -->
For each candidate model of generic interpretation (literal, pragmatic, "some", "most"), we build a joint Bayesian data-analytic model to simultaneously predict the prevalence prior data (Expt. 1a) and the generic interpretation data (Expt. 1b), similar to the methods of @Tessler2019psychrev. 
<!-- To fit these models, we follow the same Bayesian data analytic approach as Expt. 1 to jointly predict the prevalence prior data (Expt. 2a) and the generic interpretation data (Expt. 2b). -->
We model the prevalence prior data as coming from a mixture of three Beta distributions and we model the generic interpretation data as being generated from the candidate computational model of generic interpretation.
Each prevalence prior has eight parameters governing its shape: a mean $\gamma$ and variance parameter $\xi$ for each of the three Beta components and two parameters $\phi_1$, $\phi_2$ that describe the relative weighting among the three components.
We put uninformative priors over these parameters $\gamma \sim \text{Uniform}(0, 1)$, $\xi \sim \text{Uniform}(0, 100)$, $\phi \sim \text{Dirichlet}(1,1,1)$.^[
  Another reasonable, less structured model of the prevalence prior data would be a Dirichlet over discretizes bins along the prevalence scale. We choose this formulation because it assumes some correlation between near values along the scale and has fewer parameters. 
]
The inferred prevalence prior distributions are then used as the prevalence prior $P(r)$ in whatever model of generic interpretation we are testing in order to generate predictions for the generic interpretation data.^[
  Though described sequentially, this inference procedure is actually performed simultaneously.
]
In addition, the model of "most" semantics has an additional noise parameter, which adds a probability $\phi$ of a participant responding randomly; we put a uniform prior over the value of this parameter to estimate the proportion of responses this model would have to chalk up to noise in order to accomodate: $\phi \sim \text{Uniform}(0, 1)$. 
The pragmatic generic interpretation model (Eq. \ref{eq:L1}) has two free parameters : the rationality of the speaker model $\alpha$ and the speaker's cost of producing the generic utterance (in comparison to staying silent) $c = \text{cost}(generic)$, over which we put uninformative priors, consistent with the literature on this class of models: $\alpha \sim \text{Uniform}(0, 30)$ and $c \sim \text{Uniform}(0, 10)$.
The literal generic interpretation model and models of "some" and "most" quantifier semantics have no additional parameters.
To learn about the credible values of the parameters as well as generate model predictions for the generic interpretation data, we performed Bayesian inference on each model by running three chains of an incrementalized version of MCMC [@Ritchie2016] for 750,000 iterations, removing the first 250,000 iterations for burn-in.
Convergence was checked qualitatively by confirming the similarity of the results across chains; there were no appreciable differences in the results between different chains.

#### Model criticism

\ndg{we should have a brief result reporting the posterior predictive check on the priors data? ie how well does our joint data analysis accommodate the empirical patterns in the prevalence priors.}
\mht{this paragraph okay?}
A qualitative examination of the inferred prevalence prior distributions showed that they were well-modeled as a mixture of three Beta distributions. 
Often one Beta distribution is devoted to accounting for the very small numbers (0% or near 0% prevalence ratings, for all the categories for which the property is absent), while the two other components account for the other parts of the distribution of responses, the details of which depend upon the property (see Figure\ \@ref(fig:genInt-prevPrior) for examples of priors of different shapes).
To provide a quantitative examination of the fit to the priors data, we look at the posterior predictive distribution discretized to bins in increments of 10% prevalence (i.e., categorical distributions over the bins of 0%, 10%, ..., 90%, 100% prevalence) and compare the model's inferred priors to the empirical counts that fell into those bins in the prior elicitation task. 
The joint data analysis model accomodates the empirical patterns in the prevalence priors very well: $r^2(`r md.prevprior.stats[["n"]]`) = `r format(md.prevprior.stats[["r2"]], digits = 3)`$, $MSE = `r format(md.prevprior.stats[["mse"]], digits = 2)`$.

To evaluate how well each model can accomodate the implied prevalence data, we compare the Maximum A-Posteriori value of each model's posterior predictive distribution of implied prevalence ratings to the mean implied prevalence ratings for each of the 75 items.
Table 1 shows the mean squared errors and proportion of explained variance for predictions based on the prevalence prior alone, a literal "some" model, a literal "most" model (which includes an additional fit noise parameter), a literal generics model (single positive observation), and the pragmatic generics model. 
Figure\ \@ref(fig:genint-modelingResults) reveals that the models based on the prevalence prior and the quantifier "some" (i.e., a fixed threshold at the lowest possible value) consistently *underpredict* the implied prevalence, consistent with the intuition that assigning the semantics of "some" for generics is too weak.
Though the model based on a semantics for "most" (i.e., a fixed thrshold at 0.5) does better in terms of mean squared error, it explains substantially less variance; the "most" model assigns `r format_empirical_ci(m.samp.most.noise, "MAP", "cred_lower", "cred_upper")` proportion of the data to be noise, because of the high number of responses below 50\%. 
<!-- The literal generic interpretation model updates its beliefs as if the listener had observed a single, positive example of the category with the property, and the pragmatic generic interpretation model sees that single observation as one that was intentionally transmitted by the speaker. -->
Unlike each of the alternative semantic models, the two models based on belief updating according to a single, positive observation (literal and pragmatic generics models) can accomodate the highly variable generic interpretation data with a high degree of quantitative accuracy. 
The pragmatic model slightly outperforms the literal generics model in terms of both mean squared error and variance explained at the level of average responses.
The inferred values of the speaker optimality parameter $\alpha$ was `r format_empirical_ci(filter(m.genint.params.l1, src == "pragmatic_unlifted", type == "speakerOptimality") %>% rename(mean = MAP, ci_lower = cred_lower, ci_upper = cred_upper))` and the  generic cost parameter $c$ was `r format_empirical_ci(filter(m.genint.params.l1, src == "pragmatic_unlifted", type == "cost") %>% rename(mean = MAP, ci_lower = cred_lower, ci_upper = cred_upper))`. 
<!-- The literal meaning of a generic provides a weak semantics which makes interpretation heavily determined by the prevalence prior; pragmatic reasoning can then strengthen the weak meaning of a generic to derive stronger interpretations. -->

Figure\ \@ref(fig:genint-modelingResults-bars) shows empirical means and model predictions for ten example items covering the range of average responses from roughly 30\% implied prevalence to almost 100\%. 
All models exhibit some sensitivity to the variance in implied prevalence ratings, because they all have the prevalence prior in their definition. 
It is the actual magnitude of the stength of the generalization implied by the utterance that is predicted to be different, with the prior and "some" models predicting too weak of an interpretation. 
"Most" is handicapped by its high inferred rate of noise and does not make dramatically different predictions across the items.
Both the literal and pragmatic generic interpretation models track the quantitative variance across these items.
The evidence for pragmatic interpretations is most apparent for items that receive high but not at-ceiling implied prevalence ratings (e.g., *have spots*, *hunt other animals*), suggesting that interpretations are most in dangerous of being strengthened beyong a speaker's intent for properties for which the listener is most uncertain about their distribution [cf., the asymmetry reported in @Cimpian2010].
\mht{could do a regression comparing whether or not the pragmatic model is better than the literal model (or the difference in sq.err) vs. the entropy of the prior... to see if pragmatics is most visible under conditions of uncertainty}
<!-- For these items, a single positive observation, even with structured background knowledge, is not enough to sufficiently update beliefs according to the generic sentence. -->
<!-- The pragmatic model is able to strengthen the interpretation beyond the literal meaning, drawing inferences that are quantitatively more consistent with human judgments. -->

<!-- As a first attempt to understand the implied prevalence data in a quantitative way, we compare the means of the inferred prevalence priors to the empirical means of the implied prevalence data  left-most facet). -->
<!-- The means of the prevalence prior distributions do a rather poor job at predicting the implied prevalence ratings, consistently underpredicting the ratings ($r^2(`r filter(md.impprev.stats, semantics == "uncertain", parameter == "prior", src == "pragmatic_unlifted")[["n"]]`) = `r filter(md.impprev.stats, semantics == "uncertain", parameter == "prior", src == "pragmatic_unlifted")[["r2"]]`$, $MSE = `r format(filter(md.impprev.stats, semantics == "uncertain", parameter == "prior", src == "pragmatic_unlifted")[["mse"]], digits = 2)`$). -->
<!-- The fact that the prevalence prior data underpredicts the generic interpretation data is good evidence that participants are actually *updating* their beliefs about the property in the generic interpretation task, as opposed to simply responding according to their prior beliefs. -->

<!-- The alternative semantic models use the same data analytic approach but update beliefs according to a fixed-threshold semantics. -->
<!-- \ndg{similarly, a short section on alternative models at the end of the model section would save having to re-explain the alternatives each time, and give a better flow.} -->
<!-- These alternative models correspond to models of the quantifier "some" and "most" and provide strict tests of our generic interpretation model because they too have access to the full prior distribution over prevalence as well as the flexibility endowed by the Bayesian data analysis method. -->
<!-- Figure\ \@ref(fig:genint-modelingResults) shows each of the alternative model's predictions (technically, the BDA models' posterior predictive distributions) for the mean implied prevalence of each of the seventy-five novel generic sentences, in comparison to the empirical means.  -->
<!-- It is impossible for the model based on the quantifier "most" to accomodate these data: *Most* has a strict threshold at 0.5 and thus assigns probability 0 to all responses less than 0.5. \ndg{you said this already in expt 1} -->
<!-- In order to alleviate this issue, we endow the BDA model for "most" with a external noise parameter, which allows the model to randomly guess on some proportion of trial (the exact proportion is a parameter to be inferred from the data). -->
<!-- A model based on the semantics for "most" is also not well-suited for these data $r^2(`r filter(md.impprev.stats, semantics == "most", parameter == "posterior", src == "literal")[["n"]]`) = `r filter(md.impprev.stats, semantics == "most", parameter == "posterior", src == "literal")[["r2"]]`$,  $MSE = `r format(filter(md.impprev.stats, semantics == "most", parameter == "posterior", src == "literal")[["mse"]], digits = 2)`$. -->

```{r genint-modelTable, results="asis"}
md.impprev.stats.tab <- md.impprev.stats %>%
  filter(parameter == "posterior" | (src == "pragmatic_unlifted"), !src == "pragmatic") %>%
  mutate(Model = ifelse(semantics == "uncertain", 
                        ifelse(parameter == "prior", "Prior", 
                               ifelse(src == "literal", "Literal Generic", "Pragmatic Generic")),
                        ifelse(semantics == "some", '"Some"', '"Most"'))) %>%
  ungroup() %>%
  select(Model, mse, r2) %>%
  left_join(., rs.mll.formatted)
              

md.impprev.stats.tab <- md.impprev.stats.tab[with(md.impprev.stats.tab, order(-mse)),] %>%
  mutate(mse = format(mse, digits = 2),
         r2 = format(r2, digits = 3),
         mll = format(mll, digits = 1))

colnames(md.impprev.stats.tab) <- c("Model", "MSE" ,"$r^2$", "Log marginal likelihood")
print(xtable::xtable(md.impprev.stats.tab,
                     caption = "Summary statistics for models of generic interpretation. Mean Squared Errors and variance explained are calculated at the level of the item averages. Marginal likelihood is computed over the full distribution of responses."),sanitize.text.function=function(x){x}, 
      type="latex", comment = F, table.placement = "H", size="\\fontsize{9pt}{10pt}\\selectfont", include.rownames=FALSE)

```


#### Model comparison

Our second data analytic strategy is a formal model comparison to determine the extent to which the pragmatic generic interpretation model is the best explanation of the data, given it is a slightly more complex model than the literal interpretation model. 
<!-- improved quantitative fit exhibited by the pis a meaningful difference and justifies its use of additional free parameters. -->
To do this, we compute Bayes Factors which compare the likelihood of the observed data under each of the models, averaging across all values of the free parameters of a model. 
This form of model comparisons penalizes models with extra free parameters if, in fact, the model's predictions are highly sensitive to exact values of those parameters.
Comparing our models using a fully Bayesian strategy with uncertainty over the prevalence priors (analogous to what is performed in the model criticism section) is computationally challenging. 
Therefore, we compute the likelihood of the data under each model by fixing the prevalence priors to the raw empirical distributions; fixing the prevalence priors reduces the overall flexibility of the models, but does so in a way that penalizes all models to an equal extent.
To estimate the marginal likelihood of the data under each model, we used an Annealed Importance Sampling algorithm [@neal2001annealed] implemented in the probabilistic programming language WebPPL. 
The results are shown in Table 1: The pragmatic model is on the order of $e^{60}$ times better at accounting for the implied prevalence data, even taking into the model's additional flexibility. 
This is extremely strong evidence that the best model for how generics update beliefs is as a single positive example understood as being produced with a pedagogical intent.

```{r genint-empiricalData, fig.width = 11, fig.asp = 0.95, fig.cap="Implied prevalence ratings for a diverse set of seventy-five items. Vertical lines denote means, points are individual empirical judgments. Error bars denote bootstrapped 95% confidence intervals.", cache = F}
df.int.filtered <- df.int.filtered %>% 
    mutate(property = factor(property, 
                           levels =  with(df.int.filtered.bs, property[order(mean)])))
  
df.int.filtered.bs %>%
  mutate(property = factor(property,
                           levels =  with(df.int.filtered.bs, property[order(mean)]))) %>%
  ggplot(., aes( x = property, y = mean))+
  #geom_hline(yintercept = 0.5, lty = 2, alpha = 0.5)+
  geom_point(shape = 73, size = 4)+
  geom_point(data = df.int.filtered, position = position_jitter(),
             inherit.aes = F, aes( x = property, y = response),
             alpha = 0.2)+
  # geom_col(position = position_dodge(0.8),
  #          aes(y = mean),
  #          width = 0.8, alpha = 0.3, color = 'black')+
    coord_flip()+
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper),
                position = position_dodge(0.8), width = 0.3, size = 1)+
  ylab("Implied prevalence rating")+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  #scale_fill_solarized()+
  #scale_color_solarized()+
  xlab("")

# cowplot::plot_grid(
#   fig.genint.interpretation.ci + theme(plot.margin = unit(c(12, 6, 6, 6), "pt"),
#                          legend.position = 'none'),
#   fig.genint.interpretation.modelss + theme(plot.margin = unit(c(0, 6, 6, 6), "pt"),
#                          legend.position = 'none'), 
#   nrow = 1,
#   labels = c("A", "B"),
#   rel_widths = c(3.5, 1)
# )
# library(ggridges)
# df.int.filtered %>%
#   mutate(property = factor(property, 
#                            levels =  with(df.int.filtered.bs, property[order(mean)]))) %>%
#   ggplot(., aes( x = response, y = property, group = property))+
#   #geom_hline(yintercept = 0.5, lty = 2, alpha = 0.5)+
# 
#   # geom_col(position = position_dodge(0.8),
#   #          aes(y = mean),
#   #          width = 0.8, alpha = 0.3, color = 'black')+
#   geom_density_ridges(jittered_points = F)+
#   xlab("Implied prevalence rating")+
#   scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))
          #scale_fill_solarized()+
  #scale_color_solarized()+
```

```{r genint-modelingResults, fig.width = 9, fig.asp = 0.95, fig.cap="Posterior predictive model fits for quantitative models based on (from left to right): (i) the mean of the prevalence prior, (ii) a threshold semantics fixed at 0.01 (\"some\"), (iii) a threshold semantics fixed at 0.5 (\"most\"), (iv) a single positive observation (literal generic), and (v) a pragmatically interpreted single positive observation (pragmatic generic). The model for \"most\" is outfitted with a noise parameter to accomodate data points that are logically impossible given the fixed semantics. Error bars denote bootstrapped 95% confidence intervals for the behavioral data and 95% highest posterior density intervals for the model predictions.", cache = F}
md.impprev %>% 
  ungroup() %>%
  filter(!(src == "pragmatic")) %>%
  filter(((src == "pragmatic_unlifted" &	semantics == "uncertain" & parameter == "prior") |
           (parameter == "posterior")))   %>%
  mutate(src = paste(parameter, "_", semantics, "_", src, sep =""),
         src = factor(src, 
                      levels = c("prior_uncertain_pragmatic_unlifted", 
                                 "posterior_some_literal",
                                 "posterior_most_literal", 
                                 "posterior_uncertain_literal", 
                                 "posterior_uncertain_pragmatic_unlifted"),
                      labels = c("Prevalence prior mean",
                                 '"some"\n(threshold = 0.01)',
                                 '"most" + noise\n(threshold = 0.5)',
                                 'generic\n(literal)',
                                 'generic\n(pragmatic)'))) %>%
  ggplot(., aes( x = model_MAP, xmin = model_lower, xmax = model_upper, 
                        y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
  geom_errorbar(alpha = 0.3)+geom_errorbarh(alpha = 0.3)+
  geom_point(alpha = 0.7)+
  facet_wrap(~src, nrow = 1)+
  scale_x_continuous(limits = c(-0.01,1.01), breaks = c(0, 0.5, 1)) +
  scale_y_continuous(limits = c(-0.01,1.01), breaks = c(0, 0.5, 1)) +
  coord_fixed()+
  xlab("Model prediction")+
  ylab("Human implied prevalence")

```


```{r genint-modelingResults-bars, fig.width = 16, fig.asp = 0.25, fig.cap="Caption.", cache = F}

gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}



md.litprag.long <- md.impprev %>%
  ungroup() %>%
  select(-mean, -ci_lower, -ci_upper, -empirical_stat) %>%
  rename(mean = model_MAP, ci_lower = model_lower, ci_upper = model_upper) %>%
  bind_rows(., md.impprev %>%  ungroup() %>%
      select(property,n, mean, ci_lower, ci_upper) %>%
        mutate(src = 'data', semantics = 'data', parameter = 'data')) %>%
  mutate(property = ifelse(property == "live to be a hundred years old","live to be 100 years old", property))

example.items <- c("have spots", 
                   #"live in trees", 
                   #"sleep during the day", 
                   #"eat insects", 
                   "chase their tails",
                   #"ride the subway",
                   "perform in the circus",
                   "drink soda",
                   "hunt other animals", 
                   "have four legs",
                   "carry Lyme disease", 
                   "live to be 100 years old")





md.models.bars <- md.litprag.long %>%
      filter(parameter == "posterior" | parameter == "data" | src == "pragmatic_unlifted", 
             property %in% example.items,
             !(src =="pragmatic")) %>%
  mutate( src = paste(src, semantics, parameter, sep = "_"),
         src = factor(src, levels = c("pragmatic_unlifted_uncertain_prior", 
                                      "literal_some_posterior", 
                                      "literal_most_posterior", 
                                      "literal_uncertain_posterior",
                                      "pragmatic_unlifted_uncertain_posterior", 
                                      "data_data_data"),
                      labels = c("Prevalence prior",
                                 '"some" (threshold = 0.01)',
                                 '"most" (threshold = 0.5) + noise ',
                                 'Literal generic',
                                 'Pragmatic generic', 'Empirical mean')),
         property = factor(property, 
                           levels = unique(with(md.litprag.long %>%
                                           filter(parameter == "data", property %in% example.items),
                                         property[order(mean)])))) %>%
ggplot(., aes( x = src, 
               fill = src, y = mean, ymin = ci_lower, ymax = ci_upper))+
  geom_col(position = position_dodge(), color = 'black')+
  geom_linerange(alpha = 0.3)+
  facet_wrap(~property, nrow = 1, 
             labeller = labeller(property = label_wrap_gen(18)))+
  #scale_fill_viridis(discrete = T)+
  #scale_fill_manual(values = c('#F0FFFF', c("#984ea3", "#4daf4a", "#377eb8", "#e41a1c"), '#838B8B'))+
  scale_fill_manual(values = c('#F0FFFF', c("#9ecae1", "#3182bd", "#a1d99b", "#31a354"), '#838B8B'))+
  labs(y = "Implied prevalence rating", x = "Source")+
  #scale_x_continuous(limits = c(0.25, 1), breaks = c(0.25, 0.5, 0.75, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  guides(fill = guide_legend(nrow = 1, title = ""))+
  theme(axis.text.x = element_blank(),
        legend.position = 'bottom')

# ggsave(
#   md.models.bars,
#   file = 
#     "~/projects/generic-interpretation/paper/genint_files/figure-latex/genint-modelingResults-bars-1.pdf",
#   width = 10.5, height = 3.2)


# cowplot::plot_grid(
#   md.models.bars,
#   fig.litprag.scatter,
#   nrow = 1,
#   rel_widths = c(1, 0.3),
#   labels = c("A", "B")
# )
md.models.bars
```


```{r genint-litPrag-scatter, fig.width = 8, out.width = "0.8\\textwidth", fig.align="center", fig.asp = 0.5, fig.cap="Caption.", cache = F}
md.litprag.wide <- left_join(
  md.impprev %>%
    filter(src == "literal", parameter == "posterior", semantics == "uncertain")  %>%
    rename(literal_MAP = model_MAP,
           literal_upper = model_upper,
           literal_lower = model_lower) %>%
    ungroup() %>%
    select(property, literal_MAP, literal_upper, literal_lower),
  md.impprev %>%
    filter(src == "pragmatic_unlifted", parameter == "posterior", semantics == "uncertain")  %>%
    rename(pragmatic_MAP = model_MAP,
           pragmatic_upper = model_upper,
           pragmatic_lower = model_lower) %>%
    ungroup() %>%
    select(property, pragmatic_MAP, pragmatic_upper, pragmatic_lower)
) %>% left_join(., md.impprev %>%
    ungroup() %>%
    distinct(property, mean, ci_lower, ci_upper)
)

fig.litprag.scatter <- ggplot(md.litprag.wide, aes( x = literal_MAP, xmin = literal_lower, xmax = literal_upper,
                            y = pragmatic_MAP, ymin = pragmatic_lower, ymax = pragmatic_upper))+
  geom_abline(intercept  = 0 , slope = 1, linetype = "dashed")+
  geom_point()+
  geom_linerange(alpha = 0.3)+
  geom_errorbarh(alpha = 0.3)+
  coord_fixed(ratio = 1)+
  labs(y = "Pragmatic model prediction", x = "Literal model prediction")+
  scale_x_continuous(limits = c(0.25, 1), breaks = c(0.25, 0.5, 0.75, 1))+
  scale_y_continuous(limits = c(0.25, 1), breaks = c(0.25, 0.5, 0.75, 1))

fig.litprag.scatter

```


<!-- ## Discussion -->

<!-- In Expt. 1, we extended the findings the preliminary study to a larger and more diverse set of stimuli.  -->
<!-- We uncovered a continuum of interpretations of novel generic statements, and were able to predict the gradience in interpretation using the uncertain threshold generics model.  -->
<!-- Graded interpretations of generic statements result from diverse prior beliefs about properties operated over using an underspecified threshold criterion. -->
<!-- In our final experiment, we experimentally manipulate these prior beliefs to test their causal role in governing generic interpretations. -->


# Experiment 2: Prior Manipulation

<!-- In both of the previous experiments, we measured both generic interpretation and the prevalence prior and related the two via the uncertain threshold model.  -->
<!-- We have shown that prevalence priors and generic interpretation are related but not that intervening on the prevalence prior can change interpretations. -->

In this experiment, we test the causal role of the prevalence prior in generic interpretation.
In the previous experiment, we saw how participants' prior beliefs about the prevalence of the property measured by asking about alternative categories (Expt. 1a) related to the prevalence implied by a novel generic (Expt. 1b) via the pragmatic generic interpretation model.
Though the pragmatic generics model is able to finely track interpretations in a way that alternative models cannot, the evidence so far for the influence of the prevalence prior on interpretation is merely correlational. 
Here we manipulate the prevalence prior and measure the resulting influence on interpretation.
To serve as a manipulation check, we first elicit predictions about prevalences following our prior manipulation procedure; these predictions also serve as the measurements of the (manipulated) prevalence priors used in our model.

## Experiment 2a: Prior elicitation (manipulation check)

### Methods

#### Participants

We recruited 200 participants from MTurk. 
This number was arrived at with the intention of getting approximately 30 ratings for each unique item in the experiment.
The experiment took on average 3 minutes and participants were compensated \$0.30.

#### Materials 

The goal of this experiment was to manipulate the prevalence prior. 
Participants were familiarized with one of ten prevalence prior distributions, shown in Figure\ \@ref(fig:priorManipulationExpt)E). 
Nine of the ten distributions were bimodal with modes either at 0%, 25%, 50%, 75%, or 100%; the tenth distribution was uniform over all prevalence levels between 0% and 100%.
The mixture between the two modes was always 50% (half of samples were from one mode and half from the other, as shown in Figure\ \@ref(fig:priorManipulationExpt)B).
<!-- To limit the influence of our prior manipulation cover story (described below),  -->

Our cover story involved learning about a single property (*know when earthquakes are about to happen*). 
We used this property because its prevalence prior elicited in Expt. 1a suggested participants had a lot of uncertainty as to what prevalence levels to expect for this property; we thus expected this property might be particularly amenable to manipulation.^[
  Pilot testing using a completely novel property (e.g., *daxing*) appeared to make the task too artificial. Participants seemed to treat the task as a number game. 
]


#### Procedure

```{r priorManipulationExpt, fig.cap="Overview of Experiment 2. A: Prevalences of feature in animals are shown one at a time, described in text and displayed in a table. B: Participants are asked to review previous results once all displayed. C: Prior elicitation task: Participants predict the results of the next five animals. D: Generic interpretation task: Participants rate prevalence after reading generic sentence. E: Experimentally manipulated prevalence prior distributions. The distribution shown in B is the \\emph{weak or deterministic} distribution.", fig.width = 11,fig.asp = 0.6}
zero.samples <- rbeta(n = 5000, shape1 = 1, shape2 = 100)
weak.samples <- rnorm(n = 5000, mean = 0.25, sd = 0.06)
half.samples <- rnorm(n = 5000, mean = 0.50, sd = 0.06)
strong.samples <- rnorm(n = 5000, mean = 0.75, sd = 0.06)
deterministic.samples <-rbeta(n = 5000, shape1 = 50, shape2 = 1)

sample.dists <- bind_rows(
  data.frame( dist = "0/25", x = c(zero.samples, weak.samples) ),
  data.frame( dist = "0/50", x = c(zero.samples, half.samples) ),
  data.frame( dist = "0/75", x = c(zero.samples, strong.samples) ),
  data.frame( dist = "0/100", x = c(zero.samples, deterministic.samples) ),
  
  data.frame( dist = "25/50", x = c(weak.samples, half.samples) ),
  data.frame( dist = "25/75", x = c(weak.samples, strong.samples) ),
  data.frame( dist = "25/100", x = c(weak.samples, deterministic.samples) ),

  data.frame( dist = "50/100", x = c(half.samples, deterministic.samples) ),
  data.frame( dist = "75/100", x = c(strong.samples, deterministic.samples) ),
  
  data.frame( dist = "uniform", x = rbeta(n = 50000, shape1 = 1, shape2 = 1))
)



distribution.order.levels <- c("rare_weak",
                                        "rare_half",
                                        "weak_or_half",
                                        "rare_strong",
                                        "weak_or_strong",
                                        "rare_deterministic", 
                                        "weak_or_deterministic",
                                        "half_or_deterministic",
                                        "strong_or_deterministic",
                                        "uniform")

# distribution.order.labels <- c("rare weak",
#                                         "rare half",
#                                         "weak or half",
#                                         "rare strong",
#                                         "weak or strong",
#                                         "rare deterministic", 
#                                         "weak or deterministic",
#                                         "half or deterministic",
#                                         "strong or deterministic",
#                                         "uniform")

distribution.order.labels <- c("0/25","0/50","25/50","0/75","25/75","0/100", "25/100",
                                        "50/100",
                                        "75/100",
                                        "uniform")
                                        
fig.manipulatedPriors <- sample.dists %>%
  mutate(dist = factor(dist, levels = distribution.order.labels)) %>%
  ggplot(., aes(x = x))+ 
  geom_density(aes( y = ..scaled..))+ #
  # geom_histogram(bins = 20,
  #   aes(y=(..count..)/tapply(..count..,..PANEL..,sum)[..PANEL..]),
  #   color = 'black', fill= 'grey')+
  scale_x_continuous(limits = c(-0, 1), breaks = c(0, 0.5, 1))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  #scale_fill_manual(values = c("#636363", "#d7191c"))+#, "#2b83ba"))+
  #scale_fill_solarized()+scale_color_solarized()+
  #scale_color_manual(values = c("#636363", "#d7191c"))+
  ylab("Scaled Prior Probability")+
  xlab("Prevalence")+
  facet_wrap(~dist, scales = 'free', nrow = 2)+
  guides(fill = F, color = F)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal')


prior.manip.exp.1 <- ggdraw() + draw_image("figs/prior-manip-paradigm-1.jpeg", scale = 0.9) +
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1)) 
prior.manip.exp.2 <- ggdraw() + draw_image("figs/prior-manip-paradigm-2.jpeg", scale = 0.9)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1)) 
prior.manip.exp.3 <- ggdraw() + draw_image("figs/prior-manip-paradigm-priorMeasure.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))
prior.manip.exp.4 <- ggdraw() + draw_image("figs/prior-manip-paradigm-intMeasure.jpeg", scale = 0.95)+
  theme(plot.margin = unit(c(2,0,0,0), "pt"),
        panel.background = element_rect(colour = "black", size=1))




prior.manip.toprow <- plot_grid(prior.manip.exp.1, 
                             prior.manip.exp.2, labels = c("A", "B"), nrow = 1, align = 'vh')

title <- ggdraw() + draw_label("Prevalence prior manipulation")
plot_grid(
  plot_grid(title, prior.manip.toprow, ncol=1, rel_heights=c(0.15, 1)),
  plot_grid(
    plot_grid(ggdraw() + draw_label("Manipulation check"), 
              prior.manip.exp.3, ncol=1, rel_heights=c(0.15, 1) ), 
    plot_grid(ggdraw() + draw_label("Generic interpretation"), 
              prior.manip.exp.4, ncol=1, rel_heights=c(0.15, 1) ),
              labels = c("C", "D"), nrow = 1),
  # plot_grid(add_sub(prior.manip.exp.3, "Manipulation check"), 
  #           add_sub(prior.manip.exp.4,"Generic interpretation"), labels = c("C", "D"), nrow = 1),
  fig.manipulatedPriors, labels = c("","", "E"),
  rel_heights = c(1,1,1.5),
  nrow = 3)
```

In order to avoid participants learning the structure of the task, participants completed only a single trial: Each participant saw only one prevalence distribution.
Participants were given a cover story in which they were asked to imagine they were an astronaut-scientist exploring a distant planet with lots of new animals on it.
They were studying these animals with another scientist to understand the animals' ability to "know when earthquakes are about to happen".
Participants were then shown data that they and their fellow scientist had collected about the prevalence of the property among different novel animals.
In order to minimize distraction, novel animals were simply labeled by a letter (e.g., "Animal C" or just "Cs").
Information about the animal appeared in an *evidence statement* (e.g., "After studying the animal, you and your partner determine that 98\% of Cs *know when earthquakes are about to happen*.") and were displayed in a table showing the corresponding numerical data (Figure\ \@ref(fig:priorManipulationExpt)A). 
Participants proceed in a self-paced way by clicking a button to reveal information about the next animal.
After participants viewed the data for ten categories, they are told to review the information about the animals before continuing (Figure\ \@ref(fig:priorManipulationExpt)B).
Then, the data table is removed and participants are provided the following prompt:

> Tomorrow, you and your partner will study 5 other new species of animals. What percentage of each do you think will know when earthquakes are about to happen?

Participants were given five slider bars ranging from 0% - 100%, and asked to predict the prevalence for the next five categories (Animals K - P; Figure\ \@ref(fig:priorManipulationExpt)C).
After rating the slider bars, we asked participants to explain why they gave the responses that they did.
Then, they completed an attention check survey where they were asked what property was being investigated (choosing a response from a list of 10 options) and to input one of the prevalence levels they saw on the familiarization screen.

### Results

Participants who failed to either select the correct property or list a correct number from the familiarization period were excluded.
Similar to the data from Expt. 1a, a response in this task can be thought of as a sample from a prevalence prior distribution and the distribution of responses as estimates for the whole distribution.
We visualize the distributions by discretizing the responses so that each response goes into one of twenty equally spaced bins (effectively turning the 101-pt scale into a 20-pt scale).
Figure\ \@ref(fig:priorManipulationResults)A (top row) shows the empirically elicited predicted prevalence  distributions.
These distributions clearly reflect the familiarization distributions supplied to participants in the experiment, with some idiosyncractic features.
The distribution that is most different from the familiarization distribution is the uniform distribution; the empirical predicted distribution is more unimodal with a peak at 50%. 
Many distributions exhibit a regression to the mean in some participants' predictions: In the *rare or deterministic* distribution (which featured prevalence levels either around 0% or 100%), some participants guess that the next animal will have 50% prevalence; a similar phenomenon can be observed in the *weak or strong* distribution (25% or 75%) and to a lesser extent in the other bimodal distributions. 


```{r priorManipulationResults, fig.cap="Experiment 2 results. A: Empirical distribution of responses for the prior elicitation task (Expt. 2a), literal and pragmatic generics model predictions based on those empirical distributions, and empirical distributions of responses for the generic interpretation task (Expt. 3b). B: Model predicted means for alternative models and empirical means.", fig.width = 14.2,fig.asp = 0.5, cache = F}
load(file = "cached_results/gentInt_manipPriors_dists_expvals2.RData")
# md.pm.int.3.filtered.expvals.bs.summary, md.pm.int.3.filtered.dists.bs.summary,
#      md.pm.int.3.filtered.dists.bs.summary.wide, md.pm.int.3.filtered.expvals.bs.summary.wide


md.pm.int.3.filtered.expvals.bs.summary.wide.summarized <- left_join(
      md.pm.int.3.filtered.expvals.bs.summary.wide %>%
      group_by(model) %>%
      summarize(ev_r2 = format(cor(model_prediction, mean)^2, digits = 2),
                ev_n = n(),
                ev_mse = format(mean((model_prediction - mean)^2), digits = 3)),
    md.pm.int.3.filtered.dists.bs.summary.wide %>%
      group_by(model) %>%
      summarize(fd_r2 = format(cor(model_mean, data_mean)^2, digits = 2),
                fd_n = n(),
                fd_mse = format(mean((model_mean - data_mean)^2), digits = 3))
    )




distribution.order.levels <- with(md.pm.int.3.filtered.expvals.bs.summary %>%
                         filter(model == "literal"), distribution[order(mean)])
distribution.order.labels <- gsub("_", " ", distribution.order.levels)

distribution.order.labels <- gsub(" ", "/", gsub(" or", "", gsub("deterministic", "100", gsub("strong", "75", gsub("half", "50", gsub("weak", "25", gsub("rare","0", distribution.order.labels)))))))


my_label_parsed <- function (variable, value) {
  if (variable %in% c("prior", "model")) {
    return(as.character(value))
  } else {
    llply(as.character(value), function(x) parse(text = x))    
  }
}


fig.int.dists.bs <- md.pm.int.3.filtered.dists.bs.summary %>%
  filter(model %in% c("prior", "literal", "pragmatic_unlifted", "data")) %>%
  mutate(distribution = factor(distribution,  levels = distribution.order.levels, labels = distribution.order.labels),
         model = factor(model, levels = c("prior","literal", "pragmatic_unlifted", "data"),
                        labels = c("prevalence prior", "literal generic", 
                                   "pragmatic generic", "data"))) %>%
  ggplot(., aes( x = state, y = mean, ymin = lower, ymax = upper, fill = distribution))+
  geom_col(position= position_dodge(), alpha = 0.8, color = 'black')+
  geom_linerange(position = position_dodge(),  alpha = 0.6)+
  geom_text(data = data.frame(model = c("prevalence prior", "literal generic",
                                        "pragmatic generic", "data"),
    x = c(0.5, 0.5, 0.5, 0.5),
    y = c(0.5, 0.5, 0.6, 0.45),
    distribution = c("0/100","0/100","0/100","0/100")), 
    aes( x= x ,y =y, label = model), inherit.aes = F,
    size = 3.25)+
  facet_grid(model~distribution)+
  scale_x_continuous(breaks = c(0, 1))+
  scale_fill_viridis(discrete = T)+
  ylab("predicted / empirical probability")+
  xlab("prevalence")+
  theme(strip.text.y = element_blank())+
  guides(fill = F)


fig.int.expval.bs <- md.pm.int.3.filtered.expvals.bs.summary %>%
  ungroup() %>%
  filter(!(model %in% c("fixed_0.625", "fixed_0.875"))) %>%
  mutate(distribution = factor(distribution,  levels = distribution.order.levels, labels = distribution.order.labels),
         model = factor(model,
                        levels = c("prior", 
                                   "fixed_0.05", "fixed_0.1",
                                   "fixed_0.3", "literal", "pragmatic_unlifted","data"),
                        labels = c(expression(paste("prevalence prior")),
                                   expression(paste('"some" (', theta, '=0.01)')), 
                                   expression(paste('"not rare" (', theta, '=0.10)')), 
                                   expression(paste('"more than third" (', theta, '=0.33)')), 
                                   expression(paste("literal generic")),
                                   expression(paste("pragmatic generic")), 
                                   "data"))) %>%
  ggplot(., aes( x = distribution, y = mean, ymin = ci_lower, ymax = ci_upper, fill = distribution))+
  geom_col(position= position_dodge(), color  = 'black', alpha = 0.8)+
  geom_linerange(position = position_dodge())+
  scale_fill_viridis(discrete = T)+
  ylab("Mean implied prevalence")+
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank())+
  facet_wrap(~model, nrow = 1, labeller = label_parsed)+ 
  guides(fill = guide_legend(reverse=F, ncol = 2))



# fig.int.expval.bs.scatters <- md.pm.int.3.filtered.expvals.bs.summary.wide %>%
#   filter(!(model %in% c("fixed_0.625", "fixed_0.875","fixed_0.375"))) %>%
#   mutate(distribution = factor(distribution, levels = distribution.order.levels),
#          model = factor(model, levels = c("prior", "fixed_0.0175", "fixed_0.125", "literal","pragmatic_unlifted"),
#                         labels = c("prevalence prior", '"some"\n(fixed threshold = 0.01)', '"not rare" (fixed threshold = 0.10)',
#                                    "literal generic \n (single positive observation)", "pragmatic generic"))) %>%
#   ggplot(., aes(x = model_prediction, xmin = model_lower, xmax = model_upper, ymin = ci_lower, y = mean, ymax = ci_upper, fill = distribution))+
#   geom_abline(intercept = 0, slope = 1, lty = 2, alpha = 0.3)+
#   geom_errorbar(position = position_dodge(), alpha = 0.8)+
#   geom_errorbarh(position = position_dodge(), alpha = 0.4)+
#   geom_point(shape = 21, color = 'black')+
#   scale_fill_viridis(discrete = T)+
#   ylab("Empirical mean implied prevalence")+
#   xlab("Model prediction")+
#   scale_x_continuous(limits = c(0, 1), breaks = c(0, 1))+
#   scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
#   coord_fixed()+
#   facet_wrap(~model, nrow = 2)+
#   guides(fill = F)




cowplot::plot_grid(
  fig.int.dists.bs + theme(plot.margin = unit(c(6, 6, 6, 0), "pt"),
                         legend.position = 'none'), 
  fig.int.expval.bs + theme(plot.margin = unit(c(6, 6, 6, 6), "pt")),
  # cowplot::plot_grid(
  #   fig.int.expval.bs + theme(plot.margin = unit(c(6, 6, 6, 6), "pt")),
  #   # fig.int.expval.bs.scatters + theme(plot.margin = unit(c(6, 0, 0, 0), "pt"),
  #   #                        legend.position = 'none'),
  #   # get_legend(fig.int.expval.bs), rel_widths = c(1, #0.65,
  #   #                                               0.2),
  #   # nrow = 1,  labels = c("B", "")
  # ),
  nrow = 2,
  labels = c("A", "B"),
  rel_heights = c(2,1)
  #rel_widths = c(3.5, 1, 1)
)
```




## Experiment 2b: Generic interpretation

We use the empirically measured prevalence distributions from Expt. 2a to generate *a priori* model predictions about the generic interpretation data from our five models.^[
  For the pragmatic model, we fix the two parameters to be the maximum a-posteriori inferred values from the Expt. 1 analysis: $\alpha = 2; c = 3.5$.
]
To account for our uncertainty in the measurement of the prevalence priors, we bootstrap the priors by resampling subjects (with replacement), calculating the empirical prevalence distributions (by binning, as above), and generating model predictions. 
We repeated this procedure 10,000 times to generate distributions of predictions.
The boostrapped mean and 95% quantiles for each prevalence bin in each distribution are shown in Figure\ \@ref(fig:priorManipulationResults)A (middle row).
We see that context-sensitive interpretations are predicted, with idiosyncracies that reflect empirically measured distributions.
In this experiment, we test these predictions against interpretations in the same empirical paradigm as Expt. 2a.

### Methods

```{r load prior manip meta data}
load(file = "cached_results/priorManip_int_metaData.RData")
# expt3.int.catchFail,
#      expt3.int.nonEnglish,
#      expt3.int.n,
#      expt3.int.n.passCatch.English,
#      expt3.int.meanTime,
#df.prior_manip.int.filtered.n_subj_per_item
```

The sample size, exclusion criteria, and planned statistical contrasts were all preregistered: https://osf.io/n342q/register/5771ca429ad5a1020de2872e. 

#### Participants

We recruited `r expt3.int.n` participants from MTurk.
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average `r round(expt3.int.meanTime$mean_time, 1)` minutes and participants were compensated \$0.30.

#### Materials and procedure

The materials and procedure were identical to those of Expt. 2a with the following exception. 
After the familiarization period, the data table is removed and participants are provided the following prompt:

> The next day, you are busy and your partner studies a new animal on their own: Animal K.
> Your partner tells you: "Ks know when earthquakes are about to happen."

In order to encourage participants to pay attention to the language, this text is on the screen for five seconds before participants are asked: "What percentage of Ks do you think know when earthquakes are about to happen?" and provided with a slider bar ranging from 0%-100% (Figure\ \@ref(fig:priorManipulationExpt)D). 
As in Expt. 2a, participants then completed an attention check survey where they were asked what property was being investigated and to input one of the prevalence levels they saw on the familiarization screen.
The experiment can be viewed at http://stanford.edu/~mtessler/generic-interpretation/experiments/generics/prior-manipulation-2.html. 
<!-- Participants see the data from eleven different categories, though they are told the data for one kind were lost and a "?" was placed in the table (\red{Figure}).^[ -->
<!--   These lost results would be found in the *generic interpretation* task, Expt. 3b. -->
<!--   We chose to present a "lost data" category to imply that the sampling procedure for the referent-category was random. The alternative would be to present data from ten categories and have participants rate the eleventh, which could imply that the scientists are collecting data until they find one for which the property applied generically. \red{[clean up]} -->
<!-- ] -->


<!-- scientists had recently discovered lots of new animals that we did not know existed, and that scientists were trying to understand the properties of these new animals.  -->

<!-- From the stimulus set of 74 properties used in Expt. 2, we selected a subset of 24 items that received intermediate interpretation judgments (Expt. 2b: implied prevalence between: 40 - 65\%), which we believed would make them most susceptible to our prior manipulation. -->

### Results

```{r load prior manip regression}
load(file = "cached_results/gentInt_manipPriors_lm.RData")
# rs.lm.priorManip
# rs.lm.priorManip.contrast1,
# rs.lm.priorManip.contrast2,
# rs.lm.priorManip.contrast3,
# rs.lm.priorManip.contrast4,
# rs.lm.priorManip.contrast5,
# rs.lm.priorManip.contrast6,

rs.lm.priorManip.contrast1.summ <- summary(rs.lm.priorManip.contrast1)
rs.lm.priorManip.contrast2.summ <- summary(rs.lm.priorManip.contrast2)
rs.lm.priorManip.contrast3.summ <- summary(rs.lm.priorManip.contrast3)
rs.lm.priorManip.contrast4.summ <- summary(rs.lm.priorManip.contrast4)
rs.lm.priorManip.contrast5.summ <- summary(rs.lm.priorManip.contrast5)
rs.lm.priorManip.contrast6.summ <- summary(rs.lm.priorManip.contrast6)
rs.lm.priorManip.summ <- summary(rs.lm.priorManip)

```

We used preregistered exclusion criteria, which were also used for Expt. 2a.
Participants who failed to either select the correct property or list a correct number from the familiarization period were excluded ($n = `r expt3.int.catchFail`$).
In addition, we excluded participants who self-reported a native language other than English ($n = `r expt3.int.nonEnglish`$). 
This left a total of $n = `r expt3.int.n.passCatch.English`$ participants, with items receiving on average `r round(df.prior_manip.int.filtered.n_subj_per_item$n_subj_per_item)` responses (range = [`r round(df.prior_manip.int.filtered.n_subj_per_item$lb)`, `r round(df.prior_manip.int.filtered.n_subj_per_item$ub)`]).

Our main qualitative hypothesis is that there would be a difference across conditions in interpretations of the same generic sentence ("Ks know when earthquakes are about to happen").
The differences in interpretations are visually evident in the empirical distributions of responses (Figure \@ref(fig:priorManipulationResults)A, bottom row).
More specifically, we predict that the prevalence implied by the generic sentence will, to a first approximation, track the mean of the prevalence prior distribution.
To test this, we ran a linear model predicting responses with a fixed effect of the prior mean (as estimated by the data from Expt. 2a).
There was a significant main effect of prior mean, with a coefficient close to 1 ($\beta_1 = `r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "Estimate"]]`$, $SE=`r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "Std. Error"]]`$, $t = `r rs.lm.priorManip.summ[["coefficients"]][["prior_mean", "t value"]]`$), indicating a strong linear relationship between the mean of the learned prevalence prior and the resulting interpretations.
Furthermore, there was a significant main effect of intercept, indicating that participants' responses were significantly greater than the mean of the corresponding prevalence priors ($\beta_0 = `r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "Estimate"]]`$, $SE=`r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "Std. Error"]]`$, $t = `r rs.lm.priorManip.summ[["coefficients"]][["(Intercept)", "t value"]]`$).
These results show that participants interpret the novel generic sentence in a manner sensitive to the background distribution and that the generic utterance updates participants' beliefs to indicate a prevalence significantly higher than the prior.
Still, there is much more to say about these data; this simple linear model explains less than a quarter of the variance in the data ($r^2 = `r rs.lm.priorManip.summ[["r.squared"]]`$).

From the generics models, we are able to make a number of *a priori* predictions about quantitative differences in mean implied prevalence ratings across the ten conditions. 
These predictions were derived by examining pairs of conditions with small quantitative differences but for which the bootstrapped 95% confidence interval around the generics model predictions did not overlap (Figure\ \@ref(fig:priorManipulationResults)B, "literal generic" panel).^[
  These predictions were derived from the literal generic interpretation model. The experiment was planned before we developed the pragmatic generic interpretation model. Still, many of these predictions would have been derived from the pragmatic model following the same decision rule.
]
<!-- For example, Figure\ \@ref(fig:priorManipulationResults)B ("generics model" panel) -->
With those criteria in mind, we preregistered the following contrast predictions among mean implied prevalences (\{\}s indicate no predicted difference): 0/25 $<$ \{25/50, 0/50\} $<$ \{25/75, *uniform*\} $<$ 25/75 $<$ \{25/100, 50/100\}  $<$ \{75/100, 0/100\}.
To evaluate these predictions, we built a Bayesian regression model with the relevant contrasts using a "zero and one inflated beta" linking function to appropriately model the responses of exactly 0% and 100%, which are undefined under the beta distribution.
The maximum a-posteriori estimates and 95% credible intervals of the coefficients of the regression model, as well as the model predicted qualitative differences, are shown in Table 2.
Each of the five predicted positive differences were estimated to be greater than zero, and each of the four predicted null differences were not different from zero.
Overall, the literal and pragmatic generic interpretation model predicts a lot of the variance in the means of these empirical distributions: (literal results: $r^2(`r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "literal")[["ev_n"]]`) = `r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "literal")[["ev_r2"]]`; MSE = `r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "literal")[["ev_mse"]]`$; pragmatic results: $r^2(`r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "pragmatic_unlifted")[["ev_n"]]`) = `r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "pragmatic_unlifted")[["ev_r2"]]`; MSE = `r filter(md.pm.int.3.filtered.expvals.bs.summary.wide.summarized, model == "pragmatic_unlifted")[["ev_mse"]]`$). 

<!-- We compare these predictions to those made by three fixed threshold models: a literal "some" model (fixed threshold at 0.01), and fixed threshold models with thresholds at 0.1 and 0.35. -->
<!-- The fixed thresholds of 0.1 and 0.35 were chosen to lie in between the 0% and 25% distributions, and the 25% and 50% distributions, respectively. -->
<!-- Predictions of these models, as well as a model based on the prevalence prior, for the average implied prevalence given the ten distributions are shown in Figure\ \@ref(fig:priorManipulationResults)B. -->


```{r brmTable, results="asis"}

load(file = "cached_results/gentInt_manipPriors_brms_contrasts.RData") #rs.brm.priorManip.sdif

df.rs.brm.priorManip.sdif.summ <- data.frame(summary(rs.brm.priorManip.sdif)$fixed)
df.rs.brm.priorManip.sdif.summ$term <- row.names(df.rs.brm.priorManip.sdif.summ)

brm.table1 <- xtable::xtable(df.rs.brm.priorManip.sdif.summ %>%
  mutate(coefficient = factor(term, 
                              levels = c("Intercept", paste("dc", seq(1,9), sep ="")),
                              labels = c("Intercept", 
                                         "0/25 vs. {25/50, 0/50}",
                                         "25/50 vs. 0/50",
                                         "{0/25, 25/50, 0/50} vs. {25/75, uniform}",
                                         "25/75 vs. uniform",
                                         "{25/75, uniform} vs. 0/75",
                                         "0/75 vs. {25/100, 50/100}",
                                         "25/50 vs. 50/100",
                                         "{25/100, 50/100} vs. {75/100, 0/100}",
                                         "75/100 vs. 0/100")),
         prediction = c("", ">", "=", ">", "=", ">", ">", "=", ">", "=")) %>%
      mutate(credible_interval = paste(
      "[", format(l.95..CI, digits = 2), ", ",
      format(u.95..CI, digits = 2), "]", sep =""
    )) %>%
    select(coefficient, prediction, Estimate, credible_interval),
  caption = "Regression model fits for planned comparisons. Predictions are based on whether the 95\\% credible intervals for the generics model prediction overlap.")



print(brm.table1, type="latex", comment = F, table.placement = "H", size="\\fontsize{9pt}{10pt}\\selectfont", include.rownames=FALSE)
```


<!-- The generics model predicts that the *weak or half* and the *rare half* distributions will receive a higher implied prevalence than the *rare weak* distribution. This prediction is also made by the "not rare" model, though that model also predicts that the  *rare half* distribution receives a stronger interpretation that the *weak or half* distribution. As predicted by the generics model, the *weak or half* distribution received significantly stronger interpretations than the *rare weak* distribution ($\beta = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionweak_or_half", "Pr(>|t|)"]]`$) as did the rare or half distribution ($\beta = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast1.summ[["coefficients"]][["distributionrare_half", "Pr(>|t|)"]]`$). -->
<!-- The *rare half* distribution was not significantly different from the *weak or half* distribution ($\beta = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "t value"]]`$, $p = `r rs.lm.priorManip.contrast2.summ[["coefficients"]][["distributionrare_half", "Pr(>|t|)"]]`$). -->
 
<!-- All models predict that the *uniform* distribution will receive higher ratings than both the *weak or half* and the *rare half* distributions.  -->
<!-- The *weak or strong* distribution is also predicted to be greater than *weak or half* or the *rare half*, by all models except the "not absent" model, which predicts no difference for this comparison.  -->
<!-- In fact, both the *uniform* ($\beta = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "t value"]]`$, $p = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionuniform", "Pr(>|t|)"]]`$) and the *weak or strong* ($\beta = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Estimate"]]`$, $SE=`r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Std. Error"]]`$, $t = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "t value"]]`$, $p = `r rs.lm.priorManip.contrast3.summ[["coefficients"]][["distributionweak_or_strong", "Pr(>|t|)"]]`$) distributions received significantly greater implied prevalence than the *weak or half* distributions, providing some evidence against the "not absent" model.  -->

 <!-- [n.s.] -- [weak_or_strong, uniform] vs.  rare_strong -->
<!-- The *rare or strong* distribution was predicted (by most models) to receive stronger interpretations than either the *weak or strong* and/or the *uniform*.  -->
<!-- Contra these predictions, such a difference was not observed in the empirical data -->


<!-- - Some sort of regression tests to see the test the following infleunces: -->
  <!-- - The type of the weaker distribution and the type of the stronger distribution? -->

The computational models we articulate do more than just predict the *mean* implied prevalence; they predict the full distribution of responses.
Again recall for this analysis, there are no free parameters in any of these models.
The model fits in terms of variance-explained and mean squared errors for the full distribution of responses are shown in Table 3.
<!-- Unlike in Expt. 1, it is not obvious which of these models accounts best for the data.  -->
To quantitatively compare the models in their ability to predict the empirical distributions of responses for these ten different conditions, we computed the likelihood of the data under each model by discretizing the empirical distributions and model predictions into ten equally spaced bins.
<!-- \ndg{also report correlation and MSE as before, or say why we don't do so. for that matter, we should say why we didn't do BFs for earlier expts?} -->
As is evident from Figure\ \@ref(fig:priorManipulationResults)A (bottom row, "data"), however, a number of participants responded that 0% of Ks knew when earthquakes were about to happen when they read "Ks know when earthquakes are about to happen". 
Examining these participants' explanations revealed that the majority of these respondents believed they were answering a slightly different question than the one being asked; they believed they were responding to a question like in the prior elicitation task: Predict the next animal. 
These "prior-type" responses are a source of noise in the data and one that affects all models (execept the prior only model) to an equal degree. 
All of the models we articulate have no way to account for 0% responses, which are literally impossible given the utterance of the form "Ks F". 
To compute the likelihood of the complete data set under these models, we make the further assumption that the true generative process of the data is a mixture of responding according to one of the generic interpretation models and responding according to the prior. 
We calculate Bayes Factors assuming a uniform distribution of noise between 0-20% (approximated using the following discretization $= \{0.001, 0.01, 0.05, 0.1, 0.15, 0.20\}$). 
To account for uncertainty in the measurement of the prior, we again bootstrap the model predictions by resampling the prior.
As is shown in Table 3, both the literal and pragmatic generic interpretation models are the best at predicting these data.
Because this experiment was designed to test the causal influence of the prevalence priors and not directly to compare these two models, the data do not definitively distinguish these two models from each other, owing to the relatively small differences in overall predictions for the two experimental conditions.
We conclude that the prevalence prior is causally related to interpretations of novel generics in the way predicted by the generics models.
<!-- Since the models have no free parameters, the ratio of these likelihood values corresponds to the *Bayes Factor* (BF), which quantifies the evidence in favor of one model over another.  -->



<!-- For all noise probabilities, the uncertain threshold model is preferred over each of the fixed threshold models (Table 2 shows the BFs assuming a noise parameter of 0.05).  -->
<!-- In comparison to the prior only model, however, the uncertain threshold is equally as good at predicting the data with a noise parameter of 0.01, and the prior is the preferred model with noise parameters smaller than 0.01.  -->
<!-- This is unsurprising given that the uncertain threshold model cannot predict responses of exactly 0% prevalence, and decreasing the amount of assumed noise further penalizes the model for all the 0% responses. -->
<!-- The important result is that given even a little bit of noise (between 1-5% of responses), the uncertain threshold model is quantitatively the better model. -->

<!-- \ndg{shouldn't we be treating this noise as a param for BDA and integrating it out? (that should be easy if you just treat the noise vals you ran as a discrete approx.) it would simplify the explanation, since no need to talk about different noise levels...} -->

<!-- For all noise probabilities greater than 0.01, the uncertain threshold model is the preferred model by  -->
<!-- The conclusions derived from the Bayes Factors are unchanged under different noise probabilities.    -->

```{r bayes factors, results="asis", cache = F}
#load("cached_results/genint_manipPriors_bf.RData") # model.likelihoodRatios.summarized
load("cached_results/genint_manipPriors_bf_marginalizeWithinIteration.RData") # model.likelihoodRatios.summarized

md.priorManip.summary.table <-left_join(
     left_join(
      md.pm.int.3.filtered.expvals.bs.summary.wide %>%
      group_by(model) %>%
      summarize(`expected value statistics` = paste(
        "r\\textsuperscript{2}","(", n(), ") = ", format(cor(model_prediction, mean)^2, digits = 2),
        "; MSE = ", format(mean((model_prediction - mean)^2), digits = 3), sep = "")
    ),
    md.pm.int.3.filtered.dists.bs.summary.wide %>%
      group_by(model) %>%
      summarize(`full distribution statistics` = paste(
        "r\\textsuperscript{2}","(", n(), ") = ", format(cor(model_mean, data_mean)^2, digits = 2),
        "; MSE = ", format(mean((model_mean - data_mean)^2), digits = 3), sep = "")
    )) %>% ungroup() %>%
    mutate(model = ifelse(model == "fixed_0.0175", "fixed_0.05", model)), 
  model.likelihoodRatios.summarized %>%
      # group_by(alternative_model) %>%
      # summarize(expected_log_bf = log(mean(exp(expected_log_bf))),
      #       lower_log_bf =  log(mean(exp(lower_log_bf))),
      #       upper_log_bf =  log(mean(exp(upper_log_bf)))) %>% 
      ungroup() %>% 
      rename(model = alternative_model) %>%
      mutate(model = ifelse(model == "fixed_0.125", "fixed_0.1", 
                     ifelse(model == "fixed_0.375", "fixed_0.3", model)))
  ) %>%
          filter(model %in% c("literal", "pragmatic_unlifted",
                          "prior", "fixed_0.05", "fixed_0.1", "fixed_0.3")) %>%
    mutate(`Model` = factor(model, levels = c("prior", "fixed_0.05",
                                              "fixed_0.1", "fixed_0.3", "literal",
                                              "pragmatic_unlifted"),
                                    labels = c("Prior", 
                                               "Threshold = 0.01",
                                               "Threshold = 0.1",
                                               "Threshold = 0.35",
                                               "Literal Generic",
                                               "Pragmatic Generic")),
    `log Bayes Factor` = paste(
    round(-expected_log_bf, 1), " (", round(-lower_log_bf,1), ", ", round(-upper_log_bf,1), ")", sep=""
  )) %>% 
    mutate(`log Bayes Factor` = ifelse(Model == "Literal Generic", "0", `log Bayes Factor`)) %>%
    
    select(`Model`, `log Bayes Factor`, `expected value statistics`, `full distribution statistics`)




tab1 <- xtable::xtable(md.priorManip.summary.table[c(6,1,2,3,4, 5), ], 
                       caption = "Summary statistics for alternative generic interpretations. (Log) Bayes Factors quantify evidence in favor of each model in comparison to the uncertain threshold generics model. Numbers less than 0 indicate evidence in favor of the uncertain threshold model. Summary statistics disaplyed for the model fits for the expected values (means) of the distributions and the full (discretized) distributions.")

print(tab1, type="latex", comment = F, table.placement = "H", size="\\fontsize{9pt}{10pt}\\selectfont",
      sanitize.text.function = identity, include.rownames=FALSE)
```


<!-- In addition to the qualitative analyses afforded by the regression analysis, we examine the generic interpretation model's *a priori* quantitative predictions and compare those to a set of alternative models.  -->
<!-- The first alternative model we consider is prevalence prior; we have already seen via the regression analyses that this will not be sufficient.  -->
<!-- We also consider three fixed threshold models which have increasing sophisticating as to their knowledge of the prior distributions: a literal "some" model (fixed threshold at the lowest possible value), a "not absent" model where the fixed threshold is placed in between the "absent" distribution and the "weak distribution" (threshold $\sim 10\%$), and a "highest mode" model where the fixed threshold is placed, in a context-sensitive manner, in between the two component distributions thus always signalling the higher of the two.  -->
<!-- Finally, we have our uncertain threshold model, which integrates the prevalence prior with the pressure from the literal semantics that favors higher prevalences. -->
<!-- Predictions of these models for the ten distributions of interest are shown in \red{Figure X}. -->




# General Discussion

<!-- - is there a way to connect to the claims of Cimpian2009 about generics influencing feature-as-cause vs. feature-as-effect explanations? -->
  <!-- - perhaps not due to the generic vs. specific distiction, but to prior beliefs about the property? -->
  
<!-- Further, the existence of property-sensitive interpretations has been used as evidence against a quantitative account of generic language *per se* and it has been suggested that the normal tools of formal semantics are inappropriate for studying generics [@Leslie2008]. -->
<!-- \mht{not sure if we want to take up the previous point to add to the contributions of this paper} -->

<!-- Much of what we come to learn about the world comes from other people, often expressed in language. -->

Generic language is the foremost case study of how abstract, generalizable knowledge is transmitted in language, and it is commonly believed that generics carry strong interpretations (analogous to the quantifier "most" or "all").
Despite their ubiquity and relative morphosyntactic simplicity, generics have been the examined by few quantitative studies and no quantitative models that make predictions about how generic language updates beliefs.
Here, we explored the hypothesis that a generic statement can be literally thought of as conveying a single positive observation to a listener. 
We showed how this model is mathematically equivalent to the interpretation model assumed by @Tessler2019psychrev and we articulated a more elaborate model which understands generics as intentional utterances.
We tested the models using a diverse set of stimuli, including a replication data set from @Cimpian2010, and showed that the generic interpretations can be predicted with high-quantitative accuracy as a function of background knowledge about the property. 
<!-- Additionally, the model accurately predicted which generic statements would carry the weakest interpretations. -->
Furthermore, we found that a model that interprets generics pragmatically provides a better explanation of the observed data than a literal interpretation model.
<!-- Because generic interpretations are heavily sensitive to background knowledge, generics can carry weak interpretations. -->
<!-- We also find that generics can carry weak interpretations, when background knowledge would make a strong interpretation implausible. -->
<!-- We find reliable fine-grained differences in interpretations across properties, which only our uncertain threshold model can account for. -->
This work, thus, provides the computational formalism necessary to describe how beliefs are updated from generic language.

The fact that the literal meaning of a generic is equivalent to observing a single positive instance of the category with a property suggests a very primitive foundation to generic language. 
Bayesian belief-updating based on observations is a general learning mechanism for any animal \red{(is there some citation for bayes in other animals?)}.
The work here suggests that the human unique component of understanding generic language comes not from understanding
syntax or the meaning of the words but from understanding the communicative intent of the speaker \red{(cite tomasello?)}.
@Csibra2015 argues that individual objects are interpreted by infants as symbolic representations of the object kindm, and thus that it is possible to interpret a pedagogically presented exemplar from a category as a "nonverbal generic".
The pedagogical example acts as an indexical to the category, and @Sterken2015 argues that the context-sensitivity exhibited by indexicals is the exact kind we need to model generics.
The model we present that interprets generics as communicatively relevant single observations is a very natural formalization of the nonverbal generic / indexical view.
Further the fact that an underspecified quantifier semantics is equivalent to an observation model provides an obvious route to learning the belief-updating properties of natural language semantics from more general learning mechanisms. 

<!-- Indeed, @Sterken2015 argues the context-sensitivity of generic language should be understood linguisitically as an indexical.  -->
<!-- The computational model presented here provides the computational unification of these ideas:  -->

<!-- and that interpretation is best modeled as a communicatively relevant observation  -->


<!-- It has been argued that object labels denote kind concepts as soon as a child begins to understand words [@Gelman2004; @WaxmanGelamn2009] -->

Other semantic theories of generics make use of a mechanism by which the domain over which the prevalence of the feature in the category is calculated is contextually restricted [e.g., @Cohen1999; @Declerck1991].
In such a theory, the statement "Morseths have a menstrual cycle" gets evaluated as "Female morseths have a menstrual cycle"; under such an account, the implied prevalence of the feature would be 100% (or near 100%) because 100% of the *relevant lorches* (i.e., the females) have a menstrual cycle.^[
  The scientifically correct term for a "menstrual cycle" is actually an "estrous cycle". We are grateful to one of our MTurk participants for pointing this out to us. 
]
Interestingly, domain restricted interpretations are present in the empirical data.
Several properties about the reproductive behavior of animals (Expt. 2b) have interpretation distributions that are bimodal. 
For example, "Morseths have a menstrual cycle" is interpreted primarily as applying to 50% of morseths (`r sum(with(filter(df.int.filtered, property == "have a menstrual cycle"), ((response < 0.55) & (response > 0.45))))` out of `r length(with(filter(df.int.filtered, property == "have a menstrual cycle"), response))` give a response between 45% and 55%); however, some participants (`r sum(with(filter(df.int.filtered, property == "have a menstrual cycle"), response ==1 ))` of `r length(with(filter(df.int.filtered, property == "have a menstrual cycle"), response))`) rate this as applying to 100% of morseths.
<!-- \ndg{move this to a discussion section:} -->
This behavior could result from a participant interpreting the question as pragmatically restricting the domain to only the members of that category that *could* have the property (e.g., 100% of female morseths have a menstrual cycle).
The domain restricted interpretation is not limited to the generic interpretation data, however.
The bimodality is present in our prevalence elicitation task (Expt. 2a) as well: The distribution over several of the reproductive properties is tri-modal, with peaks at 0% (the animals that don't have a menstrual cycle), 50% ("only females") and 100% (perhaps, "all females").
The fact that this multi-modality appears in the prior elicitation task means that the generic interpretation model predicts bi-modal interpretation distributions for the implied prevalence data, which is what we observe.
Still, the question remains why some participants interpret the noun phrase in generic statements as restricted to a salient subset of the category. 

In what follows, we elaborate the assumptions in our modeling framework, describe how conceptual knowledge can guide prevalence-based interpretations, discuss how our answer to the problem of generic interpretation can inform an answer to the parallel problem of *generic identification*, and speculate about the implications for communicating weak generics.

\ndg{i'm not so sure about including these following discussion sections. they seem like interesting but only thematically related, as apposed to being crucial to our story or responses to probably objections. i didn't comment / edit them yet -- let's first decide if they stay.}


## The relationship between prevalence priors and generic interpretations

Across our three experiments, we measured participants' background knowledge about the property using knowledge elicitation tasks.
These background knowledge elicitation tasks involve reasoning about the properties of alternative categories of animals.
Conveying the instructions and questions of such prior elictation tasks require language. 
Thus, there is the potential for circularity in our argument: We are predicting beliefs updated from language via beliefs measured from language.
Our argument is not circular, however, because of the combination of the nature of the prior elicitation questions and our control quantitative models, which we describe below.

Consider first the nonparametric elicitation task of Expt. 2a. 
Here, we asked participants to generate different animal categories and later rate prevalence for different properties for those animals.
It is possible that participants answer the prevalence question (e.g., "What percentage of dogs develop phobias?") by first asking themselves a generic question ("Do dogs develop phobias?") and then rating their personal generic-interpreted prevalence if the answer to this question was yes and rating 0% prevalence if the answer to the generic question was no.
In this hypothetical, there are only two possibilities for participants' beliefs---the category either does or does not (generically) have the property---and the generic interpretation task reduces to simply reporting back the part of the prior distribution that was generated when participants answered "yes" to their internally-directed generic question ("Do dogs develop phobias?").
If this were the strategy participants employed, however, the most accurate model for the generic interpretation data would be the "some" model, which rules out the 0% component of the distribution. 
The fact that the "some" model consistently underpredicts the generic interpretation data argues against the circularity in our measurements of the prevalence prior and generic interpretations.

<!-- a quantitative study of generic interpretations would reduce based on such a knowledge elicitation would be circular. -->

A similar argument can be made for the structured elicitation task in Expt. 1a.
In Expt. 1, we assumed a particular model of the prevalence prior (i.e., a mixture of Betas) and elicited participants' knowledge about the property using questions that we assumed tapped into this structure. 
One question concerned the probability that a kind would have 0% prevalence of the feature (or, equivalently, the proportion of categories for whom the property is expected to be present at some non-zero level of prevalence) by asking "How likely is it that *there is a K with F*?"
Intuitively, the response to such a question should be that it is very likely for properties that exist in many categories (e.g., *is female*) and very unlikely for properties that only exist in a few categories (e.g., *has purple feathers*).
The second question concerned the shape of the non-zero conditional distribution of prevalence; this was measured by asking "*Suppose there is a K with F*, how many Ks do you think have F?".
This question measures the projectibility of the property given a single, positive exemplar.
Thus, the predictions of the alternative, fixed-threshold "some" model (which rules out 0% prevalence) for this experiment correspond to the inferences upon learning that *at least one member of the category has the property*.
In Expt. 1, we again find the generic interpretation data are distinct from those of the conditional, non-zero distributioun of prevalence, as evidenced by the "some" model comparisons.
The fact that we use this "single, positive observation" question to elicit prior knowledge may in fact have blurred the distinction between the literal and the pragmatic model, which is why we Expt. 2 was conducted. 
<!-- Learning from a generic carries stronger interpretations than just learning the existential information that a property exists in the category. -->
<!-- are derived from the background knowledge in the correct way: They  -->

The patterns of data we observe in our three generic interpretation experiments are additionally unlikely to be identical to those obtained using the quantifier "Some" (i.e., "Some Ks have F").
Though we did not empirically measure judgments given the existentially quantified statements, the alternative model that assumes a fixed-threshold at 0%-prevalence provides insight into what should be expected for those data.^[
  Recall the fixed-threshold models are "literal interpretation" models and do not compute scalar implicatures.
]
Across our three experiments, we found that the "some" model provided too weak of interpretations, consistent with the intuition that generic statements convey stronger prevalence implications that the existential statement.

Another interesting question is whether the same pattern of data could arise from a bonafide pedagogically sampled observation [cf., @Csibra2015]. 
Given the success of the pragmatic model, we suspect this is possible.
We also suspect, however, an interaction with the alternative actions available to the speaker or teacher. 
That is, we do not expect bonafide generic interpretations from pedagogical observations when the speaker could have said a generic. 
Finally, it should be noted that many of the properties used in our experimental stimuli are not directly observable either due to the nature of the property (e.g., "transmit HIV") or to the habituality of the predicate (e.g., "fly into building windows").


\ndg{people may worry about circularity of argument, given rich background knowledge elicitation that also uses language (similar to ellen's point). i think we should discuss this directly. relatedly we should discuss the implications of our structured background knowledge (elicitation and results).}
\mht{does that address it?}

<!-- - Finer points about the data? Discussion of explanation data? Have relationship to DR theories come here? -->

## Conceptual generic interpretations

In this paper, we tested a particular model of generic interpretation that uses a prior distribution over the prevalence of a feature across relevant categories and assumes the truth-functional meaning of the generic is an uncertain threshold on the prevalence. 
The prevalence prior distributions, which we elicit from participants, are likely influenced by the conceptual knowledge of our participants.
For example, the prevalence priors over properties that have to do with reproduction (e.g., *have menstrual cycles*, *get erections*) are multi-modal with one mode around 50%, clearly reflecting participants' biological knowledge that these properties are specific to one sex of the animal.
We thus predict that 4-year-olds, who lack significant biological knowledge [@carey1985conceptual], would derive different interpretations than adults from generics about these properties.


<!-- The uncertain threshold model predicts that generics about these properites should imply roughly 50% have the feature (Figure\ \@ref(fig:modelSimulations)). -->
<!-- Thus, though we do not explicitly model conceptual knowledge *per se*, this behavior is consistent with a richer, conceptual-informed interpretation (e.g., *female morseths have menstrual cycles*). -->

Consistent with the intuition that conceptual knoweldge underlies prevalence judgments, participants appeal to domain knowledge in their explanations for low-prevalence ratings, revealing aspects of their intuitive theories which guide their judgments.
One participant wrote: "About half are females and females have menstrual cycles."
Another: "I assumed that only a percentage of Moxes were female and young enough to have a menstrual cycle."
A typical explanation of a participant's response to *Dobles live in zoos* was "I thought that there would be many more Dobles in the wild than in zoos. This is true of most animals in zoos today".
Here, it is clear that the participant is using their knowledge of the property, derived from other animals, to learn about the category.
Participants often wrote about the lack of an enabling causal situation when describing their weak interpretations: "Most frams don't have access to nicotine."
Explicitly modeling the intuitive theories that give rise to the intuitions about prevalence is an obvious future direction of this work.

<!-- Our model restricts the domain by reasoning about a threshold that would make sense for that particular property, given its background distribution on prevalence.  -->
<!-- Reasoning about an uncertain threshold via background knowledge is one mechanism by which the domain can be restricted. -->


<!-- Indeed, this kind of conceptual knowledge is present in our participants, as evidence by their explanations. -->






<!-- From a purely theoretical perspective, domain restriction is an attractive mechanism because it would allow the semantic theory to be a context-invariant fixed threhsold [e.g., at 50%; @Cohen1999]. -->
<!-- However, such a theory still needs to explain why and formalize how the domain gets restricted; without such a formal account to accompany the domain restriction hypothesis, the hypothesis explains but does not predict the data. -->
<!-- The uncertain threshold model that we present here can be seen as a particular instantiation of a domain restriction theory.  -->
<!-- In our model, the statement "Lorches have a menstrual cycle" is resolved by a literal interpretation model to mean mean "50% of lorches have a menstrual cycle". -->
<!-- Were a listener to have the background theory that the reason why half of a category would have such a property is for reproductive purposes, the listener's interpretation would most naturally be expressed as "Female lorches have a menstrual cycle". -->


## Generic identification
\mht{i think this should be cut... unless you think there is something here?}
Though often interpreted as generics, bare plural sentences (e.g., "Dogs bark") can also manifest as *non-generics* (e.g., "Dogs are on my front lawn").
Figuring out when a sentence expresses a generic meaning vs. a non-generic meaning is called the problem of generic identification and is a parallel challenge to the problem of generic meaning [@Carlson1995].
Our answer to the question "what do generics mean?" involves vagueness concerning the prevalence, which recruits beliefs about the property in question.
The properties themselves may involve vagueness in terms of genericity, or its event counterpart *habituality*, such as *barks*: How often does an individual dog need to bark in order to qualify as a thing that barks?
@Tessler2019psychrev showed that the same uncertain threshold mechanism explains graded endorsements of habitual statements like "John's dog barks".

The habituality of the predicate may actually be the source of the perceived generic/non-generic distinction.
For instance, "Dogs are on my front lawn" exhibits only a single generalization (e.g., over *dogs*); it is not a habitual predicate like *barks*.
The habituality of the predicate may influence the expected prevalence in the head noun category: A more vague, habitual predicate may be applied by human observers to more instances of the category because it is harder to verify than static properties.
For non-habitual predicates, the expected prevalence would be substantially more restricted.
Thus, we may find continuity in interpretations of underspecified noun phrases, including so-called "non-generics": "Tigers have stripes" has higher implied prevalence than "Tigers are in the savanna" which is higher than "Tigers are in zoos", "Tigers are in my local zoo", and "Tigers are on my front lawn".
An uncertain threshold mechanism of the kind we have posited here may be such a way as to pragmatically infer a continuum of interpretations, from generics that are interpreted as universals ("Tigers are mammals") to those that are interpreted existentially but habitually ("Tigers are in zoos") to those that are interpreted existentially and non-habitually ("Tigers are on my front lawn").
Deriving such a prediction would necessarily involve a model of compositional generic interpretation, in which a habitual predicate meaning can be resolved jointly with a generic subject.

<!-- Interpreting the uncertain threshold as a kind of domain restriction might be sufficient to extend the model to interpreting "non-generic" bare plurals (e.g., "Dogs are on my front lawn"). -->

<!-- First, let us note that many generics actually expressed two or more generalizations.  -->
<!-- For example, "Lorches eat insects" says that $[gen \text{ lorches}] [hab \text{ eat}] [gen \text{ insects}]$. -->
<!-- <!-- In this paper, we have looked at the first aspect of generic understanding, the generalization over the head noun category (e.g., lorches) though in previous work we have shown that the same uncertain threshold model can explain habitual language as well [e.g., "John smokes"; @Tessler2019psychrev]. -->
<!-- In this paper, we have looked at the first aspect of generic understanding, the generalization over the head noun category (e.g., lorches) though in the next chapter we will see that the same uncertain threshold model can explain habitual language as well (e.g., "John smokes"). -->

<!-- An old intuition with generics is that at least some express properties that are especially timeless and enduring [@Lyons1977; @Gelman2003; @Gelman2007], though it is not clear what generalization this should be attributed to.  -->



## Implications for stereotype propogation

The fact that understanding generics relies heavily on background knowledge about properties provides a clear avenue for miscommunication or deceit when using generics. 
A speaker might endorse a generic when the prevalence is quite low [see @Tessler2019psychrev] believing that they and the listener share the background assumptions that the property is a low-prevalence property in general. 
The extent to which a listener has a different set of assumptions of the distribution of the property will result in miscommunication. 
For many properties, the difference in implied prevalence may be inconsequential. 
For features of social categories, which have difficult-to-understand statistical properties \red{(cite something about how people misunderstand the distributions of properties within social categories?)}, the misalignment of background believes can lead to the propogation of stereotypes. 

The experimental data we present provides strong evidence for the existence of "weak generics", which we operationalize here as generic statements that imply a prevalence less than 50%. 
Statements like "Lorches live in zoos", "Feps perform in the circus", and "Wugs have seizures" all received average implied prevalence ratings below 50%, with many individual participants providing ratings of 10% or less (Expt. 2b).
When manipulating the prevalence prior distributions to suggest only prevalence levels of around 0% or aroud 25%, participants readily interpret the statement "Ks know when earthquakes are about to happen" to mean about 25% (Expt. 3b). 
This is an aspect of generic language understanding often swept under the rug: Generics are believed (in a generic sense) to carry strong implications.
Here, we show that generics also carry weak implications.

Adults flexibly use their causal knowledge of the world to restrict the interpretation of a generic statement to a reasonable population, as evidenced by their explanations (e.g., "I thought that there would be many more Dobles in the wild than in zoos").
At the same time, the communicative force of a generic encodes a pressure for higher prevalence interpretations (e.g., without strong prior knowledge, generics imply high prevalence).
Thus, listeners who do not have the substantial prior knowledge to restrict an interpretation of generics to a weak interpretation (e.g., young children) might derive a high-prevalence interpretation and furthermore posit abstract relations that would make a high-prevalence interpretation more likely.
For example, upon learning "Dobles live in zoos", a naive learner might infer that there is something about Dobles that causes them to live in zoos [cf., @Prasada2013]. 
This is potentially problematic for language about social categories (e.g., "Tall people are good at basketball") for which an adult speaker might wish to convey a relatively low prevalence interpretation (e.g., *some tall people...* because most people are not good at basketball) but which a naive listener might derive a higher prevalence interpretation.
This points to an aspect of generic meaning that might differ between adults and children: Whether or not children understand weak generics.

<!-- It is often argued that children have adult-like understanding of generics from a very young age . -->
<!-- These studies often look at a small range of properties that do not exhibit much context-sensitivity.  -->
<!-- It is an open-question whether or not children understand "weak generics". -->

<!-- There are theoretical reasons to think that the understanding of weak generics may be later developing.  -->
<!-- Prevalence priors are naturally modeled as Beta distributions, which are probability distributions over numbers ranging from 0 - 1: In other words, Beta distributions are probability distributions over probabilities.  -->
<!-- These probabilities can be thought of as the weight of a biased coin, and a common interpretation of the parameters of the Beta distribution are as *pseudo-counts*, imaginary outcomes of a previous experiment of flipping the coin. -->
<!-- For example, a Beta(1, 9) distribution describes beliefs about the weight of the coin upon observing 1+9 = 10 flips of that coin, 1 of which landed on heads and 9 of which landed on tails.  -->
<!-- This probability distribution heavily favors numbers around 0.1, as a coin of that weight is most likely to have generated the observed 1 out of 10 heads.  -->

<!-- If we take the pseudo-counts as indices of worldly experience, it is interesting to note what happens as the pseudo-counts approach 0.  -->
<!-- As the parameters approach zero, the Beta probability distribution increasi -->
<!-- ngly prefers extreme values: 0 or 1.  -->
<!-- If our knowledge about predictive probabilities are represented by Beta distributions, then early on, our belief distributions would favor categorical thinking.  -->
<!-- However, once we start to learn about different kinds of categories and properties, probably via labeling, we may begin to represent multiple distributions in our prevalence priors.  -->
<!-- Such a development would predict that children start out interpreting all generics as strong generics, and that the understanding of weak generics is later developing ability. -->





<!-- ## Implications for social categories -->


<!--   - since properties tend to be rare of objects (e.g., Oaksford & Chater wason), false-positives are not as bad as false-negatives (from a purely informational standpoint, because false-negatives are less likely? because most things are going to be correct rejections?) -->
<!--   - howver, for social categories this can be bad, as social categories are socially-constructed, reflecting what the speaker believes is a relevant categorization to make (Foster-Hansen, maybe Steve Roberts?) -->
<!-- - does this *strong interpretations before weak interpretations* relate at all to Gelman & Bloom's developmental changes? -->

## Conclusion

An easy way to acquire abstract knowledge is to interpret a novel generic statement but heretofore no quantitative models have accounted for this belief updating processs.
A computational model that understands generics as conveying communicatively relevant observations is able to predict the large variability in generic interpretations, even predicting the existence of "weak generics", or generics that only apply to a minority of the category.
This work opens the door to understanding how abstract knowledge is learned from language.

\newpage

# References 

<!-- \setlength{\parindent}{-0.1in}  -->
<!-- \setlength{\leftskip}{0.125in} -->
<!-- \noindent -->

<div id = 'refs'></div>

\newpage

```{r child = 'appendix_items.Rmd'}
```

\newpage

```{r child = 'appendix_cimpianMethods.Rmd', eval = F}
```
