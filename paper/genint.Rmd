---
title             : "Context-sensitive interpretations of generic statements"
shorttitle        : "Context-sensitive generic interpretations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mhtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  
author_note: >
  This manuscript is currently in prep. Comments or suggestions should be directed to MH Tessler.

abstract: |
  Enter abstract here. Each new line herein must be indented, like this line.
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---
\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}

\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}
\newcommand{\ndg}[1]{{\textcolor{Green}{[ndg: #1]}}}
\newcommand{\red}[1]{{\textcolor{Red}{#1}}}


```{r load_packages, include = FALSE}
library("papaja")
```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Introduction

Learning from others comes in many forms. 
An expert may tolerate onlookers, a demonstrator may slow down when completing a particularly challenging part of the task, and a teacher may actively provide pedagogical examples and describe them with language [@Kline2014; @Csibra2009]. 
Informative demonstrations may be particularly useful for procedural learning (e.g., hunting seals, riding a bicycle). 
Language, however, is uniquely powerful in its ability to convey information that is abstract or difficult to observe, or information that otherwise does not have a way of being safely acquired [e.g., learning that certain plants are poisonous, staring at the sun causes blindness; @Gelman2009]. 

The premier example of language's ability to transmit abstract knowledge comes in the form of statements that convey generalizations, otherwise known as *generic language* [e.g., "Birds fly"; @Carlson1977]. 
One of the most important roles for generic language is to provide learners with information about new or poorly understood categories.
As such, generics are ubiquitous in everyday conversation and child-directed speech [@Gelman1998].
But unlike statements about concrete events or exemplars  (e.g., "This bird flys"), generics convey information that is not directly observable and, in fact, resilient to counterexamples (e.g., penguins do not fly). 

Understanding how generic language gets interpreted is an open question in cognitive science.
Intuitively, interpretations of generics map best onto the quantifier *most* (e.g., "Most birds fly").
But interpreting all generic statements as meaning *most* is too strong: "Mosquitos carry malaria" really should be taken to mean *some* (i.e., "Some mosquitos carry malaria").
Interpreting a generic as *most* can also be too weak: "Triangles have three sides" is a statement about *all triangles*; if a shape does not have three sides, it is not a triangle. 
Indeed, the differences between quantified statements and generic statements are appreciated by children as yougn as three years [@Gelman2015genericsQuantifiers].

The flexibility of generic language can be in part attributed to beliefs about the property [@Nisbett1983].
Properties that are biological in nature (e.g., "Wugs have wings") are interpreted as applying to most or all of the category [@Gelman2002; @Brandone2014], while features that could be construed as incidental (e.g., "Crullets have fungus-covered claws") are interpreted as applying to relatively fewer members of the category [@Cimpian2010].
These context-sensitive interpretation have been explained as theory-based expectations [@Leslie2008; @Cimpian2010theory; @Prasada2013], but such notions have not been made sufficiently precise to make a quantative predictive account.

@TesslerLangGenUnderReview proposed a quantitative model of generic endorsement, which uses a threshold function to update a listener's prior beliefs from a generic statement.
This model accounted for truth judgments (e.g., is "Mosquitos cary malaria" true or false?) about a wide range of linguistic stimuli including generics about animals, habitual statements about actions (e.g., "John runs"), and causal language (e.g., "This herb makes animals sleepy").
Their endorsement model is cast a speaker deciding whether or not to assert a generic statement to a naive listener.
<!-- can be accounted for well by a "speaker model" that can transmit information to the generic listener model.  -->
The predictions of the generic listener model, which provides a mapping from a generic statement to the prevalence implied by that statement, have not been tested.

The paper is organized as follows.
First, we review @TesslerLangGenUnderReview's computational model of generic interpretation. 
In Expt. 1, we replicate and extend the basic findings of @Cimpian2010 that biological and accidental properties receive differential interpretations. 
In Expt. 2, we extend these findings using a larger and more diverse stimulus set better able to test the quantitative predictions of our model.
In Expt. 3, we experimentally manipulate the background knowledge (the prevalence prior) and test its effect on interpretations of novel generics.
In each experiment, we compare our model's predictions to alternative models, finding that our quantitative model is better able to acconut for context-sensitive generic interpretations. 
Together, these results provide strong evidence for an quantitative account of generic language.

<!-- First, we test whether or not weak interpretations of generics are possible by testing interpretations of generics about a wide range of properties (Expt. 1a).  -->
<!-- Then, we measure interlocutors' prior beliefs about the prevalence of the property (Expt. 1b) using the prior elicitation paradigm in @TesslerGenerics.  -->
<!-- We compare our interpretation model's predictions, which use the empirically measured priors, to the implied prevalence data and find that our model is able to predict, with high quantitative accuracy, the context-sensitive interpretations of generics.  -->
<!-- Finally, we show that, rather than being merely correlationa in nature, the prevalence prior is causally related to generic interpretation by manipulating the prevalence prior and measuring participants interpretaions of novel generics (Expt. 2).  -->


<!-- Despite these variable truth conditions, generic sentences are thought to have charasterically "strong implications" [@Gelman2002; @Cimpian2010; @Brandone2014]. -->
<!-- It is these strong interpretations that make generics lead to stereotypes [@Rhodes2012]. -->

<!-- We recently proposed that generics communicate in an underspecified way about the prevalence of the feature. -->
<!-- That is, our model predicted that "Mosquitos carry malaria" is a felicitious utterance because listeners expect *carrying malaria* to not be widespread within a category, even when it is present in some of the category.  -->

<!-- @Cimpian2010 too found that generics predicating biological properties of kinds carry strong interpretations.  -->
<!-- The properties used in those studies were all related to body parts of animals (e.g., "Lorches have purple feathers" $\rightarrow$ *almost all lorches have purple feathers*).  -->
<!-- Bare plural nouns predicated with *accidental* properties (e.g., "Lorches have broken legs") had significantly weaker interpretations in terms of how many of the kind were expected to have the property.  -->
<!-- However, these accidental properties do not lend themselves to generic interpretation and sound infelicitous.  -->
<!-- At the same time, the average implied prevalence for the *accidental property* generics was close to 70\%, leaving open the question as to whether or not generics can have truly weak (i.e., minority-based) interpretations. -->


# Computational model

Generic language conveys generalizations about categories [@Carlson1977; @Leslie2008].
Given that probability is a useful representation for human generalization from observational data [@Shepard1987; @Tenenbaum2011], it makes sense that probability would be at the core meaning of generalizations from language. 
@TesslerLangGenUnderReview formalize such a hypothesis, positing that the semantics of generics is simple but underspecified or vague. 

Using the truth-functional tools of formal semantics [@Montague1973], the literal meaning of a generic statement can be modeled as a threshold function operating on the prevalence $p$ of the feature in the category (e.g., the proportion of mosquitos that carry malaria): $\denote{generic} = \{p > \theta\}$.
The literal meaning of quantifiers can also be described by a threshold-function (e.g., $\denote{some} = \{p > 0\}$, $\denote{most} = \{p > 0.5\}$, $\denote{all} = \{p = 1\}$).
@TesslerLangGenUnderReview posit that the corresponding threshold for the generic $\theta$ is *a priori* uncertain---formally, is drawn from a context-invariant, uniform prior distribution thresholds $P(\theta)$---and is resolved by context. 

In order for a statement to update beliefs about prevalence, a listener must have prior beliefs about prevalence.
To interpret a generic sentence, a listener draws upon abstract knowledge about properties, formalized by a prior distribution over likely prevalence levels $P(p)$. ^[
  Though not the direct focus of this work, the prevalence prior distribution can be seen as a relatively shallow formalization of theory-based expectations. 
  That is, different theory-based relationships between kinds and properties will give rise different prior distributions over prevalence. 
  The prevalence prior, thus, acts as a statistical layer between conceptual knowledge and natural language semantics. 
].
Thus the quantitative model for generic interpretation is given by:

\begin{eqnarray}
L(p, \theta \mid u) &\propto& {\delta_{\denote{u}(p, \theta)} \cdot P(p) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

Formally, the truth-functional meaning is represented by the Kronecker delta function  $\delta_{\denote{u}(p, \theta)}$ that returns probabilities proportional to $1$ when the utterance is true (i.e., when $p > \theta$) and $0$ otherwise.

\begin{eqnarray}
\delta_{\denote{u_{gen}}(p, \theta)} &\propto  & \begin{cases}
1 & \text{if } p > \theta \\
0 & \text{otherwise}
\end{cases}\label{eq:delta}
\end{eqnarray}

\red{Schematic model predictions are shown in Figure X. }

\red{Explain priors as a distribution over animals.}

<!-- 
- Figure:
  - 3 schematic priors, different fixed thresholds, generic interpretation 
  - possibly with bar plot summarizing means
  
- Figure:
  - comparison to a fixed threshold ("some") model, fixed threshold ("most") model, and L1 model (?)
-->

# Experiment 1: Replication and extension of Cimpian et al. (2010)

Our model of generic interpretation (Eq.~\ref{eq:L0}) predicts that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
@Cimpian2010 found a difference in the implied prevalence between biological properties (e.g., *yellow fur*) and accidental properties (e.g., *fungus-covered claws*).
Classic work in generalization suggests beliefs about the prevalence of properties include relatively fine distinctions among properties that are all biological in nature [@Nisbett1983].
For this reason, we elaborated the stimulus set from @Cimpian2010 to include three types of biological properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*) and body parts described with a vague adjective (e.g., *curly fur*). 
Here, we empirically measure the prevalence priors using a structured prior elicitation task (Expt. 1a) and use our model to predict the prevalence implied by a generic statement about a novel category (e.g., "Wugs have yellow fur"; Expt. 1b). 

<!-- %We also coded the accidental properties from Expt.~2a as either ``common'' or ``rare'' using a by-item median split based on \emph{a priori} expected prevalence when present. -->
<!-- %Most of the materials we used were from \citeauthor{Cimpian2010}.  -->
<!-- %The materials used were 30 novel animal categories (e.g. lorches, morseths, blins) each paired with a unique property.  -->
<!-- %Biological properties were made by pairing a color with a body-part (e.g. purple feathers, orange tails).  -->
<!-- %Accidental properties used the same set of body-parts but modified it with an adjective describing an accidental or disease state (e.g. broken legs, wet fur).  -->
<!-- %Each participant saw a random subset of 10 unique animal-property pairs for each type of property (biological and accidental).  -->


## Experiment 1a: Prior elicitation


### Method

#### Participants

We recruited 40 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk). We chose this number of participants based on intuition from similar experiments which were designed primarily to test a quantitative model.
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
All participants were native English speakers. 
The experiment took about 5-7 minutes and participants were compensated \$0.75.

#### Procedure and materials

We constructed a stimulus set of forty different properties to explore a wide range of *a priori* beliefs about prevalence. 
These items make up four categories of properties: body parts (e.g., *fur*), body parts of a particular color (e.g., *yellow fur*), body parts described with a vague adjective (e.g., *curly fur*, and body parts with in an accidental or disease state (e.g., *wet fur*).
Because pilot testing revealed more variability for items in the accidental category relative to the other types of properties, we used twice as many exemplars of accidental properties, yielding a more thorough test of the quantitative predictive power of the generic interpretation model. 
We used 8 exemplars of each of the three non-accidental properties (parts, colored parts, vague parts), and 16 exemplars of accidental properties, building on a stimulus set from @Cimpian2010.
All materials are shown in Table X of the Appendix.

In previous work, we have found that prevalence priors are well-modeled by a mixture of two Beta distributions. ^[
  The Beta distribution is chosen because the support of this distribution is numerical values between 0 - 1, which is the appropriate support for a distribution over prevalence.
]
One of these distributions represents kinds of animals who *do not have* a stable causal mechanism that could give rise to the property (e.g., *lions* and *lay eggs*), which results in prevalence or prevalence values close to or equal to 0.^[
  This assumption is similar in spirit to that employed by *Hurdle Models* of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., when modeling adverse reactions to vaccines; @hurdleModels]
]
This "null distribution" is potentially present for all features in exactly the same way (i.e., the lack of producing the feature).
The second distribution represents kinds of animals who *do have* such a mechanism, and the two parameters of this distribution are not specified *a priori* and are not the same for all properties.
Properties may also vary in the relative contribution of these two component distributions.

In this experiment, we used a two-stage structured, elicitation procedure, aimed to measure the two relevant components of the structured prior model: (1) the relative contributions of null prevalence distribution and stable prevalence distribution (i.e., the mixture parameter, or the *potential for a property to be present*), and (2) prevalence among kinds where the property is present (*prevalence when present*). 
Participants were first introduced to a "data-collection robot" that was tasked with learning about properties of animals. 
Participants were told the robot randomly selected an animal from its memory to ask the participant about (e.g., The robot says: "We recently discovered animals called feps."). 
To measure the *potential to be present* (1), the robot asked how likely it was that there was a fep with *property* (e.g., "How likely is it that there is a fep that has wings?"), to which participants reported on a scale from *unlikely* to *likely*.
To measure *prevalence when present* (2), the robot then asked the likely prevalence assuming that at least one has the property (e.g., "Suppose there is a fep that has wings. What percentage of feps do you think have wings?"). 

Participants completed a practice trial using the property *are female* to make sure they understood the meanings of these two questions.
For example, it is very likely that there is a fep that is female because almost all animals have female members (high potential to be present).
Additionally, when present, the property is only expected in about 50\% of the category.

<!-- potential of the property to be present in a kind and  -->

<!-- In order to integrate the results of this prior measurement task into our model of endorsement, we must accurately model the prior elicitation data. -->
<!-- We approximate the prevalence distribution for each property (e.g., \textsc{lay eggs}) with a Mixture of Betas model, which assumes that the data generated for each kind comes from one of two underlying Beta distributions. -->
<!-- ] -->
<!-- The *Mixture of Betas* distribution has a third free parameter (for each property), the relative contribution of the "null distribution" (for example: we expect the "null distribution" to not contribute at all to properties like *being female*, for which almost all categories have at least some members with the property). -->
<!-- To ensure the *Mixture of Betas* model of the prior is not overly complex, we fit an additional model that represents only a single underlying distribution (*Single Beta*) for a comparison. -->
<!-- <!-- is flexible and can represent different shapes for different properties (e.g., the differences in prevalence judgments for \textsc{lay eggs} vs. \text{has wings}). --> -->
<!-- For more details about model implementation and inference, see Appendix C.  -->


<!-- %responded using slider bars for each question. -->
<!-- % -->
<!-- %$P(x)$ was measured empirically ($n=40$, {\it Experiment 2a}), and the most likely priors were inferred using the same structured, statistical approach used for the familiar generics experiment. -->

### Data analysis and results

Question 1 elicits the potential for a property to be present in a kind (the mixture parameter of a mixture of 2 Betas model). 
We assume responses are generated from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma_1, \xi_1)$. 
Question 2 elicits the prevalence in a kind where the property is present.
We also model these responses are coming frmo a Betas distribution: $d_{2} \sim \text{Beta}(\gamma_2, \xi_2)$.
Each item was modeled independently for each gender.
We learned about the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burn-in.

The priors elicited cover a range of possible parameter values as intended (Figure \@ref(fig:priorScatter), scatter):
\red{describe scatter plot}.

To generate prevalence prior distributions, we built a Bayesian mixture-model for this prior elicitation task, analogous to that used in Case Study 1 (Expt. 1b).
The only difference is that we estimate the mixture component $\phi$ directly from responses to Question 1. 
We assume that those who have not done the action before will probably not do the action in the future. 
With these assumptions, the prevalence distribution is given by:

\begin{align}
\phi & \sim \text{Beta}(\gamma_{Q1}, \xi_{Q1}) \nonumber \\ 
\ln h & \sim \begin{cases}
		\text{Gaussian}(\mu_{Q2}, \sigma_{Q2}) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{T} \label{eq:priorModel}  \\
				\text{Delta}(h=0.01) &\mbox{if } \text{Bernoulli}(\phi) = \textsc{F} \\
		\end{cases}
\end{align}

Figure \@ref(fig:habituals-prior-figure) (right) shows example reconstructed priors.
In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior resolves two technical difficulties.
First, it smooths effects that are clearly results of the response format. 
For example, a very common rating for certain events is \emph{1 time per year}.
Presumably participants would be just as happy reporting \emph{approximately} 1 time per year (e.g., on average, 1.2 times per year); the raw data does not reflect this due to demands of the dependent measure.
Second, this methodology better captures the tails of the prior distribution (i.e., very frequent or very infrequent rates) which have relatively little data and need to be regularized by the analysis.





We used the same structured, statistical model for the prior data from Expt.~1.
The only difference from Expt.~1a. is that our experimental data comes from inquiring about the parameters of the priors directly, as opposed to asking about particular samples from the prior (i.e. particular kinds) as was done in Expt.~1a. 
<!-- %For Expt.~2a, participants are asked questions directly targeting $\theta$ and $\gamma$ in the above model (see {\it Expt. 2a}). -->
We assume these two measurements follow Beta distributions ($d_{potential} \sim \text{Beta}(\gamma_{1}, \xi_{1})$; $
d_{expected} \sim \text{Beta}(\gamma_{2}, \xi_{2})$), and construct single prevalence distributions, $P(x)$, by sampling from the posterior predictive distribution of prevalence as we did before: $P(x) = \int [ \phi\cdot \text{Beta} (x \mid \gamma_{2}, \xi_{2}) + (1 -  \phi) \cdot \delta_{x=0} ] \cdot \text{Beta}(\phi \mid \gamma_{1}, \xi_{1}) d\phi$.
We used the same uninformative priors over parameters $\phi, \gamma_{i}, \xi_{i}$ as in Expt.~1a.

Figure \ref{fig:prior2}a shows a summary of the elicited priors, in terms of the diversity of $d_{potential}$ and $d_{expected}$.
Biological properties are expected to be \emph{a priori} more prevalent within a kind when present than accidental properties, with additional fine-grained differences within biological and accidental properties.
Like the priors elicited using familiar categories, these priors elicited using unfamiliar categories have diverse shapes (see insets). 
Biological properties (``biological'', ``vague'', and ``color'' body parts) have prevalence distributions that are bimodal with peaks at 0\% and near-100\% prevalence. 
Interpretations of generics about these properties ($L_1$ model, Eq.~\ref{eq:L1}) update these distributions to concave posteriors peaked at 100\% (Figure \ref{fig:prior2}a; red, blue and green insets); the model predicts these novel generics will be interpreted as implying the property is widespread in the category.
By contrast, accidental properties (both ``rare'' and ``common'') follow unimodal prior distributions and update to convex posterior distributions, predicting weaker and more variable interpretations of novel generics for these properties. 
<!-- %These convex posteriors show that generics about accidental or temporary properties come with highly uncertain interpretations, plausibly as a consequence of theory-driven considerations \cite{Cimpian2010c}.  -->


<!-- \begin{figure*} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{prevalence-implied-wPriors} -->
<!--     \caption{Understanding novel generics. (a) Prevalence prior distributions empirically elicited for 40 animal properties. -->
<!--     Parameters of the structured statistical model---$\phi$ and $\gamma$---reveal quantitative differences in beliefs about the prevalence of conceptually different types of properties (scatterplot).  -->
<!--     Inset plots show differences in shapes between biological properties (red, green, blue; bimodal) and accidental properties (orange, purple; unimodal).    -->
<!--   These differences in the prior (darker shade) give rise to the variability of $L_1$ interpretations of generic utterances (lighter shade). -->
<!--   (b) -->
<!--   Human interpretation of prevalence upon hearing a generic compared with the $L_1$ model posterior predictive.  -->
<!--     Participants and the model interpret generics differently for different property types: Generics of biological properties (red, blue, green) have  strong interpretations while generics of accidental properties (purple, orange) are weaker.  -->
<!--       Error bars denote Bayesian 95\% credible intervals. -->
<!--   } -->
<!--   \label{fig:prior2} -->
<!-- \end{figure*} -->







## Experiment 1b: Generic interpretation


<!-- %The full cover story is described in {\it SI Section C} and is the same for Expt.~2c. -->


\subsubsection*{Method}

\paragraph*{Participants}

We recruited 40 participants over MTurk to determine how widespread different properties are believed to be upon hearing a novel generic.  
The experimental design is very similar to \citeA{Cimpian2010}, and we chose to have a sample size at least twice as large as the original study (original n=15). 
%This is a quantitative experiment with only quantitative comparisons planned.
%Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

\paragraph*{Procedure and materials}

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with the generic (e.g., ``Feps have yellow fur.'') and asked to judge prevalence: ``What percentage of feps do you think have yellow fur?''. 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental (described in Expt.~2a).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 


<!-- One of the most important roles for generic language is to provide learners information about new or poorly understood categories.  -->
<!-- This role depends on how unfamiliar generic sentences are interpreted \cite<e.g.>{Gelman2002, Cimpian2010}. -->
<!-- The pragmatic theory we present includes such a theory of generic comprehension: the listener model (Eq.~\ref{eq:L1}) describes interpretation of a generic utterance---\emph{Kind \textsc{has property}}---without previously knowing the prevalence of the property within this kind. -->
<!-- In our theory, the meaning is uncertain, but the pressure to be informative operates over \emph{a priori} beliefs about properties to produce an interpretation.  -->
<!-- Classic work in generalization suggests beliefs about the prevalence of properties differ by type of property, including relatively fine distinctions among properties that are all biological in nature \cite{Nisbett1983}.  -->
<!-- We leverage these diverse expectations, using properties that explore a wide range of \emph{a priori} beliefs about prevalence.  -->

Measuring \emph{a priori} beliefs is tricky when the kinds are unknown.
We cannot, as before, have participants fill out a table with rows corresponding to different animal kinds and columns corresponding to different properties:  Nothing would distinguish the rows.
Instead, we leverage the latent structure uncovered in our extended model analysis of Expt.~1 and decompose prevalence priors into 2 components: the property's potential to be present in a kind and the mean prevalence when present.

We use this novel method for measuring \emph{a priori} beliefs about the prevalence of these properties for unfamiliar kinds (Expt.~2a).
We then test the predictions of the pragmatic listener model $L_1$ using these empirically derived priors against human \emph{interpretations} of novel generic sentences (Expt.~2b).
Finally, we explain a previously reported empirical asymmetry between truth conditions and interpretations by comparing the speaker $S_2$ and listener $L_1$ models in the same experimental context (Expt.~2c).


\subsubsection*{Analysis and results}

The pragmatic listener $L_1$ model provides posterior beliefs about prevalence, given prior beliefs and a generic utterance.
This model has one parameter governing the optimality of the hypothetical speaker $S_1$ in Eq.~2. 
We put the same uninformative prior over this parameter as previously: $\lambda_1 \sim \text{Uniform}(0, 20)$.
We learned about the parameter's \emph{a posteriori} credible values by running 3 MCMC chains of 100,000 samples (removing 50,000 for burn-in) using the Metropolis-Hastings algorithm.
The MAP and 95\% credible interval for $\lambda_1$ are 14.8 [6.4, 19.9].

We look at the posterior predictive distribution of $L_1$, integrating out the model parameter.
We first explore two important trends predicted by the pragmatic listener model.
In Figure \ref{fig:exp2b} (solid lines) we see the implied prevalence judgments are predicted (at the property class level) to vary linearly with the \emph{a proiri} expected prevalence. 
A mixed-effects linear model with random by-participant effects of intercept and slope indeed reveals the more prevalent a property is expected to be \emph{a priori}, the stronger the implications of a generic statement ($\beta = 0.57; SE = 0.08; t(39) = 7.12; p < 0.001$).
The prevalence implied by a generic is also predicted to be greater than the \emph{a proiri} expected prevalence (i.e., greater than the prevalence expected among the kinds with the potential to have the property).
A mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the \emph{a priori} prevalence ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$).
%The generic thus does more than simply signal the category has the property; it carries with it the communicative weight of a speech-act, and implies a prevalence even higher than one would infer just by generalizing based on instances.
As for the quantitative accuracy of the model, on a by-item level, the pragmatic listener model predictions closely align with the human judgments of prevalence for novel generics ($r^2(40)=0.94$, MSE=0.002).
Human participants and our model display the same sensitivity of generic interpretation to details of the property (Figure \ref{fig:prior2}b). 
We now have strong support for both of the major predictive components of our model: generic endorsement, modeled as a speaker $S_2$, and generic interpretation, modeled as a listener $L_1$.


<!-- %\ndg{i don't really understand this paragraph...} -->
<!-- %To understand more fully how the model makes these predictions, we performed two exploratory analyses to test whether or not: -->
<!-- %(1) implied prevalence judgments varied linearly with the \emph{a proiri} expected prevalence; -->
<!-- %and (2) the prevalence implied by a generic is greater than the \emph{a proiri} expected prevalence. -->
<!-- %Both exploratory analysis returned confirmatory evidence.  -->
<!-- %A mixed-effects linear model with random by-participant effects of intercept and slope reveals the more widespread a property is expected to be \emph{a priori}, the stronger the implications of a generic statement ($\beta = 0.57; SE = 0.08; t(39) = 7.12; p < 0.001$, providing evidence for (1). -->
<!-- %A mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the \emph{a priori} prevalence ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$), providing evidence for (2). -->
<!-- %The generic does more than simply signal the category has the property; it carries with it the communicative weight of a speech-act, and implies a prevalence even higher than one would infer just by generalizing based on instances. -->


<!-- % We subjected the human prevalence judgments to a mixed-effects  -->
<!-- % -->
<!-- %Human prevalence judgments after reading the generic were affected by the type of property and its corresponding mean prevalence when present )\footnote{These statistics are the result of a mixed-effects linear regression with a maximal mixed-effect structure: Random by-participant effects of intercept and slope}.  -->
<!-- %We compared participants judgments to interpretations of the pragmatic listener $L_1$ (Eq.~\ref{eq:L1})) about the likely prevalence of the property after hearing a generic about an unfamiliar kind (e.g. \emph{Lorches have green feathers.}).  -->

<!-- %, in the same spirit as \cite{Gelman2002}.  -->



<!-- %The pragmatic listener in Eq.~\ref{eq:L1} is sensitive to the property in question and its corresponding distribution on prevalence. -->
<!-- %type of property (and its corresponding prior distribution on prevalence) when interpreting a novel generic. -->





<!-- %Particularly, the \emph{a priori} mean conditional prevalence will guide interpretation as it describes the distribution assuming the property is present. \ndg{why?} -->
<!-- %Again, $P(x)$ was measured empirically ($n=40$, see Supplement Section C). -->
<!-- %The five property types fell on a continuum of \emph{a priori} mean conditional prevalence (Figure \ref{fig:prior2}; x-axis).  -->
<!-- %Biological properties are expected \emph{a piori} to be more prevalent within a kind than accidental properties, with fine-grained differences even among types of biological and accidental properties. -->
<!-- %For instance, within a given kind, colored body parts (e.g. \textsc{green wings}) are expected \emph{a priori} to be less prevalent than some gradable adjectives (e.g. \textsc{small wings}).  -->
<!-- %Some accidental properties are expected to be relatively more prevalent \emph{a priori} than others (``common accidental'' vs. ``rare accidental''; e.g. \textsc{wet fur} vs. \textsc{broken legs}; Figure \ref{fig:prior2}, orange and purple plots). -->

<!-- %These distributions are not necessarily peaked at 100\%, and the expected  -->
<!-- %Figure \ref{fig:prior2} (right) shows the region of interest of these distributions by removing the mass at 0. % -->
<!-- %With the exception of the body part category, properties are mostly likely to be absent from the category (Figure \ref{fig:prior2} left; modes of distributions are at 0). -->
<!-- %If the property is present in the category, the most likely prevalence for biological properties (``part'', ``color part'', and ``vague part'') is 100\% (Figure \ref{fig:prior2} right; modes of blue, green, and red distributions are at 1). -->
<!-- %This is not the case with the prevalence priors for accidental properties, for which lower values are more likely (Figure \ref{fig:prior2} right; modes of orange and purples distributions are at some low prevalence). -->
<!-- %\ndg{this analysis has gotten a lot less transparent since i looked last. it's not at all clear why we should care about the gradient against "some". } -->
<!-- % -->
<!-- %However, the generic does more than merely inform a listener that the property is present:  -->
<!-- %A generic carries the communicative force of a speech act, and thus implies the property is \emph{more prevalent} than a listener would expect (Figure \ref{fig:exp2b} solid line lies above $y=x$ line). -->
<!-- %\ndg{i think if we're going to rely on it we need to set this contrast up much more clearly earlier: many properties are never present in most categories. at the least, the generic rules out the absence of the property (ie ``some''). we want to test whether it is stronger than some. -- though really?} -->
<!-- %The listener model (Eq.~\ref{eq:L1}) produced the same strong interpretation along a gradient (Figure \ref{fig:exp2b}, Right, solid line), displaying the sensitivity to abstract beliefs about the properties that human participants show.  -->
<!-- %We performed a by-item analysis comparing the implied prevalence data to model predictions and found a good quantitative fit ($r^2(40) = 0.89$; see Supplement Section D5).  -->
<!-- %\ndg{ -->
<!-- %Generics ``once accepted [...] appear to be commonly taken in a rather strong sense, as if the qualifier \emph{always} had implicitly crept into their interpretation'' (\cite{Abelson1966}, Cf.~\cite{Cimpian2010}).  -->
<!-- %\cite{Gelman2002} found that adults interpret novel generic statements about familiar kinds (e.g. \emph{Bears like to eat ants.}) as implying that almost all of the category have the property (e.g. almost all bears like to eat ants). -->
<!-- %Why is generic language interpreted so strongly if the criterion for endorsement is so flexible?  -->
<!-- %} -->



# Experiment 2a: Prevalence prior elicitation

## Methods

### Participants

We recruited 180 participants from Amazon's crowd-sourcing platform Mechanical Turk (MTurk). 
This number was arrived at with the intention of getting approximately 40 independent sets of ratings for each unique item in the experiment.
Participants were restricted to those with U.S. IP addresses and with at least a 95\% MTurk work approval rating (these same criteria apply to all experiments reported).
The experiment took on average `r round(d.prior.time.summary[[1, "aveTime"]], 1)` minutes and participants were compensated \$K.

### Procedure and materials

On the first trial, participants were asked to list their five favorite kinds of animals. 
On subsequent trials, participants were asked how many of each of the elicited animal categories they believed had a property (e.g., "How many cheetahs do you think attack hikers?").
Participants responded using a slider bar with endpoints labeled 0\% and 100\%. 
The percentage corresponding to their slider bar rating was displayed once participants clicked on the slider bar.

Judgments were obtained for 72 items, of which each participant saw a random selection of 12.
Items were generated by the first author by considering six different kinds of properties: physical characteristics (e.g., *have brown spots*), psychological characteristics (e.g., *experience emotions*), diet (e.g., *eat human food*), habitat (e.g., *live in zoos*), disease (e.g., *get cancer*, *carry malaria*), reproductive behavior (e.g., *have a menstrual cycle*), and other miscellaneous behaviors (e.g., *pound their chests to display dominance*, *perform in the circus*); online sources about strange animal behaviors were consulted in order to find more obscure properties. 

As an attention check, at the end of the generic interpertation trials, participants were asked to select (from a list of 10) all of the properties they could remember being tested on. 
The list included 5 properties that they had been tested on and 5 distractors. 



## Results

Each item received between X and Y responses. 
<!-- Distributions of responses for each item were smoothed using nonparametric density estimation for ordinal categorical variables (Li & Racine, 2003) with the np package in R (Hayfield & Racine, 2008).  -->
For each item, we then computed its belief distributionâ€™s expected value.
As intended, items covered a wide range of probabilities and expected values (see Figure 2).


# Experiment 2: Interpretation

### Methods

#### Participants

We recruited N participants from Amazon's crowd-sourcing platform Mechanical Turk (MTurk). 
This number was arrived at with the intention of getting approximately 50 ratings for each unique item in the experiment.
The experiment took on average `r round(d.int.time.summary[[1, "aveTime"]], 1)` minutes and participants were compensated \$K.

#### Procedure and materials

Participants were told that scientists had recently discovered a new island with lots of new animals on it, and that they would be told facts about the new animals and be asked to translate it into the how many (what percentage) of that animal it applies to.
On each trial, participants read a bare plural statement about a familiar property $F$ (e.g., *attack hikers*) applying to a novel animal category $K$ (e.g., *Javs*).
They were then asked "How many *Ks* do you think *F*?" (e.g., "How many javs do you think attack hikers?").
Participants responded using a slider bar with endpoints labeled 0\% and 100\%. 
The percentage corresponding to their slider bar rating was displayed once participants clicked on the slider bar.

Novel animal category names were mostly taken from @Cimpian2010, as well as a few other studies on generic language. 


### Results

## Cognitive modeling results

# Experiment 3: Prior manipulation and interpretation

## Methods

### Participants

### Material

### Procedure

## Data analysis

## Results

# References


```{r create_r-references}
r_refs(file = "generics.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
